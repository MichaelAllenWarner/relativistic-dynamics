\documentclass[12pt]{article}
\usepackage{gensymb}
\usepackage[tbtags]{amsmath}
\usepackage{bm, upgreek}
\usepackage{amssymb}
\usepackage{hyperref}
\hypersetup{bookmarksnumbered,colorlinks=true, linkcolor=yaleblue, urlcolor=yaleblue}
\usepackage{xcolor}
\definecolor{dimgray}{rgb}{0.41, 0.41, 0.41}
\definecolor{airforceblue}{rgb}{0.36, 0.54, 0.66}
\definecolor{yaleblue}{rgb}{0.06, 0.3, 0.57}
\definecolor{tealblue}{rgb}{0.21, 0.46, 0.53}
\usepackage{textcomp}
\usepackage{centernot}
\usepackage{ragged2e}
\usepackage{esvect}
\renewcommand{\vv}[1]{\mathbf{#1}}
\newcommand{\dd}[1]{\mathrm{d}#1}
\newcommand{\vvbeta}{\bm{\upbeta}}
\newcommand{\hatbeta}{\bm{\hat{\upbeta}}}
\newcommand{\vvomega}{\bm{\upomega}}
\newcommand{\vvphi}{\bm{\upphi}}
\newcommand{\hatphi}{\bm{\hat{\upphi}}}
\newcommand{\vvzeta}{\bm{\upzeta}}
\newcommand{\hatzeta}{\bm{\hat{\upzeta}}}
\newcommand{\vvsigma}{\bm{\upsigma}}
\newcommand{\del}{\boldsymbol{\nabla}}
\newcommand{\tightoverset}[2]{%
  \mathop{#2}\limits^{\vbox to -.5ex{\kern-0.75ex\hbox{$#1$}\vss}}}
\newcommand{\inlinedy}[1]{\tightoverset{\text{\tiny$\bm\leftrightarrow$}}{#1}}
\newcommand{\fnoverset}[2]{%
  \mathop{#2}\limits^{\vbox to -.5ex{\kern-0.85ex\hbox{$#1$}\vss}}}
\newcommand{\footnotedy}[1]{\fnoverset{\text{\tiny$\bm\leftrightarrow$}}{#1}}
\newcommand{\superoverset}[2]{%
  \mathop{#2}\limits^{\vbox to -.5ex{\kern-0.95ex\hbox{$#1$}\vss}}}
\newcommand{\superdy}[1]{\superoverset{\text{\tiny$\bm\leftrightarrow$}}{#1}}
\newcommand{\capdy}[1]{ \overset{ \text{\tiny$\bm\leftrightarrow$} }{\vphantom{\text{\small{A}}}\smash{#1}} }
\usepackage{tkz-euclide}
\usepackage{tikz}
\usepackage{tkz-tab}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hypcap}
\usepackage{cancel}
\interfootnotelinepenalty=10000
\DeclareFontEncoding{FML}{}{}%
\DeclareFontSubstitution{FML}{futm}{m}{it}%
\DeclareSymbolFont{fourier}{FML}{futm}{b}{it}%
\DeclareMathSymbol{\partialup}{\mathord}{fourier}{130} % Upright partial derivative symbol, bold
\DeclareFontEncoding{FML}{}{}%
\DeclareFontSubstitution{FML}{futm}{m}{it}%
\DeclareSymbolFont{fourierNotBold}{FML}{futm}{m}{it}%
\DeclareMathSymbol{\partialrm}{\mathord}{fourierNotBold}{130} % Upright partial derivative symbol, not bold
\DeclareMathAlphabet{\mathsfit}{T1}{\sfdefault}{\mddefault}{\sldefault}
\DeclareMathOperator{\diag}{diag}
% center title
\usepackage{titling}
\renewcommand\maketitlehooka{\null\mbox{}\vfill}
\renewcommand\maketitlehookd{\vfill\null}

% TOC formatting
\usepackage{etoolbox}
\makeatletter
\pretocmd{\section}{\addtocontents{toc}{\protect\addvspace{14\p@}}}{}{}
\pretocmd{\subsection}{\addtocontents{toc}{\protect\addvspace{6\p@}}}{}{}
\pretocmd{\subsubsection}{\addtocontents{toc}{\protect\addvspace{4\p@}}}{}{}
\makeatother
\usepackage{tocloft}
\cftsetrmarg{9em}% TOC right-margin (default is 2.55em)

\usepackage{nomencl}
\makenomenclature

\renewcommand{\nomname}{List of Symbols}

\renewcommand{\nompreamble}{This isn't comprehensive. The symbols are presented in a kind of ``logical" order, corresponding \emph{very loosely} with their order of appearance in the text.}

\begin{document}

\title{Covariant ``Electrodyadics":\\A Gentle, ``Massless" Introduction to\\Relativistic Dynamics and Maxwell's Theory}
\author{Michael Allen Warner}
\date{Last updated: \today}

\begin{titlingpage}
\addcontentsline{toc}{section}{Title}
\maketitle
\end{titlingpage}

\clearpage

\phantomsection
\setcounter{page}{2}
\addcontentsline{toc}{section}{Contents}
\hyphenchar\font=-1 % disable hyphen
\tableofcontents
\hyphenchar\font=`\- % reset hyphen

\clearpage

\phantomsection
\addcontentsline{toc}{section}{List of Symbols}

\nomenclature[010]{\( ct_0 \)}{Proper time. The naught subscript almost always means the ``proper" version of a quantity---i.e., the quantity as measured in the relevant rest frame.}
\nomenclature[020]{\( \dot{ \text{\underline{\hspace{.6em}}} } \)}{Overdot means derivative with respect to $ct$ (coordinate time).}
\nomenclature[030]{\( \mathring{ \text{\underline{\hspace{.6em}}} } \)}{Over-circle means derivative with respect to $ct_0$ (proper time).}
\nomenclature[033]{\( \vv 0 \)}{Zero vector.}
\nomenclature[039]{\( \textrm{\mbox{\boldmath$\emptyset$}} \)}{Zero four-vector.}
\nomenclature[040]{\( \vv r \)}{Position vector $\langle x, y, z \rangle$.}
\nomenclature[050]{\( \vv R \)}{Position four-vector $\langle ct, \vv r \rangle$ (four-position).}
\nomenclature[053]{\( \Delta \vv r \)}{Displacement or separation vector (\emph{displacement} for something in motion, \emph{separation} for an abstract vector ``connecting" two points).}
\nomenclature[055]{\( \Delta \vv R \)}{Four-displacement or four-separation.}
\nomenclature[057]{\( \Delta s \)}{Spacetime interval $\sqrt{ (c \mkern.5mu \Delta t)^2 - ( \Delta \vv r )^2 }$ (magnitude of $\Delta \vv R$).}
\nomenclature[060]{\( \vvbeta, \beta \)}{Normalized velocity $\dot{\vv r}$ and its magnitude. Dimensionless; $0 \leq \beta < 1$.}
\nomenclature[070]{\( \vv B, B \)}{Normalized four-velocity $\mathring{\vv R}$ and its magnitude. Dimensionless; $B = 1$.}
\nomenclature[080]{\( \vvzeta, \zeta \)}{``Normalized" acceleration $\dot{\vvbeta}$ and its magnitude.}
\nomenclature[090]{\( \vv Z, Z \)}{``Normalized" four-acceleration $\mathring{\vv B}$ and its magnitude.}
\nomenclature[092]{\( \gamma \)}{Lorentz factor $1 / \sqrt{1 - \beta^2} = \frac{\dd (ct)}{\dd (c t_0)}$ (time-dilation factor).}
\nomenclature[094]{\( \vvomega, \omega \)}{``Normalized" celerity $\mathring{\vv r} = \gamma \dot{\vv r} = \gamma \vvbeta$ and its magnitude.}
\nomenclature[096]{\( \nu \)}{Frequency.}
\nomenclature[098]{\( \lambda \)}{Wavelength.}
\nomenclature[100]{\( E_0 \)}{Rest energy (proper energy). Equals zero when $\beta = 1$ (e.g., for light).}
\nomenclature[110]{\( E \)}{Total energy. Equals $\gamma E_0$ when $E_0 > 0$.}
\nomenclature[120]{\( E_{\mkern.5mu \textrm{k}} \)}{Kinetic energy $E - E_0$. Equals $E_0 (\gamma - 1)$ when $E_0 > 0$.}
\nomenclature[130]{\( \mathcal{P} \)}{``Normalized" power $\dot{E}$.}
\nomenclature[140]{\( \vv pc \)}{Momentum $E \vvbeta$. Equals $\gamma E_0 \vvbeta$ when $E_0 > 0$.}
\nomenclature[150]{\( \vv P \)}{Four-momentum $\langle E, \mkern.5mu  \vv pc \rangle$. Equals $E_0 \vv B$ when $E_0 > 0$.}
\nomenclature[160]{\( \vv f \)}{Force $\dot{\vv p}c$.}
\nomenclature[170]{\( \vv F \)}{Four-force $\mathring{\vv P} = \gamma \langle \mathcal{P}, \mkern.5mu \vv f \rangle$.}
\nomenclature[180]{\( \phi \)}{Rapidity $\tanh^{-1} \beta$.}
\nomenclature[190]{\( \vvphi \)}{Rapidity vector $\phi \hatbeta$ (where $\hatbeta = \vvbeta / \beta$).}
\nomenclature[200]{\( \varphi \)}{Longitudinal rapidity (signed rapidity along the axis of a Lorentz boost; e.g., a particle's $\tanh^{-1} \beta_x$ if the boost is along the $x$-axis).}
\nomenclature[210]{\( \del \)}{Del operator $\big \langle \frac{\partial}{\partial x}, \, \frac{\partial}{\partial y}, \, \frac{\partial}{\partial z}  \big \rangle$.}
\nomenclature[213]{\( \del_{\vv q} \)}{Feynman subscript. Example: $\del_{\vv q} ( \vv w \cdot \vv q ) = w_x \del q_x + w_y \del q_y + w_z \del q_z$.}
\nomenclature[215]{\( \nabla^2 \)}{Laplacian $\del \cdot \del = \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} + \frac{\partial^2}{\partial z^2}$.}
\nomenclature[220]{\( \partialup \)}{Four-del $\big \langle \frac{\partial}{\partial (ct)}, \, - \del \big \rangle = \big \langle \partial^{ct}, \, \partial^{x}, \, \partial^{y}, \, \partial^{z} \big \rangle$. Admits Feynman subscript.}
\nomenclature[225]{\( \)}{``Triple product": $\vv W \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv Q) \equiv \partialup_{\vv Q} ( \vv W \cdot \vv Q) - ( \vv W \cdot \partialup) \vv Q$.}
\nomenclature[230]{\( \Box \)}{d'Alembertian $\partialup \cdot \partialup = \frac{\partial^2}{\partial (ct)^2} - \frac{\partial^2}{\partial x^2} - \frac{\partial^2}{\partial y^2} - \frac{\partial^2}{\partial z^2}$ (the wave operator).}
\nomenclature[240]{\( q \)}{Electric charge.}
\nomenclature[250]{\( \rho \)}{Charge density.}
\nomenclature[260]{\( \vv j \)}{``Normalized" current density (the flux density of charge).}
\nomenclature[270]{\( \vv J \)}{``Normalized" four-current density $\langle \rho, \, \vv j \rangle$, with $\partialup \cdot \vv J =  0$.}
\nomenclature[280]{\( \vv A \)}{Four-potential $\langle A^{ct}, \, \vv a \rangle$. Any vector satisfying $- \partialup \mkern1mu `` \mkern-4mu \times ( \partialup \times \mkern-4mu " \mkern1mu \vv A ) = \vv J$.}
\nomenclature[283]{\( \vv e \)}{Electric field $- \del A^{ct} - \partial^{ct} \vv a$.}
\nomenclature[286]{\( \vv b \)}{Magnetic field $\del \times \vv a$.}
\nomenclature[290]{\( \vv F_{\textbf{L}} \)}{Lorentz four-force $q \vv B \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A) = \gamma q \mkern.5mu \langle \vv \vvbeta \cdot \vv e , \, \vv e + \vvbeta \times \vv b \rangle$.}
\nomenclature[300]{\( \bm{\mathfrak{F}} \)}{Lorentz four-force density $\vv J \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A) = \langle \, \vv j \cdot \vv e, \, \rho \mkern1mu \vv e + \vv j \times \vv b \rangle$.}
\nomenclature[310]{\( u \)}{Electromagnetic energy density $(e^2 + b^2)/2$.}
\nomenclature[320]{\( \vv s \)}{Poynting vector $\vv e \times \vv b$ (the flux density of electromagnetic energy, and also the electromagnetic momentum density).}
\nomenclature[330]{\( \otimes \)}{Symbol for the dyadic product (and the tensor product generally).}
\nomenclature[335]{\( I_n \)}{$n$-by-$n$ identity matrix; e.g., $I_3 = \diag(1, 1, 1)$.}
\nomenclature[340]{\( \inlinedy{\upiota} \)}{Euclidean unit dyadic, with Cartesian matrix-representation $I_3$.}
\nomenclature[350]{\( \inlinedy{\upsigma} \)}{Maxwell stress tensor $u \mkern-4.5mu \inlinedy{\upiota} - \, \vv e \otimes \vv e - \vv b \otimes \vv b$ (the flux density of electromagnetic momentum).}
\nomenclature[360]{\( \vv K \)}{Four-wavevector.}
\nomenclature[400]{\( \inlinedy{\upeta} \)}{Minkowski metric dyadic (inverse/contravariant metric tensor).}
\nomenclature[410]{\( \eta \)}{Matrix representation of Minkowski metric, $\diag(1, -1, -1, -1)$.}
\nomenclature[415]{\( \wedge \)}{Wedge product for vectors: $\vv Q \wedge \vv W = \vv Q \otimes \vv W - \vv W \otimes \vv Q$.}
\nomenclature[420]{\( \inlinedy{\mathsf{F}} \)}{Faraday tensor $\partialup \wedge \vv A$, with $\partialup \cdot \inlinedy{\mathsf{F}} = \vv J$ and $\mkern2mu \bm{\mathfrak{F}} = \inlinedy{\mathsf{F}} \cdot \mkern2.5mu \vv J$.}
\nomenclature[430]{\( \inlinedy{\mathsf{G}} \)}{Maxwell tensor $\star \inlinedy{\mathsf{F}}$ (Hodge dual of Faraday), with $\partialup \cdot \inlinedy{\mathsf{G}} = \textrm{\mbox{\boldmath$\emptyset$}}$.}
\nomenclature[440]{\( \inlinedy{\mathsf{T}} \)}{Total stress--energy tensor, with $\partialup \cdot \inlinedy{\mathsf{T}} = \textrm{\mbox{\boldmath $\emptyset$}}$ (conservation of $\vv P$).}
\nomenclature[450]{\( {\inlinedy{\mathsf{T}}}_{\textrm{em}} \)}{Electromagnetic stress--energy $( \inlinedy{\mathsf{F}} \cdot \inlinedy{\mathsf{F}} + \inlinedy{\mathsf{G}} \cdot \inlinedy{\mathsf{G}} )/2$, with $\partialup \cdot  {\inlinedy{\mathsf{T}}}_{\textrm{em}} =$ $- \bm{\mathfrak{F}}$.}
\nomenclature[460]{\( \vv l \)}{Angular momentum $\vv r \times \vv pc$.}
\nomenclature[470]{\( \inlinedy{\mathsf{L}} \)}{Angular-momentum four-dyadic $\vv R \wedge \vv P$.}
\nomenclature[480]{\( \inlinedy{\mathsf{0}} \)}{Zero four-dyadic.}


\printnomenclature

\clearpage

\section*{Preface}\label{sec:p}
\addcontentsline{toc}{section}{Preface}

This is the gentlest introduction to relativistic dynamics you'll find. Ideally you're already familiar with basic relativistic kinematics, but we'll review what we need in Section \ref{ssec:sr}. Math prerequisites are minimal. Through Section \ref{ssec:ra}, the only calculus we'll use is simple single-variable differentiation. After that, there are integrals here and there, and Section \ref{sec:rem} on electromagnetism has partial derivatives and vector calculus. Section \ref{sec:dy} on ``electrodyadics" uses tensors but doesn't require prior experience with them.

The ``massless" in the title is half tongue-in-cheek. The serious bit is that beyond Newtonian mechanics, I mostly avoid the word \emph{mass} and the symbol $m$. This does set my approach apart, but you'll see that it's just a linguistic and notational preference. There's nothing radical about it.

You should know that I'm not a physicist. I'm a motivated amateur, and this document amounts to glorified notes I've made for myself as a self-study aid. I have no professional aspirations, and my focus is resolutely \emph{not} on developing problem-solving techniques. \emph{This isn't a textbook}; I'm not nearly qualified to write one! What you'll find here is a unique path through the material that worked for me. When I started, I was competent in algebra, rusty at calculus, and somewhat familiar with Newtonian mechanics, classical electromagnetism, and rudimentary special relativity. To be honest, I'm still only competent in algebra and mediocre at calculus, but I do now understand a lot of physics for a layman.

If you're still here, I'll tell you more about my unusual approach and why I took it. Several years ago, I wanted to better grasp the concepts of mass, energy, and momentum in special relativity (i.e., relativistic dynamics). This---and an interest in history---led me to Einstein's original ``$E = mc^2$" paper, which involves a clever thought experiment concerning a body that emits two light waves in opposite directions. It occurred to me that considering the thought experiment from a \emph{pre}-relativistic perspective might prove educational. I was right! It admits no non-relativistic ``resolution," but naively attempting to find one lays bare the incompatibility between Newtonian mechanics and Maxwell's electromagnetic theory, entirely in terms of the dynamics of a very simple system. So I adopted an outlook that views resolution of the thought experiment as \emph{motivation} for special relativity. The thought experiment became my anchor, and my goal was to dissect it as fully as I could. This guided my path of self-study and became a running theme of this paper. It even motivates the development of covariant electrodynamics in Sections \ref{sec:rem} and \ref{sec:dy}.

Since this document roughly follows my own winding journey through the material, it's ``personal" and unapologetically quirky. The breadth and depth of topics covered, the order and style of presentation---it all reflects my evolving interests and increasing proficiency as I learned. For instance, gaining intuition for the geometric four-vector formalism was challenging for me, and so I'm sure I spend many more pages easing the reader into it than some will deem necessary or palatable. By contrast, I was comfortable with four-vectors by the time I began digging into electrodynamics, and as a result the coverage of that topic is rather sophisticated from the get-go. It is my hope that readers with a background similar to mine will find the pace about right overall.

More notable things about this paper:
\begin{itemize}
\item I emphasize the concept of \emph{rest energy} almost immediately, and it remains central throughout the text. (This is connected to the ``massless" in the title.)
\item Though the review of relativistic kinematics (Section \ref{ssec:sr}) is far from comprehensive, its treatment of velocities, angles, aberration, and the Doppler effect is (I think) clearer than most. I took great care here because the relativistic Doppler effect is at the heart of resolving Einstein's thought experiment, and in my framing obtaining it was the \emph{purpose} of the whole section.
\item Some ``tangential" topics are covered fairly thoroughly (for an introductory text): transformation formulas for acceleration and force; rapidity and related concepts; the ultrarelativistic limit; angular momentum (eventually).
\item I avoid index notation. This is good for the ``lazy" reader but probably bad for the ambitious one (who really \emph{must} learn index notation). Still, even the ambitious reader may gain insight from the abstract notation found here, especially in later sections.
\item Section \ref{sec:rem} gives a ``complete" treatment of electrodynamics that uses four-vectors but no explicit rank-2 tensors.\footnote{By ``complete" I only mean that the treatment covers both the Maxwell equations and the Lorentz force. Its scope is actually quite limited.} Section \ref{sec:dy} expands on that material by introducing tensors, but only in the guise of \emph{dyadics}. All this is done without index notation, covectors, or differential forms. My intent is to make covariant electrodynamics accessible to as wide an audience as possible. The treatment even touches on Lagrangian methods---not at all rigorously or adequately, but in a novel way meant to offer some ``feeling" for the subject while ultimately pointing to the need for a more robust mathematical toolkit.
\item There are almost no diagrams. Sorry; maybe I'll add some later.
\end{itemize}

I've made a few unconventional choices in notation. Most aren't particularly consequential, but here are two worth mentioning straight away:
\begin{itemize}
\item Newton's overdot notation signals a derivative taken with respect to $ct$, not $t$. For example, where $\vv r$ is position and $\vv v$ velocity, ${\dot{\vv r} \neq \dd \vv r / \dd t = \vv v}$. Instead, $\dot{\vv r} = \dd \vv r / \dd (ct) \equiv \vvbeta$. Likewise, where $\vv a = \dd \vv v / \dd t$ is acceleration, $\ddot{\vv r} = \dot{\vvbeta} = \dd \vvbeta / \dd (ct) = \vv a / c^2 \equiv \vvzeta$. Once I introduce the dimensionless $\vvbeta$ (and the dimensionful $\vvzeta$), I treat it not as shorthand but as the natural way to express velocity. I have no use for $\vv v$ (or $\vv a$).
\item For $c t_0$-derivatives (where $ct_0$ is \emph{proper} time), I use a little over-circle instead of an overdot: $\mathring{\vv r} = \dd \vv r / \dd (c t_0)$.
\end{itemize}
I'll remind you of this dot and circle business along the way.

On vector notation: vectors are boldface, their magnitudes are italic (like any scalar, but see below), a vector in Cartesian-component form is enclosed by angle brackets (not parentheses), and squaring a vector means taking its dot product with itself, which is equivalent to squaring its magnitude. So velocity is $\vv v = \langle v_x, v_y, v_z \rangle$, speed is $v$, and $\vv v^2 = \vv v \cdot \vv v = v_x^2 + v_y^2 + v_z^2 = v^2$.

Occasionally italics aren't suitable for notating a vector's magnitude. A good example is the vector $\Delta \vv r$, because $\Delta r$ already signifies the change in $r$ (magnitude of position $\vv r$). In such cases we'll indicate the vector's magnitude with double bars instead (e.g., $\Vert \Delta \vv r \Vert$). Moreover, notating a vector's magnitude with plain italic type is convenient but somewhat dangerous, as doing so makes it easy to forget to treat magnitude as a \emph{function} when necessary, as when differentiating:
\begin{equation*}
\begin{aligned}
\frac{\dd \Vert \vv q \Vert}{\dd (ct)} &= \frac{\dd}{\dd (ct)} \sqrt{ \vv q \cdot \vv q } \\[2pt]
&= \frac{\dd}{\dd (\vv q \cdot \vv q)} \left( \vv q \cdot \vv q \right) ^{1/2} \, \frac{\dd (\vv q \cdot \vv q)}{\dd (ct)} \\[2pt]
&= \frac{1}{2 \sqrt{\vv q \cdot \vv q}} \, \left( \dot{\vv q} \cdot \vv q  \, + \, \vv q \cdot \dot{\vv q} \right) \\[4pt]
&= \frac{\vv q \cdot \dot{\vv q}}{q} \\[4pt]
&= \vv{ \hat q } \cdot \dot{ \vv q }
\end{aligned}
\end{equation*}
(where $\vv{ \hat q } = \vv q / q$ is the unit vector pointing in the direction of $\vv q$). That's just something to keep in mind.

When we get to \emph{four-vectors} (don't worry if you don't know what they are yet), we'll use essentially the same notation, but with uppercase symbols like $\vv Q$. I'm pretty consistent with this lowercase vs. uppercase convention, going so far as to use $\vv e$ and $\vv b$ instead of $\vv E$ and $\vv B$ for the electric and magnetic fields. When a four-vector is ``analogous" to a normal Euclidean vector, I'll usually give it the uppercase counterpart to the lowercase symbol already in use; for instance, the four-vector analogue of $\vvbeta = \vv v / c$ will be $\vv B$ (the dimensionless ``four-velocity"), and the four-vector analogue of $\vvzeta = \vv a / c^2$ will be $\vv Z$ (a ``four-acceleration" with the same dimension as $\vvzeta$).

The footnotes are mainly for exposition, not citations. See the \hyperref[sec:r]{References} for my most important sources of inspiration.

\clearpage


\section{Relativistic Dynamics, No Four-Vectors}\label{sec:rd}


In Einstein's original ``$E=mc^2$" paper,\footnote{Einstein, A.: Ist die Tr\"agheit eines K\"orpers von seinem Energieinhalt abh\"angig? Ann.\ Phys.\ \textbf{18}, 639--641 (1905). English translation here: \url{https://www.fourmilab.ch/etexts/einstein/E_mc2/www/}.} he proposes a thought experiment in which a body at rest briefly emits two equally energetic light waves in opposite directions at the same time.\footnote{We treat them as idealized monochromatic plane waves, though we'll soon temporarily invoke the photon model as a shortcut (later we'll reproduce our results without it).} With the aid of his newly developed special theory of relativity, he considers the situation from the perspectives of two observers: one at rest relative to the body, the other moving with an arbitrary constant velocity.
 
The paper is concise---almost perfunctory---but it's also rather subtle. We're going to walk through the thought experiment twice. The first time we'll feign ignorance of special relativity, putting ourselves in the shoes of a physicist in mid-1905. We'll fail, but our failure will be instructive. On the second pass, we'll retrace Einstein's steps (more or less).


\subsection{Einstein's Thought Experiment without Einstein}
 
\subsubsection{Newton and Maxwell; Additivity and Conservation}

What can we say about Einstein's light-emitting body if we have Newtonian physics and Maxwell's equations, but no special relativity? To start, classical mechanics gives us linear momentum
\begin{equation}\label{eq:1}
\vv p=m\vv v \quad \text{\footnotesize{ (classical)}}
\end{equation}
and kinetic energy
\begin{equation}\label{eq:2}
E_{\mkern.5mu \textrm{k}}= \frac{1}{2} \, m v ^2 \quad \text{\footnotesize{ (classical)}},
\end{equation}
where $m$ is mass and $\vv{v}$ velocity. We also have the \textbf{conservation} laws for mass, momentum, and total energy $E$ (i.e., $\Delta m_{\mathrm{system}} = \Delta E_{\mathrm{system}} = 0$, and $\Delta \vv p_{\mathrm{system}} = \vv 0$), valid for \textbf{closed} systems that don't interact with the outside world. Truly closed systems only exist in thought experiments, though. For an \textbf{open} system, any change of $m_{\mathrm{system}}$, $E_{\mathrm{system}}$, or $\vv p_{\mathrm{system}}$ is equal-and-opposite to a corresponding change in the environment, meaning that these three conserved quantities are also \textbf{additive}:\footnote{In this paper we distinguish between conservation and additivity, but you should be aware that many treatments lump both properties together to define \emph{conservation} (sometimes only tacitly). Their ``conserved" is our ``additive and conserved."} a system's mass, momentum, and total energy are the sums (respectively) of the masses, momenta, and energies of the system's constituents.\footnote{\label{fn:ad}This is how things stand without special relativity, anyway. If we find that any of this additivity breaks down, we'll want to take note.} In particular, a system's total energy is the sum of \emph{all} energy contributions, kinetic and otherwise.

Because Equations \ref{eq:1} and \ref{eq:2} depend on velocity, a closed system's momentum and total energy remain unchanged only for inertial observers. Different inertial observers generally don't agree on the \emph{value} of a closed system's momentum or total energy, but they agree that the value doesn't change.

And then there's Maxwell. From him, we know that light is a wave that carries energy and momentum in directly proportional magnitudes, even though it has no mass. In other words---and this should make us deeply uncomfortable---light is ``exempt" from Newton's mass-dependent Equations \ref{eq:1} and \ref{eq:2} (which would return values of zero!), but it still carries energy and momentum
\begin{equation}\label{eq:3}
E \propto p >0 \quad \text{\footnotesize{ (for light)}} .
\end{equation}
Yet, in early 1905, Einstein proposed (correctly) that, \emph{pace} Maxwell, the energy (and momentum) of a \textbf{photon}---a then-theoretical ``corpuscle" of light---is directly proportional to the light's \emph{frequency} $\nu$, an idea related to one that Max Planck had floated in 1900:
\begin{equation}\label{eq:4}
E \propto \nu \quad \text{\footnotesize{ (for photons)}}.
\end{equation}
We're committing a minor sin by mentioning photons at all in our non-quantum context, but this Planck--Einstein relation will allow us to take a shortcut, and in Section \ref{sec:rem} we'll redeem ourselves by replicating our results without it. Besides, it's not as though Planck and Einstein were just making things up; Equation \ref{eq:4} explained empirical results that Maxwell's wave model couldn't, such as the photoelectric effect.

Now on to the thought experiment!

First we consider things from the perspective of the observer at rest relative to the body. In this observer's frame, the body's initial speed is zero, which means its initial momentum and kinetic energy are also zero. The light waves, however, carry energy, and that energy had to come from somewhere. Did it come from the body's kinetic energy?

No, because the $E_{\mkern.5mu \textrm{k}}$ was already zero, and Equation \ref{eq:2} tells us that $E_{\mkern.5mu \textrm{k}}$ can't be negative (mass can't be negative). Here's another argument that leads to the same conclusion: By Equation \ref{eq:3}, our equally energetic light waves carry momenta that are equal in magnitude. Since those momenta have opposite direction, the vectors cancel, leaving the body's momentum unaffected. The massless light likewise leaves the body's \emph{mass} unaffected.\footnote{\label{fn:ma}Or so we think. If it turns out that mass isn't additive, then we'll have to be open to the possibility that the body's mass \emph{is} affected, even though light is massless.} If neither the body's momentum nor its mass changes, then neither does its velocity (Equation \ref{eq:1})---the body remains at rest. And if neither its mass nor its velocity changes, then neither does its kinetic energy (Equation \ref{eq:2}).

\subsubsection{Systems and Rest Energy All the Way Down}\label{sssec:sy}

This energy carried off by the light waves---where did it come from? All we really know is that it came from the resting body itself, and not from the body's kinetic energy, which remains zero for our first observer. By conservation and additivity of energy, anything at rest that can radiate energy without moving must already have some to lose, \emph{independent} of its motion.

That observation may strike you as trivial, especially if you're familiar with the concept of \textbf{internal energy} from thermodynamics. But it isn't trivial, and the category of energy we're introducing is more general than internal energy. Internal energy specifically refers to the kinetic and potential energy associated with the molecules of a substance, transferable by work or heat (or substance transfer). By contrast, we haven't specified what energy was lost by the body---molecular kinetic? potential? something else?---and transfer-mechanisms won't figure into the definition we're working toward.

The energy we're talking about is something of a black box. Simply put, it's \emph{any and all energy a system has that's independent of its velocity}. Because this energy is independent of the system's velocity, all observers will agree that it's there and how much of it there is.\footnote{\label{fn:re}At this stage, don't worry about how they'd measure this energy. We'll get there.} To an observer who shares a body's rest frame, the body has zero $E_{\mkern.5mu \textrm{k}}$, and this new category of energy constitutes the \emph{entirety} of the body's total energy $E$. So we can think of it as the total energy a system has when it's at rest. For that reason, it's usually called \textbf{rest energy} (or sometimes \textbf{proper energy}). Let's give it the symbol $E_0$. By the additivity of energy, we can now say of any system:
\begin{equation}\label{eq:5}
\boxed{E=E_0+E_{\mkern.5mu \textrm{k}}} \, ,
\end{equation}
where different observers disagree on the velocity-dependent $E_{\mkern.5mu \textrm{k}}$ but agree on $E_0$. In the system's rest frame, Equation \ref{eq:5} becomes
\begin{equation}\label{eq:6}
E=E_0 \quad \textrm{\footnotesize{ (for $E_{\mkern.5mu \textrm{k}}=0$)}}.
\end{equation}
A quantity like rest energy whose value everyone agrees on is said to be \textbf{invariant}.\footnote{\label{fn:inv}Really, one must specify the \emph{context} in which a quantity is invariant. Take kinetic energy: it certainly isn't invariant with respect to a change in observer velocity (a \textbf{boost}), but it \emph{is} invariant under rotation of a chosen set of Cartesian axes. In this paper, when we say that a quantity is invariant, by default we mean with respect to observer velocity.} Mass is another invariant, and (spoiler alert) we'll soon discover a close connection between mass and rest energy.

If your instinct is to resist Equation \ref{eq:5} on the grounds that an open system may also have potential energy owing to its position relative to some outside body, remember that the potential energy in that case is a property not of the open system or the outside body, but rather of the greater system that encompasses both. It's accounted for in the greater system's \emph{rest energy}. Equation \ref{eq:5} works. Resistance is futile.\footnote{Okay, one caveat: we're assuming that the potential energy isn't associated with the kind of gravitational field that would require the \emph{general} theory of relativity to model. Actually, let's broaden that caveat---for this entire paper, \emph{assume the absence of gravity}!}

Again, we've stipulated nothing about the ``inner workings" of rest energy. From the point of view of an outside observer trying to find a body's total energy $E$, all that matter are the values $E_{\mkern.5mu \textrm{k}}$ and $E_0$; the physical phenomena behind that $E_0$ are irrelevant. But suppose we're curious and peer inside. What do we see? Equation \ref{eq:6} and the additivity of energy tell us only that whatever we see, if we sum up all the ``inside" energy contributions to find $E$ in the body's rest frame, then we'll get exactly the same value that anyone ``outside" gets for $E_0$. Those energy contributions could be anything. Maybe the body is full of gas molecules of various types, in which case the molecules have kinetic energy associated with their motion and some (negligible) potential energy associated with their relative positions, but each molecule \emph{also has its own rest energy}, further itemizable as the kinetic, potential, and rest energies associated with its constituent atoms and their relative positions. Regardless of the details, we must add up \emph{all} the ``inside" energy contributions---kinetic energies, potential energies, rest energies---to calculate the body's total energy $E$ in its rest frame. If we're thorough, then Equation \ref{eq:6} guarantees that our sum will match the number that outside observers assign $E_0$.

It's worth emphasizing that anything or any group of things can be regarded as a system. And if a system has a rest frame, then it has rest energy. But what \emph{is} a rest frame? It's self-explanatory when a non-rotating system is confined to a fixed volume (like the body in Einstein's thought experiment); then it's just the frame in which the system's bounds aren't moving. More generally, though, it's the inertial frame in which all the momenta within a system sum to a zero vector. Another name for it is the \textbf{center-of-momentum frame}.\footnote{In Newtonian physics, this frame is identical to the center-of-\emph{mass} frame. Not so in relativistic mechanics (hint: Footnote \ref{fn:ma}). It's momentum that we care about here.} If a system isn't confined to a fixed volume, this special frame can be difficult or altogether unfeasible to find, but in principle one can still determine it.\footnote{\label{fn:rl}Unless the system \emph{has no rest frame}, and thus no rest energy ($E = E_{\mkern.5mu \textrm{k}}$). Such systems are ``\textbf{restless}" (my own term), and we'll discuss them more in Section \ref{ssec:em}.} When we speak of a system's velocity, we are referring to the velocity of its rest frame relative to some observer.

Now, imagine the universe as systems within systems within systems. In Einstein's thought experiment, the closed system consists at first of only the body, but then of the body plus the two light waves. And although the body loses rest energy, the \emph{closed system} does not. Equation \ref{eq:6} tells us that to find the system's rest energy, we find the system's total energy in its rest frame, which, after emission, means accounting for the body's rest energy \emph{and also the energy carried away by the light waves}! That's why our closed system's rest energy doesn't change. A closed system's rest energy is both invariant \emph{and conserved}, the latter a trivial consequence of the conservation of \emph{total} energy (for which the rest frame is no exception).

If we ``zoom in" and regard the \emph{body} as our (open) system, we'll find that it does lose rest energy, but that Equations \ref{eq:5} and \ref{eq:6} still hold. So an \emph{open} system's rest energy is invariant but \emph{not} conserved, which again is really a statement about \emph{total} energy that holds in the rest frame as well as in any other (namely, its value for an open system can change).\footnote{When we get to special relativity, we'll have to qualify the statement that an open system's rest energy is invariant (because of the relativity of simultaneity).} Unlike total energy, however, rest energy is \emph{not} additive. \emph{This is an absolutely essential point!} Rest energy is the sum of \emph{all} energy contributions in the rest frame, not just the sum of the constituent rest energies. Obvious but crucial.

If we zoom in further and treat, say, a CO$_2$ molecule in the body as our system, we can again describe its energy using Equations \ref{eq:5} and \ref{eq:6}. The molecule consists of carbon and oxygen atoms with their own rest and kinetic energies, and there's also some (comparatively negligible) potential energy associated with the chemical bonds keeping them together.\footnote{\label{fn:be}This \textbf{binding energy} is actually negative, since energy must be invested to decompose a bound system. And binding energy certainly isn't negligible at the level of an individual atom or nucleus! For instance, the rest energy of a bound proton--neutron system (a \textbf{deuteron}) is measurably less than the sum of the rest energies of the two constituent nucleons---by about 2.224 mega-electron-volts (MeV), which is well over four times the rest energy of an electron.} To find the molecule's total energy $E$ in its rest frame, we just add up all of those energy contributions, and, lo and behold, our tally matches what outside observers call the molecule's rest energy $E_0$.

We can keep zooming in. When we get to the smallest bound systems, we find that kinetic and potential energies contribute substantially to system rest energy.\footnote{To be clear, not all potential energy is negative like binding energy is (see Footnote \ref{fn:be}). Potential energy associated with an \emph{attractive} force is negative, but potential energy associated with a \emph{repulsive} force is positive. Either way, we're constrained here to define the potential energy such that it approaches zero in the limit that the system's constituents are infinitely far apart. That's because we're no longer only interested in \emph{differences} in potential energy. The invariance of rest energy demands that all ``inside" energy contributions have an unambiguous value.} In fact, only some 1\% of a proton's or neutron's rest energy comes from the rest energies of its constituent quarks. The other 99\% of the ``inside" energy contributions are kinetic and potential.\footnote{\label{fn:mwm}Because the rest energies of nucleons in turn constitute almost all of the rest energy of the macroscopic world, we can say that, at heart, the rest energy of everything around us is mainly kinetic and potential energy within protons and neutrons.} And although it takes quantum chromodynamics to suss out the details, Equations \ref{eq:5} and \ref{eq:6} work just fine for nucleons.

We can even take an elementary particle like an electron as our system. We can't speak of an electron having an ``inside," but it obeys Equations \ref{eq:5} and \ref{eq:6} nevertheless, and it has a rest energy that's now known to arise from the Higgs mechanism (about 0.511 MeV).

It's systems and rest energy all the way down!

Let's zoom back out, lest we forget that the beauty of the rest-energy concept is that it permits us to \emph{ignore} the energy contributions at the ``zoomed-in" levels. If we're concerned with a macroscopic system, there's no need to add up all the kinetic, potential, and rest energies associated with the elementary particles. Thanks to our new equations and the additivity of total energy, those contributions are automatically included in the system's invariant $E_0$. We just need a way to measure it---and we'll have one soon (I promise!). Once we know $E_0$, it's a cinch to find the system's total energy $E$ in any frame: merely add its velocity-dependent $E_{\mkern.5mu \textrm{k}}$, as per Equation \ref{eq:5}.

Equations \ref{eq:5} and \ref{eq:6} are simple but profound. Get to know them.

\subsubsection{The Catch}\label{sssec:tc}

Now let's return to Einstein's thought experiment and ponder how things are for the observer moving at some constant velocity relative to the body.

In this observer's rest frame, the body starts with non-zero velocity, momentum, and kinetic energy. What changes when the light is emitted? Certainly not the body's velocity---if it's constant for one observer, it's constant for the other. And since the body's mass is likewise unaffected (but see Footnote \ref{fn:ma}), the moving observer says there's no change in the body's momentum or kinetic energy, either. The energy lost to the light waves must have come from the body's rest energy, as before (right?).

So it seems that the observer at rest and the moving observer are in agreement: the body's kinetic energy and momentum remain the same, and the light waves carry away combined energy equal to $-\Delta E_0$ (where $\Delta E_0$ is the invariant change in the body's rest energy, a negative value). Lovely.

Lovely but wrong!

First, any pair of things emitted in opposite directions according to the source do \emph{not} travel in opposite directions according to a moving observer (unless that observer's motion is parallel to the paths of the emitted things, but we'll tackle that in the next paragraph). Rather, each ``gains" a component of velocity in the direction of the source's motion.\footnote{We can calculate this component with Newtonian mechanics, as long as the source isn't moving too fast (and even if it is, the qualitative argument suffices here).} This means that our light waves don't travel in opposite directions for the moving observer. Consequently, their momentum vectors can't cancel, and by conservation the body's momentum must change! And if the body's momentum changes for the moving observer, so too must its mass or velocity (hint: it isn't the velocity), and thus also its kinetic energy.

Second, we've neglected Maxwell. Light is an electromagnetic wave, and the Doppler effect tells us that measured frequency depends on the observer's and wave's velocity vectors and the cosine of the angle between them.\footnote{Granted, the classical Doppler effect wrongly assumes that the light wave propagates at different speeds in different frames. But redshift had been observed and recognized as a Doppler phenomenon decades before Einstein arrived at the correct \emph{relativistic} Doppler formula. So the qualitative argument suffices here and isn't ahistorical.} Except for the special case that our moving observer's motion is perpendicular to the velocity vectors of both light waves according to the body (but see the previous paragraph), that cosine will of course be \emph{different} for each wave, and the waves will therefore have different frequencies in our moving observer's rest frame. By Equations \ref{eq:3} and \ref{eq:4}, different frequencies mean that the photon energies and momenta corresponding to one light wave will differ from those corresponding to the other, and again, by conservation, the body's momentum must change (and its mass\textinterrobang).

Where was the flaw in our ``lovely but wrong" analysis? We've unsubtly hinted at it several times now, so let's just say it outright: the culprit is mass! Specifically, it's our Newtonian assumption that mass is additive. Recall our unease when we introduced Equation \ref{eq:3}---we were justifiably suspicious that light could carry energy and momentum but be exempt from Equations \ref{eq:1} and \ref{eq:2}. Something had to give, didn't it? Our attempt to work out Einstein's thought experiment without special relativity has led us to the crux of the matter. If something massless like light can carry momentum (verified experimentally in 1901), and if momentum is indeed additive and conserved, then, in general, the mass of a system is \emph{not} the sum of the masses of its constituents. The body's mass must change when it emits the massless light waves.

But how is that possible? Isn't mass just the ``amount of matter"? Maybe not! Stay tuned.



\subsection{New Ground Rules: Special Relativity Review}\label{ssec:sr}

When Einstein wrote about this thought experiment, he'd already published his first paper on special relativity.\footnote{\label{fn:ep}Einstein, A.: Zur Elektrodynamik bewegter K\"orper. Ann.\ Phys.\ \textbf{17}, 891--921 (1905). English translation here: \url{https://www.fourmilab.ch/etexts/einstein/specrel/www/}.} In that earlier work, he posited a cosmic speed limit---i.e., that the speed of light in vacuum $c$ is invariant---and derived many of its consequences. Let's review some of them. Once we have the relativistic Doppler formula in hand, we'll come back to the thought experiment.


\subsubsection{Spacetime Interval}

An \textbf{event}, you may already know, is a location in space at a moment in time, or a point in \textbf{spacetime}. We begin our review of special relativity by taking for granted\footnote{The invariance of the spacetime interval can be derived from the invariance of $c$, but we won't bother.} the invariance of the \textbf{spacetime interval} between two events:
\begin{equation}\label{eq:sti}
\Delta s \equiv \sqrt{ (c \mkern.5mu \Delta t)^2 - ( \Delta \vv r )^2 } ,
\end{equation}
where $\vv r$ is position and ${\Delta \vv r = \langle \Delta x, \Delta y, \Delta z \rangle}$ is the spatial separation vector.\footnote{We'll call $\Delta \vv r$ \emph{displacement} when referring to the physical motion of something, and \emph{separation} when speaking in the abstract of a vector ``connecting" two points.}


\subsubsection{Standard Configuration and the Lorentz Transformation}\label{sssec:lt}

We'll say more about the interval in a bit, but first we'll use its invariance to work out how space and time coordinates transform under a change of inertial frames. In \emph{Newtonian} physics, the coordinates transform according to the \textbf{Galilean transformation} (and its inverse, on the right side below):
\begin{equation*}
\begin{aligned}
t^{\prime} &= t& t&= t^\prime \\
x^{\prime} &= x - vt& \qquad x &= x^\prime + vt^\prime\\
y^{\prime} &= y& y&= y^\prime\\
z^{\prime} &= z& z&= z^\prime .
\end{aligned}
\end{equation*}
Here the frames are in \textbf{standard configuration} for a \textbf{boost} along the coincident $x$/$x^\prime$-axes. That means that the Cartesian axes of each frame are parallel to the corresponding axes of the other, the origins coincide at $t=t^\prime=0$, and the ``primed" frame moves with speed $v$ in the positive ``unprimed" $x$-direction (equivalently, the unprimed frame moves with speed $v$ in the negative $x^\prime$-direction, so that the \textbf{relative speed} $v$ is really the shared magnitude of two different velocity vectors, each the velocity of one frame relative to the other).\footnote{Having the primed frame move in the \emph{positive} $x$-direction is an arbitrary and unnecessary restriction that often isn't included as part of the ``standard configuration." We include it because it lets us speak unambiguously of relative \emph{speed} as the boost parameter. Allowing the frames to move in either direction relative to each other is of course just fine, but it requires using relative \emph{velocity} as the boost parameter, and one must then choose (arbitrarily!) which of the two velocity vectors to use (primed frame's velocity in unprimed frame? or unprimed frame's velocity in primed frame?). By making a slightly more restrictive arbitrary decision up front, we avoid ``polluting" our transformation equations with a boost parameter that ``privileges" one frame over the other.} How do things change in special relativity?

An elegant way to answer that question starts with the observation that, \emph{under standard configuration}, the invariance of the interval implies the invariance of $(ct)^2 - x^2$, since $(ct)^2 - x^2 - y^2 - z^2$ is the squared interval between an arbitrary event and the event that constitutes the frames' shared spacetime origin (where $ct = ct^{\prime} = 0$ and $\vv r = \vv r ^{\prime} = \vv 0$), and since we (safely) assume that we'll still have $y^{\prime} = y$ and $z^{\prime} = z$. Defining $A \equiv ct + x$ and $B \equiv ct - x$, we can express that invariant \emph{difference} as an invariant \emph{product}, $A^{\prime} B^{\prime} = AB$. This is nice because, unlike $ct$ and $x$, $A$ and $B$ aren't ``coupled" to each other under a boost: in the language of $A$ and $B$, the linear transformation\footnote{``Linear" here means that we'll have $ct^{\prime} = \mu(v) ct + \nu(v) x$ and $x^{\prime} = \sigma(v) x + \rho(v) ct$, where $ct$ and $x$ appear \emph{only to first order}. (The factors $\mu$, $\nu$, $\sigma$, and $\rho$ are just functions of $v$, the frames' relative speed.) If this weren't so---if, say, the transformation involved $x^2$---then choice of spacetime-origin event would affect the \emph{physics}, which is absurd.} that takes $ct$ and $x$ to $ct^{\prime}$ and $x^{\prime}$ can \emph{only} entail multiplying $A$ by some function of the frames' relative speed $v$ and \emph{dividing $B$ by the same factor}. In other words, to satisfy $A^{\prime} B^{\prime} = AB$, we're restricted to ${A^{\prime} = hA}$ and ${B^{\prime} = B/h}$ for some $h = h(v)$.\footnote{$A$ and $B$ are known as \textbf{light-cone coordinates}.} Now the algebra will be a breeze!

From the definitions of $A$ and $B$, we have $A^{\prime} + B^{\prime} = 2ct^{\prime}$. Then:
\begin{equation}\label{eq:ctp}
\begin{split}
2 ct^{\prime} &= A^{\prime} + B^{\prime} \\
&= hA + h^{-1}B \\
&= h(ct + x) + h^{-1}(ct - x) \\[1pt]
ct^{\prime} &= \dfrac{h + h^{-1}}{2} \, ct \, + \, \dfrac{h - h^{-1}}{2} \, x.
\end{split}
\end{equation}
With $A^{\prime} - B^{\prime} = 2x^{\prime}$, a similar calculation gives:
\begin{equation}\label{eq:xp}
x^{\prime} = \dfrac{h + h^{-1}}{2} \, x \, + \, \dfrac{h - h^{-1}}{2} \, ct .
\end{equation}
This is a good time to remind ourselves of the definitions of the hyperbolic sine, hyperbolic cosine, and hyperbolic tangent functions and their inverses:
\begin{equation}\label{eq:hf}
\begin{aligned}
\cosh{\phi} = \dfrac{\mathrm{e}^\phi + \mathrm{e}^{-\phi}}{2} \qquad \quad \sinh{\phi} &= \dfrac{\mathrm{e}^\phi - \mathrm{e}^{-\phi}}{2} \qquad \quad \tanh{\phi} = \dfrac{\mathrm{e}^\phi - \mathrm{e}^{-\phi}}{\mathrm{e}^\phi + \mathrm{e}^{-\phi}} \\[10pt]
\cosh^{-1}{q} = \ln{ \left( q + \sqrt{q^2 - 1} \right)} &\qquad \quad \sinh^{-1}{q} = \ln{ \left( q + \sqrt{q^2 + 1} \right)} \\[5pt]
\tanh^{-1}{q} &= \dfrac{1}{2} \mkern1mu \ln{ \left( \dfrac{1 + q}{1 - q} \right) } .
\end{aligned}
\end{equation}
We immediately see an opportunity to rewrite Equations \ref{eq:ctp} and \ref{eq:xp} with the substitution $h = \mathrm{e}^{- \phi}$ for some function $\phi (v)$ (where we choose $- \phi$ instead of $+ \phi$ just because it will jibe better with our standard-configuration stipulation that the primed frame move in the \emph{positive} unprimed $x$-direction):
\begin{equation*}
ct^{\prime} = ( \cosh \phi ) \, ct - ( \sinh \phi ) \, x , \qquad x^{\prime} = ( \cosh \phi ) \, x - ( \sinh \phi ) \, ct .
\end{equation*}

Finally, we can determine the relationship between $\phi$ and $v$ (the relative speed of the frames) by noting first that for $x^{\prime} = 0$ (e.g., the primed origin), the above equation for $x^{\prime}$ gives $x / (ct) = \sinh \phi / \cosh \phi = \tanh \phi$, and second that, by standard configuration and the very definition of the frames' relative speed, the unprimed position $x = x(ct)$ of the primed origin must be ${x = vt}$, so that $x / (ct) = v / c$ (again, for $x^{\prime} = 0$). We therefore have $\tanh \phi = v / c$, or $\phi = \tanh^{-1}{(v / c)}$. Invoking the identities ${\cosh (\tanh^{-1}{q}) = (1 - q^2)^{-1/2}}$ and ${\sinh (\tanh^{-1}{q}) = q(1 - q^2)^{-1/2}}$, we obtain the \textbf{Lorentz transformation} and its inverse:
\begin{equation}\label{eq:lt}
\begin{aligned}
ct^{\prime} &= \dfrac{ct - \beta x}{\sqrt{1- \beta ^2}}& ct &= \dfrac{ct^\prime + \beta x^\prime}{\sqrt{1- \beta ^2}} \\[3pt]
x^{\prime} &= \dfrac{x - \beta \mkern1mu c t}{\sqrt{1- \beta ^2}}& \qquad x &= \dfrac{x^\prime + \beta \mkern1mu c t^\prime}{\sqrt{1- \beta ^2}}\\
y^{\prime} &= y& y&= y^\prime \\
z^{\prime} &= z& z&= z^\prime,
\end{aligned}
\end{equation}
valid for frames in standard configuration.\footnote{More general transformations drop certain stipulations about the axes and origins. We'll soon get a \emph{little} more general, but we'll mainly stick with rotation-free boosts, which have everything we need to study the ``new" physics of special relativity.} Here we've introduced the \textbf{normalized speed} $\beta$, which is the magnitude of the \textbf{normalized velocity} $\vvbeta=\vv v/c$. By \emph{normalized}, we mean that $\beta$ is a dimensionless number between 0 and 1.

Don't think of $\vvbeta$ as shorthand! The universe has a speed limit, and all speeds \emph{really are} fractions of it. Plus, working with $\vvbeta$ reveals mathematical elegance that's masked when working with $\vv v$. A good example is the symmetry of the $ct^\prime$ and $x^\prime$ equations above; if we use $v$ instead of $\beta$ (or $t$ instead of $ct$, for that matter), the symmetry of the numerators is obscured:
\begin{equation*}
ct^\prime = \frac{ct - \dfrac{vx}{\vphantom{V_v} c}}{\sqrt{1 - \dfrac{v^2}{c^2}}} \qquad x^\prime = \frac{x - vt}{\sqrt{1 - \dfrac{v^2}{c^2}}},
\end{equation*}
or worse:
\begin{equation*}
t^\prime = \frac{t - \dfrac{vx}{\vphantom{V_v} c^2}}{\sqrt{1 - \dfrac{v^2}{c^2}}} \qquad x^\prime = \frac{x - vt}{\sqrt{1 - \dfrac{v^2}{c^2}}}.
\end{equation*}
From now on we spurn $\vv v$. We'll refer to the classical limit not as $v \ll c$, but as $\beta \ll 1$. We regard $\vvbeta$ as the ``natural" way to express velocity. It goes hand in hand with measuring distance and time in the same unit.
 
Because the Lorentz transformation is linear (space and time coordinates are to first power only), it also works for \emph{differences} in the coordinates between two events: just replace the likes of $x$ and $ct$ with those of $\Delta x$ and $c \mkern.5mu \Delta t$ (or of $\dd x$ and $c \, \dd t$ if need be).\footnote{\label{fn:in}In physics it's normal to interpret the derivative as a ratio of infinitesimals. Some find this objectionable, but it's a great heuristic for well-behaved single-variable situations.} We now adopt the standard shorthand $\gamma \equiv (1 - \beta^2)^{-1/2}$ (called the \textbf{Lorentz factor}):
\begin{equation}\label{eq:lt2}
\begin{aligned}
c \mkern.5mu \Delta t^{\prime} &= \gamma \left( c \mkern.5mu \Delta t - \beta \Delta x \right) & c \mkern.5mu \Delta t &= \gamma \left( c \mkern.5mu \Delta t^\prime + \beta \Delta x^\prime \right) \\
\Delta x^{\prime} &= \gamma \left( \Delta x - \beta \mkern1mu c \mkern.5mu \Delta t \right) & \qquad \Delta x &= \gamma \left( \Delta x^\prime + \beta \mkern1mu c \mkern.5mu \Delta t^\prime \right) \\
\Delta y^{\prime} &= \Delta y& \Delta y&= \Delta y^\prime \\
\Delta z^{\prime} &= \Delta z& \Delta z&= \Delta z^\prime ,
\end{aligned}
\end{equation}
with the inverse transformation on the right. Equations \ref{eq:lt2} have slightly broader applicability than Equations \ref{eq:lt}, as they don't require the frames to share a spacetime origin. They're sometimes called the ``inhomogeneous" Lorentz transformation or the Poincar\'e transformation, but we'll usually just call them the Lorentz transformation. Later we'll identify other sets of four quantities that transform together under a Lorentz boost in the same way, and we'll say that they too transform Lorentzianly.

For the sake of completeness, let's derive the transformation for a boost in an arbitrary direction, $\hatbeta = \vvbeta / \beta$. Under standard configuration, Equations \ref{eq:lt} are in play, and we have (going from unprimed frame to primed) ${ \hatbeta = \langle 1, 0, 0 \rangle }$. With the position vector $\vv r = \langle x, y, z \rangle$, we've got ${ \vv r _\parallel = \langle x, 0, 0 \rangle }$ and ${\vv r _\perp = \langle 0, y, z \rangle}$ for the component-vectors of $\vv r$ that are parallel and perpendicular to the boost-direction (respectively the ``projection" of $\vv r$ onto $\hatbeta$ and the ``rejection" of $\vv r$ from $\hatbeta$). Since then $\vv r _\parallel \cdot \hatbeta = x$, substitution gives:
\begin{equation*}
c t^{\prime} = \gamma \big( c t - \beta (\vv r _\parallel \cdot \hatbeta ) \big),
\end{equation*}
which clearly works for \emph{arbitrary} $\hatbeta$. Likewise, we must have $\vv r^{\prime} _\perp = \vv r _\perp$ and (by $\vv r _\parallel = x \langle 1, 0, 0 \rangle = x \hatbeta$):
\begin{equation*}
\vv r^{\prime} _\parallel = \gamma ( \vv r _\parallel - \beta \mkern1mu c t \, \hatbeta ),
\end{equation*}
where again $\hatbeta$ can be arbitrary. Finally, we use $\vv r = \vv r _\parallel + \vv r _\perp$, the vector-projection formula $\vv r _\parallel = (\vv r \cdot \hatbeta) \hatbeta$, and a bit of algebra to condense our new arbitrary-boost relations to two equations without $\vv r _\parallel$ or $\vv r _\perp$:
\begin{equation}\label{eq:lt3}
c t^{\prime} = \gamma \big( c t - \beta ( \vv r \cdot \hatbeta ) \big) \qquad \quad
\vv r^{\prime} = \vv r + \big( (\gamma - 1)(\vv r \cdot \hatbeta) - \gamma \beta c t \big) \hatbeta .
\end{equation}
For the arbitrary-direction version of Equations \ref{eq:lt2}, insert $\Delta$'s.


\subsubsection{Relative Simultaneity, Time Dilation, Length Contraction}\label{sssec:td}

From the Lorentz transformation for differences in the coordinates between events, we can derive many important results. The most novel is probably the inequality of $c \mkern.5mu \Delta t^{\prime}$ and $c \mkern.5mu \Delta t$, which holds even when one of them equals zero, as long as $\Delta x$ and $\Delta x^\prime$ don't also equal zero (this is the \textbf{relativity of simultaneity}). Let's now examine two others that we'll use in this paper: \textbf{time dilation} and \textbf{length contraction}.

When $\Delta x^{\prime} = 0$ in Equations \ref{eq:lt2}, the expression for time dilation emerges: $c \mkern.5mu \Delta t = \gamma \mkern2mu c \mkern.5mu \Delta t^{\prime}$. In the special case that $\Delta x^{\prime} = \Delta y^{\prime} = \Delta z^{\prime} = 0$, the primed time $ct^{\prime}$ corresponds to the time logged by the wristwatch of an inertial traveler (real or imagined) who is physically present at both events. This invariant ``wristwatch time" is what we call the traveler's \textbf{proper time}, and we label it $c \mkern.5mu t_0$. Hence, $c \mkern.5mu \Delta t = \gamma \mkern2mu c \mkern.5mu \Delta t_0$ (where $ct$ is now actually the coordinate time in an \emph{arbitrary} inertial frame, standard configuration or no). For a traveler whose velocity isn't constant, we can still use infinitesimals: $c \, \dd t = \gamma \mkern2mu c \, \dd t_0$ (more on this in Section \ref{sssec:ds}).\footnote{\label{fn:ch}\emph{Can} we just use the infinitesimals? Isn't it possible that time dilation depends not only on relative velocity but also on acceleration? Good question. The \emph{assumption} that time dilation doesn't depend on acceleration is called the \textbf{clock hypothesis}; it's been experimentally confirmed to high precision.}

To derive length contraction, we'll use time dilation.\footnote{If one instead uses the Lorentz transformation directly, one must remember that length is the distance between an object's endpoints \emph{at a single moment in coordinate time}. So in a given inertial frame, an observer measures the spatial separation of two \emph{simultaneous} events---one located at each of the object's endpoints.} Say a ruler has length $L_0$ in its rest frame (this is its \textbf{proper length}), and that a clock moving at speed $\beta$ relative to the ruler traverses its length in a time $c \mkern.5mu \Delta t$ (the ruler's rest frame is unprimed). We then have $\beta = L_0 / (c \mkern.5mu \Delta t)$. In the clock's (primed) rest frame, the entire length of the ruler $L^\prime$ passes the clock's location in a time $c \mkern.5mu \Delta t^\prime$, giving $\beta = L^\prime / (c \mkern.5mu \Delta t^\prime)$. Now we've got two expressions for the relative speed $\beta$. Put them together: $L_0 = L^\prime \mkern2mu c \mkern.5mu \Delta t / (c \mkern.5mu \Delta t^\prime) = \gamma L^\prime$ (by time dilation).\footnote{Be sure you understand how time dilation applies here. The clock is the inertial ``traveler" present at both events (the events being ``clock and one end of ruler coincide" and ``clock and other end of ruler coincide"); in the clock's rest frame (\emph{but not the ruler's}), these two events occur at the same spatial position.} So an object's length when moving is shorter than its proper length by a factor of $\gamma$. Note that an object's \emph{volume} when moving is likewise scaled down from its ``proper volume" by a factor of $\gamma$, since only the length along the axis of relative motion is contracted.


\subsubsection{More on the Spacetime Interval}\label{sssec:sti2}
Before moving on, we'll say a few more things about the spacetime interval $\Delta s = [(c \mkern.5mu \Delta t)^2 - ( \Delta \vv r )^2]^{1/2}$. First, let's confirm that the Lorentz transformation keeps it invariant (if it doesn't, then we made a mistake when deriving the Lorentz transformation from the interval's invariance!). In standard configuration, we can ignore the $y$ and $z$ coordinates without sacrificing generality:
\begin{equation*}
(c \mkern.5mu \Delta t^{\prime})^2 - (\Delta x^{\prime})^2 = \big[ \gamma \left( c \mkern.5mu \Delta t - \beta \Delta x \right) \big] ^2 - \big[ \gamma \left( \Delta x - \beta \mkern1mu c \mkern.5mu \Delta t \right) \big]^2 ,
\end{equation*}
which, after some algebra, yields:
\begin{equation*}
\begin{aligned}
(c \mkern.5mu \Delta t^{\prime})^2 - (\Delta x^{\prime})^2 &= \gamma^2 \left( 1 - \beta^2 \right) \left[ (c \mkern.5mu \Delta t)^2 - ( \Delta x )^2 \right] \\[2pt]
&= (c \mkern.5mu \Delta t)^2 - ( \Delta x )^2 ,
\end{aligned}
\end{equation*}
where we've used the useful identity
\begin{equation}\label{eq:20}
\gamma^{-2} = 1 - \beta^2 .
\end{equation}
So $(\Delta s)^2 = (c \mkern.5mu \Delta t)^2 - ( \Delta \vv r )^2$ remains invariant. Since it can be negative, positive, or zero, we have three ``categories" of interval:
\begin{itemize}
\item When $(\Delta s)^2<0$, the interval is \textbf{spacelike}, and $\lvert (\Delta s)^2 \rvert ^{1/2} = \Delta s / \mathrm{i}$ represents the \textbf{proper distance} between two events, which is their spatial separation in an inertial frame for which they occur simultaneously (there are infinitely many such frames, since motion perpendicular to the line connecting the locations of simultaneous events doesn't ``break" their simultaneity). Nothing---not even a light signal---can travel from one event to the other. The events cannot have a causal relationship, and their chronological order depends on one's frame of reference.
\item When $(\Delta s)^2>0$, the interval is \textbf{timelike}, and $\lvert(\Delta s)^2\rvert ^{1/2} = \Delta s $ represents the \textbf{inertial proper time interval} between the events, which is the proper time $c \mkern.5mu \Delta t_0$ that elapses for an inertial traveler who journeys from one event to the other.\footnote{\label{fn:pma}The qualifier \emph{inertial} is crucial because non-inertial journeys between timelike-separated events are possible, too. But among all travelers who make the journey, it's the inertial one who ages the most en route (we won't show this until Section \ref{sssec:ele}). This maximal aging---the inertial traveler's elapsed proper time---is the spacetime interval between the events.} Equivalently, it's the coordinate time that elapses between the events in the unique inertial frame for which they occur at the same location. The events may be causally related, and their chronological order is fixed.
\item When $(\Delta s)^2=0$, the interval is \textbf{lightlike}, meaning that there's no inertial frame in which the events occur simultaneously or at the same location. Only a signal moving at the speed of light can travel from one event to the other. The events may be causally related (by such a signal), and their chronological order is fixed.
\end{itemize}


\subsubsection{Velocities, Angles, and the Aberration of Light}\label{sssec:va}
In deriving the relativistic transformation rules for velocity components, we must not confuse the relative speed of two inertial observers in standard configuration with their measurements of some third party's velocity. To that end, let's append the subscript ``rel" to the observers' relative speed for now ($\beta_{\textrm{rel}}$), and also to the corresponding Lorentz factor: $\gamma_{\textrm{rel}} = (1 - \beta_{\textrm{rel}}^2)^{-1/2}$. Then $\vvbeta$ and $\vvbeta^{\prime}$ are the observers' measurements of the third party's velocity. To find how the components of $\vvbeta$ transform into those of $\vvbeta^{\prime}$ (and vice versa), we divide $\Delta x^\prime$ and $\Delta x$ (etc.) by $c \mkern.5mu \Delta t^\prime$ and $c \mkern.5mu \Delta t$, respectively, and evaluate the limits of the quotients as $c \mkern.5mu \Delta t^\prime \rightarrow 0$ and $c \mkern.5mu \Delta t \rightarrow 0$ (equivalent conditions, since we're not dealing with spacelike-separated events). For example:
\begin{equation*}
\begin{aligned}
\beta_x^\prime &\equiv \dfrac{\dd x^\prime}{\dd (ct^\prime)} = \lim_{\substack{c \mkern.5mu \Delta t^\prime \to 0 \\[1.5pt] [c \mkern.5mu \Delta t \to 0]}} \, \frac{\Delta x^\prime}{c \mkern.5mu \Delta t^\prime}\\[6pt]
&= \lim_{\substack{\\ c \mkern.5mu \Delta t \to 0}} \, \frac{\gamma_{\textrm{rel}} \left( \Delta x - \beta_{\textrm{rel}} \, c \mkern.5mu \Delta t \right)}{\gamma_{\textrm{rel}} \left( c \mkern.5mu \Delta t - \beta_{\textrm{rel}} \, \Delta x \right)}
\end{aligned}
\end{equation*}
(by Equations \ref{eq:lt2}), and dividing numerator and denominator by $c \mkern.5mu \Delta t$:
\begin{equation*}
\begin{aligned}
\beta_x^\prime &= \frac{\lim\limits_{\substack{\\ c \mkern.5mu \Delta t \to 0}} \left( \dfrac{\Delta x}{c \mkern.5mu \Delta t} - \beta_{\textrm{rel}} \right) \hfill}{\lim\limits_{\substack{\\ c \mkern.5mu \Delta t \to 0}} \left( 1 - \beta_{\textrm{rel}} \, \dfrac{\Delta x}{c \mkern.5mu \Delta t} \right) }\\[7pt]
&= \frac{\beta_x - \beta_{\textrm{rel}}}{1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}}},
\end{aligned}
\end{equation*}
and so on. Or we can take a shortcut: sidestep the limits and just ``divide" the infinitesimals in the first place (see Footnote \ref{fn:in}). Either way, we end up with the following transformation for velocity components:
\begin{equation}\label{eq:vt}
\begin{aligned}
\beta_x^{\prime} &= \frac{\beta_x - \beta_{\textrm{rel}}}{1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}}}& \qquad \beta_x &= \frac{\beta_x^{\prime} + \beta_{\textrm{rel}}}{1 + \beta_x^{\prime} \mkern1mu \beta_{\textrm{rel}}}\\[5pt]
\beta_y^{\prime} &= \frac{\beta_y}{\gamma_{\textrm{rel}} \left( 1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}} \right)}& \qquad \beta_y &= \frac{\beta_y^{\prime}}{\gamma_{\textrm{rel}} \left( 1 + \beta_x^{\prime} \mkern1mu \beta_{\textrm{rel}} \right) }\\[5pt]
\beta_z^{\prime} &= \frac{\beta_z}{\gamma_{\textrm{rel}} \left( 1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}} \right)}& \qquad \beta_z &= \frac{\beta_z^{\prime}}{\gamma_{\textrm{rel}} \left( 1 + \beta_x^{\prime} \mkern1mu \beta_{\textrm{rel}} \right)}.
\end{aligned}
\end{equation}

These vector components can be positive or negative, though standard configuration guarantees that $ \beta_x \geq \beta_x^\prime $.\footnote{Equal only if they're both $1$ or $-1$, as with a light wave moving parallel to the $x^{[\prime]}$-axis.} We see that even the components of velocity \emph{perpendicular} to the observers' axis of relative motion are nontrivially transformed. This is in contrast to classical Galilean relativity, where $v_y^{\prime} = v_y$ and $v_z^{\prime} = v_z$. In both theories, of course, the unprimed observer measures a different angle $\theta$ between $\vvbeta$ and the positive $x$-direction than the primed observer does ($\theta^\prime$) between $\vvbeta^{\prime}$ and the positive $x^\prime$-direction (assuming the angle isn't $0$ or $\pi$ in both frames). And in both theories, relationships between $\theta$ and $\theta^{\prime}$ can be found with simple trigonometry and the appropriate transformation of velocity components. Using the cosine function and Equations \ref{eq:vt}, we get for special relativity:
\begin{equation*}
\begin{aligned}
\beta^\prime \cos \theta^\prime &= \beta_x^\prime &\qquad \beta \cos \theta &= \beta_x \\[3pt]
&= \dfrac{\beta_x - \beta_{\textrm{rel}}}{1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}}} &\qquad &= \dfrac{\beta_x^\prime + \beta_{\textrm{rel}}}{1 + \beta_x^\prime \mkern1mu \beta_{\textrm{rel}}} \\[3pt]
\beta^\prime \cos \theta^\prime &= \frac{\left( \beta \cos \theta \right) - \beta_{\textrm{rel}}}{1 - \beta_{\textrm{rel}} \left( \beta \cos \theta \right)} &\qquad \beta \cos \theta &=\frac{\left( \beta^\prime \cos \theta^\prime \right) + \beta_{\textrm{rel}}}{1 + \beta_{\textrm{rel}} \left( \beta^\prime \cos \theta^\prime \right)} .
\end{aligned}
\end{equation*}
When the third party whose velocity is being measured is a light wave, we have $\beta = \beta^\prime = 1$, and we obtain the formula for the \textbf{aberration of light}:
\begin{equation}\label{eq:7}
\cos \theta^\prime =\frac{\cos \theta - \beta_{\textrm{rel}}}{1 - \beta_{\textrm{rel}} \cos \theta} \qquad \cos \theta =\frac{\cos \theta^\prime + \beta_{\textrm{rel}}}{1 + \beta_{\textrm{rel}} \cos \theta^\prime}.
\end{equation}

Equations \ref{eq:7} are pertinent to Einstein's thought experiment because source and moving observer don't agree on the angle between an emitted light wave and the axis of their relative motion. In the observer's frame, the light is ``tilted" toward the direction of the source's velocity vector, and by more than Galilean relativity would predict. Near the end of Section \ref{sssec:tc}, we alluded qualitatively to the fact that two light waves emitted in opposite directions according to the source do \emph{not} generally travel in opposite directions in other frames. Now we can actually crunch the numbers, even for observers moving near the speed of light relative to the source.

Because all traces of $\vvbeta$ and $\vvbeta^{\prime}$ have disappeared, we're free to drop the ``rel" subscript when we apply Equations \ref{eq:7}.


\subsubsection{Relativistic Doppler Effect}\label{sssec:rdf}
Imagine two frames of reference in standard configuration once again. This time, a source of light at rest with respect to the unprimed frame moves with normalized speed $\beta$ (and Lorentz factor $\gamma$) in the negative $x^\prime$-direction, and an observer sits stationary at the primed origin. (The source need not travel \emph{on} the $x^\prime$ axis---only parallel to it.)

For a brief moment, the source emits light directly toward the observer. To the observer, the velocity vector of the approaching light makes an angle $\theta^\prime$ with the positive $x^\prime$-direction (obtuse if the source is approaching, acute if the source is receding). To the source, the light's velocity vector makes an angle $\theta$ with the positive $x$-direction (which is also the direction of the observer's motion---remember this). Thus, the angles $\theta^\prime$ and $\theta$ are related by Equations $\ref{eq:7}$ (substitute $\beta$ for $\beta_{\textrm{rel}}$).

In the source's frame, the light oscillates with a period $c \mkern.5mu \Delta t$ equal to its wavelength $\lambda$. By time dilation, the period in the \emph{observer's} frame is $c \mkern.5mu \Delta t^\prime = \gamma \lambda$. During this time interval $\gamma \lambda$, the light travels toward the observer a distance $\gamma \lambda$, while the \emph{source} is displaced a distance $\gamma \lambda \, \beta$ in the negative $x^\prime$-direction. But we only care about the \emph{radial component} of the source's displacement (i.e., the component that's parallel to the light's path toward the observer at the primed origin, defined as negative if it's directed toward the origin and positive if it's directed away). A bit of trigonometry tells us that this radial displacement is $\gamma \lambda (\beta \cos \theta^\prime)$, regardless of whether the source is on the negative or positive $x^\prime$-side of the $y^\prime$/$z^\prime$-plane. To see why, realize that $\theta^\prime$ \emph{is} the angle between the source's displacement and the outward radial in either case. If the source is on the negative $x^\prime$-side (receding from the observer), our radial component is positive because $\cos \theta^\prime$ is positive; and if the source is on the positive $x^\prime$-side (approaching the observer), the radial component is negative. (If the source is \emph{on} the $y^\prime$/$z^\prime$-plane, then the radial component is zero because $\cos \theta^\prime = 0$.)

So in the observer's frame, the source emits the light wave's second peak a time $\gamma \lambda$ after emitting the first, but the source has also moved toward or away from the observer a distance $| \gamma \lambda \, \beta \cos \theta^\prime |$ during that time. The second peak therefore trails the first by a distance equal to the sum of $\gamma \lambda$ (the distance light travels in the observer's frame between peak-emissions) and $\gamma \lambda \, \beta \cos \theta^\prime$ (the appropriately signed radial component of the source's concurrent displacement).\footnote{To keep things simple, we've neglected any change in $\theta^\prime$ during the time interval $\gamma \lambda$. This is no crime, but we should acknowledge that the angle in this scenario does in fact change continuously (unless it's $0$ or $\pi$)---\emph{even over the course of a single cycle}. Strictly speaking, we should be working with infinitesimals, not finite displacements.} Naturally, that trailing distance is just the wavelength $\lambda^\prime$ measured in the observer's frame:
\begin{equation*}
\begin{aligned}
\lambda^\prime &= \gamma \lambda + \gamma \lambda \, \beta \cos \theta^\prime \\
&= \gamma \lambda \left( 1 + \beta \cos \theta^\prime \right).
\end{aligned}
\end{equation*}

For our purposes, it will be more useful to express this \textbf{relativistic Doppler formula} in terms of the unprimed angle $\theta$ (measured in the \emph{source's} frame), and in terms of photon energy. By Equations \ref{eq:7} (aberration):
\begin{equation*}
\begin{split}
\lambda^\prime &= \gamma \lambda \left( 1 + \beta \, \frac{\cos \theta - \beta}{1 - \beta \cos \theta} \right) \\[3pt]
&= \gamma \lambda \left( \frac{ 1 - \beta \cos \theta + \beta \cos \theta - \beta^2 }{1 - \beta \cos \theta} \right) ,
\end{split}
\end{equation*}
which by Equation \ref{eq:20} is:
\begin{equation}\label{eq:rdw}
\lambda^\prime = \frac{\lambda}{\gamma \left( 1 - \beta \cos \theta \right)}.
\end{equation}
The frequency is inversely proportional to the wavelength, so:
\begin{equation}\label{eq:rdf}
\nu^{\prime} = \gamma \mkern1mu \nu \left( 1- \beta \cos\theta \right).
\end{equation}
And by Equation \ref{eq:4} for photons:
\begin{equation}\label{eq:8}
\boxed{E^{\prime} = \gamma E \left(1-\beta \cos\theta\right)} \quad \text{\footnotesize{ (for light)}} .
\end{equation}
Bingo! In words: if we know the energy $E$ of (the photons in) a light wave as measured in the source's frame, as well as the angle $\theta$ that the light's direction of propagation makes with a moving observer's velocity vector $\vvbeta$ (again, as measured in the source's frame), then we can calculate the energy $E^\prime$ of (the photons in) the light wave as measured in the \emph{observer's} frame. As you may suspect, Equation \ref{eq:8} will be the key to overcoming our earlier impasse.

Actually, our photonic parentheticals in the previous paragraph weren't necessary: Equation \ref{eq:8} works for Maxwell's wave model of light, too, though demonstrating this requires digging a bit deeper into classical electromagnetism (we'll do this in Section \ref{sec:rem}). Heuristically, we can reason that if the energies of all the photons transform this way, then the energy of the light wave must also. As promised when we introduced Equation \ref{eq:4}, we've used the Planck--Einstein relation to take a shortcut. From now on we'll drop the photon talk.

If you're wondering how the observers in Einstein's thought experiment can possibly detect both waves (because nobody can be in more than one place at once!), recall that a frame of reference isn't just a single observer. We pretend that each inertial frame is full of magically incorporeal measuring devices: a grid of arbitrarily precise rulers, synchronized clocks, and whatever else might come in handy, like spectrometers for analyzing light waves passing through. Every measurement is made at a place and time specified by the imagined clock-and-ruler lattice, and everyone \emph{at rest in that frame} agrees on the measurement's where and when. Further, a measurement like $E^\prime$ in Equation \ref{eq:8} that depends only on relative velocity is \emph{itself} valid for the whole frame.

So when we apply Equation \ref{eq:8} to our thought experiment, we can arrange for the moving observer to personally detect one of the light waves with a spectrometer, but then the other wave must be detected by a \emph{second} spectrometer elsewhere in the frame, along the wave's path. Or we can disregard the observer altogether and simply speak of two spectrometers. It doesn't matter. The important thing is that the spectrometers move with the same velocity $\vvbeta$ relative to the light-emitting body. And since the light waves propagate in opposite directions from the body's perspective, the angles they form with $\vvbeta$ must differ by $\pi$. How convenient! The cosines of such angles sum to zero (i.e., $\cos \mkern1.5mu (\theta+\pi)=-\cos\theta$), and that's just what we'll need for all the $\theta$'s to disappear when we use Equation \ref{eq:8} to add the $E^{\prime}$ of one light wave to the $E^{\prime}$ of the other. Apparently this Einstein fellow knew what he was doing. But we're getting ahead of ourselves.


\subsection[Einstein's Thought Experiment \emph{with} Einstein: Relativistic Energy and $m=E_0/c^2$]{Einstein's Thought Experiment \emph{with} Einstein:\\Relativistic Energy and $\bm{m=E_0/c^2}$}\label{ssec:re}

Let's take stock. When we analyzed the thought experiment without special relativity, we hit a roadblock: on the one hand, the additivity of mass implies that the observers should agree that the light waves have equal-and-opposite momenta and carry away a combined energy equal to $-\Delta E_0$ (the rest energy lost by the body); on the other hand, Maxwell and Doppler say that \emph{only} our rest observer can find that to be the case. Now we have a path forward---abandon the assumption that mass is additive, prioritize Maxwell and Doppler, and retrace Einstein's steps (more or less) in deriving a new \emph{relativistic} equation for kinetic energy. Then we'll tackle momentum.

We stand by the logic behind Equations \ref{eq:5} and \ref{eq:6}. The concept of invariant rest energy is crucial. In fact, Einstein formulated it in precisely the same context that we did. He didn't draw attention to it, but his whole ``$E=mc^2$" derivation hangs on it. In light of the relativity of simultaneity, however, we must be careful when discussing the rest energy of an \emph{open} composite system. If the body in our thought experiment is an extended object that emits the two light waves from its extremities simultaneously \emph{in its own rest frame}, then in general the moving observer will say that the waves are \emph{not} emitted simultaneously. The exception is when the moving observer's trajectory is perpendicular to the waves' velocity vectors according to the body, but aside from that special case, the moving observer will say that some time elapses between the emissions of the waves. ``During" that gap, the observers do \emph{not} agree on the body's rest energy! \emph{After} the gap, they do. So when we use $\Delta E_0$ below, we'll mean the difference in the body's rest energy before and after the emission of both waves for both observers. That way it's invariant.

Now, we're looking for a speed-dependent expression for $E_{\mkern.5mu \textrm{k}}$ that reduces to Equation \ref{eq:2} in the classical limit but works for arbitrary $\beta$. To find it, we'll apply Equations \ref{eq:5} and \ref{eq:8} to Einstein's thought experiment.

We know that for the observer at rest relative to the body, the equal-but-opposite light waves carry combined energy $-\Delta E_0$. If we say that each light wave carries energy $E_{\ell}$ in the body's rest frame ($\ell$ for light), then we have:
\begin{equation}\label{eq:9}
2E_{\ell}=-\Delta E_0.
\end{equation}
Simple.

What about the observer moving relative to the body?

Still pretty simple. As we concluded at the end of the previous section, the angle $\theta$ that we use in Equation \ref{eq:8} for one light wave must be $(\theta + \pi)$ for the other. We don't assume that the waves carry equal energy in this frame---actually, Equation \ref{eq:8} says that they don't, unless $\theta = \pm \, \pi / 2$---so we distinguish their energies as $E_{\ell \, 1}^{\prime}$ and $E_{\ell \, 2}^{\prime}$. From Equations \ref{eq:8} and \ref{eq:9}:
\begin{equation}\label{eq:10}
\begin{split}
E_{\ell \, 1}^{\prime} + E_{\ell \, 2}^{\prime} &= \gamma E_{\ell} \left(1-\beta\cos\theta \right) + \gamma E_{\ell} \big[ 1 - \beta \cos \mkern1.5mu ( \theta + \pi ) \big] \\[2pt]
&= \gamma E_{\ell} \left(1 - \beta\cos\theta + 1 + \beta\cos\theta \right) \\[2pt]
&= 2 \gamma E_{\ell} \\[3pt]
E_{\ell \, 1}^{\prime} + E_{\ell \, 2}^{\prime} &= - \, \gamma \Delta E_0.
\end{split}
\end{equation}

Now we have $E_{\ell \, 1}^{\prime} + E_{\ell \, 2}^{\prime}$ in terms of $\Delta E_0$ and $\beta$ (remember that $\gamma$ is a function of $\beta$). So after our inertial body emits the light waves without changing its velocity, all observers agree on the amount of rest energy it's lost, but moving observers measure the light waves as carrying more (combined) energy than observers in the body's rest frame do---by the Lorentz factor, in fact.\footnote{Don't mistake this for a blanket statement about the relativistic Doppler effect. Equation \ref{eq:8} certainly allows for a moving observer to measure a light wave as \emph{less} energetic than an observer in the body's rest frame does. Indeed, $E_{\ell \, 1}^\prime$ or $E_{\ell \, 2}^\prime$ may be less than $E_{\ell}$, but then the other is greater than $E_{\ell}$ by a larger amount such that $E_{\ell \, 1}^{\prime} + E_{\ell \, 2}^{\prime} > 2E_{\ell}$.} We'll have to account for that extra energy, since conservation demands that our moving observer measure equal values for the total energy lost by the body ($-\Delta E^{\prime}$) and the energy carried away by the light:
\begin{equation}\label{eq:11}
E_{\ell \, 1}^{\prime} + E_{\ell \, 2}^{\prime}=-\Delta E^{\prime}.
\end{equation}
Here comes the fun part. Substitute Equation \ref{eq:11} into Equation \ref{eq:10}:
\begin{equation*}
\Delta E^{\prime} = \gamma \Delta E_0,
\end{equation*}
and drop the deltas (kosher if we throw in a constant---call it $C$):
\begin{equation*}
E^{\prime} = \gamma E_0 + C.
\end{equation*}
When $\beta = 0$ (i.e., when $\gamma = 1$), that's just $E^{\prime} = E_0 + C$, but by Equation \ref{eq:6} we know that $E^{\prime} = E_0$ in that case, so $C=0$:
\begin{equation*}
E^{\prime} = \gamma E_0.
\end{equation*}

Look at this beautiful new equation for total energy! It might not be immediately apparent, but that result is good for \emph{any} system with a rest frame.\footnote{As mentioned in Footnote \ref{fn:rl}, some systems are ``restless" and have no rest frame or rest energy. We'll discuss these in Section \ref{ssec:em}.} Why? Because there's nothing special about the energy carried by electromagnetic radiation. Energy is energy, so we would have arrived at this equation regardless of what mechanism caused the body to lose some of its $E_0$. Moreover, this result holds \emph{even if the body's velocity is changing}. Calculus permits us to model an accelerating body's trajectory as a succession of infinitesimally short mini-trajectories. For every such mini-trajectory, the body's $\gamma$ has an instantaneous value in a given frame, and our new relation momentarily applies with that value.\footnote{It's a common misconception that analyzing accelerated motion requires the mathematical tools of general relativity. Basic calculus suffices, in both classical mechanics and special relativity. More on these ``instantaneous" mini-trajectories in Section \ref{sssec:ds}.} This is why Einstein's setup is so clever. Its symmetry facilitates analysis but leads to general results. It doesn't matter how a system's $\gamma$ and $E_0$ came to be what they are, or whether (or why) they're changing over time; at any moment, an inertial observer can simply use their current values to calculate the total energy. Since our result is a general one, let's ditch the prime symbol and make it official:
\begin{equation}\label{eq:12}
\boxed{E = \gamma E_0} \quad \textrm{\footnotesize{ (for $E_0 \neq 0$)}}.
\end{equation}
When $\gamma=1$ ($\beta=0$), Equation \ref{eq:12} becomes Equation \ref{eq:6}. And as $\beta \rightarrow 1$, $E$ increases without bound. As for kinetic energy, which suddenly seems like an afterthought, by Equations \ref{eq:5} and \ref{eq:12} we have:
\begin{equation}\label{eq:13}
\boxed{E_{\mkern.5mu \textrm{k}} = E_0 (\gamma - 1)} \quad \textrm{\footnotesize{ (for $E_0 \neq 0$)}}.
\end{equation}
Done.

Note that kinetic energy now depends on velocity and \emph{rest energy}. That solves our problem, at least as far as energy is concerned: from the perspective of our moving observer, the body's velocity doesn't change when it emits the light waves, but its rest energy does, which means that its non-zero \emph{kinetic} energy does, too. So all observers agree on the body's $\Delta E_0$, but moving observers also measure a decrease in its $E_{\mkern.5mu \textrm{k}}^\prime$. And where did that extra energy go? Into the light waves! That's what Einstein's relativistic Doppler shift told us. The moving observer measures the light waves in toto as \emph{more energetic} than the rest observer does, and that ``extra" energy is exactly counterbalanced by the measured decrease in the body's kinetic energy: $-\Delta E_{\mkern.5mu \textrm{k}}^\prime = (E_{\ell \, 1}^{\prime} + E_{\ell \, 2}^{\prime}) - 2E_{\ell}$. For both observers, then, total energy of the closed system is conserved.

Equation \ref{eq:12} should remind you of $c \mkern.5mu \Delta t=\gamma c \mkern.5mu \Delta t_0$, the equation for time dilation (where $ct_0$ is proper time). In fact, kinetic energy is a ``relativistic effect" in precisely the way that time dilation is. You can tell from Equations \ref{eq:12} and \ref{eq:13} that when $\beta \ll 1$, $E \approx E_0$ and $E_{\mkern.5mu \textrm{k}} \rightarrow 0$, just as $c \mkern.5mu \Delta t \approx c \mkern.5mu \Delta t_0$ and $c \mkern.5mu \Delta t - c \mkern.5mu \Delta t_0 \rightarrow 0$. Even at speeds we'd consider extremely fast by everyday standards, a body's rest energy dwarfs its kinetic energy. Only when $\beta \centernot{\ll} 1$ does kinetic energy contribute appreciably to total energy, and it doesn't exceed rest energy until $\beta > \sqrt{3}/2 \approx .866$, which you can verify by setting $E_{\mkern.5mu \textrm{k}}=E_0$ in Equation \ref{eq:13}. By the same token, only when $\beta > \sqrt{3}/2$ is $c \mkern.5mu \Delta t - c \mkern.5mu \Delta t_0 > c \mkern.5mu \Delta t_0$.

But there's an important practical difference between time dilation and kinetic energy: although each becomes negligible in the classical limit \emph{as a proportion of its corresponding ``total" quantity} ($c \mkern.5mu \Delta t$ and $E$, respectively), we have no difficulty detecting kinetic energy when $\beta \ll 1$, as Equation \ref{eq:2} makes clear.\footnote{It's actually the gargantuan \emph{rest energy} that escaped the notice of physicists before Einstein! But it was hiding in plain sight, going by another name \dots} So once Einstein derived Equation \ref{eq:13}, his final step was to compare its (non-zero!) low-$\beta$ approximate value to Equation \ref{eq:2}. He did this by using the binomial theorem to expand the Lorentz factor $\gamma = (1-\beta^2)^{-1/2}$. Fittingly, it was Newton who developed the procedure for arbitrary exponent:
\begin{equation*}
(a+x)^n = a^n + na^{n-1}x + \frac{n(n-1)}{2!} \, a^{n-2}x^2 + \frac{n(n-1)(n-2)}{3!} \, a^{n-3}x^3 + \dots ,
\end{equation*}
convergent if $|x/a| < 1$. In our case, $a=1$, $x=-\beta^2$, and $n=-1/2$. Because our exponent isn't a positive integer, we end up with an infinite series:
\begin{equation*}
\left(1-\beta^2\right)^{-1/2}=1 + \dfrac{1}{2}\,\beta^2 +  \frac {3}{8} \, \beta^4 + \frac{5}{16} \, \beta^6 + \frac{35}{128} \, \beta^8 + \dots
\end{equation*}
Usually we'll use $\gamma \approx 1$ when speaking of the ``classical limit," but that would lead to $E_{\mkern.5mu \textrm{k}} \approx 0$ here, which won't do. Instead we include the second term, and we can stop there since $\beta^2 \gg \beta^4$ when $\beta \ll 1$:
\begin{equation*}
\gamma \approx 1 + \dfrac{1}{2}\,\beta^2.
\end{equation*}
Now we plug that into Equation \ref{eq:13}:
\begin{equation}\label{eq:14}
\begin{split}
E_{\mkern.5mu \textrm{k}} &=E_0 (\gamma - 1)\\[5pt]
&\approx E_0 \left[ \left( 1 + \dfrac{1}{2}\,\beta^2 \right) -1 \right] \\[5pt]
E_{\mkern.5mu \textrm{k}} &\approx \frac{1}{2} \mkern2mu E_0 \mkern.5mu \beta^2 \quad \textrm{\footnotesize{ (for $\beta \ll 1$)}}.
\end{split}
\end{equation}
Equating Equations \ref{eq:2} and \ref{eq:14} then gives us Einstein's famous result,
\begin{equation}\label{eq:15}
\boxed{m = \frac{E_0}{c^2}} \, ,
\end{equation}
better known as ``$E = mc^2$" (mind the subscript).\footnote{Here's how Einstein originally put it: ``If a body gives off the energy $L$ in the form of radiation, its mass diminishes by $L / c^2$. \ldots The mass of a body is a measure of its energy-content."} Excellent!

Equation \ref{eq:15} does \emph{not} mean that mass and rest energy can be ``converted" into one another. It means that they're \emph{exactly the same thing} expressed in different units, like $\vvbeta$ and $\vv v$ are. Therefore anything we know about one of them applies to the other. Notably:
\begin{itemize}
\item Mass is an invariant that quantifies a system's total energy in its rest frame. A ``restless" system is a massless system (see Footnote \ref{fn:rl}).
\item Since rest energy isn't additive, mass isn't additive either, though they're approximately so in the classical limit (see Footnote \ref{fn:ad}). We surmised as much in Section \ref{sssec:tc}, but now we see why.
\item Similarly, we now see why the body in Einstein's thought experiment lost mass when it emitted the massless light waves: it lost rest energy, and mass \emph{is} rest energy! We must abandon the idea that mass is the ``amount of matter" (an ill-defined concept in the first place).
\item The mass of everything around us comes almost entirely from the kinetic and potential energies ``inside" nucleons, rather than from the masses of elementary particles (see Footnote \ref{fn:mwm}).
\item We were looking for a way to measure a system's rest energy (see Footnote \ref{fn:re}). Well, now we have one: weigh it!
\item Since mass is the resistance to change in velocity in Newtonian physics, so is rest energy. We'll discuss this more in the following section.
\item Since mass is the gravitational ``charge" in Newtonian physics, so is rest energy: $f_{\mathrm{g}} = G(m_a m_b)/r^2 = (G/c^4)(E_{0 \mkern.5mu a} E_{0 \mkern.5mu b})/r^2$.\footnote{With energy-units, the dimensional analysis here becomes ${E/L = (L/E) (E^2 / L^{2})}$, which is ``cleaner" than the mass-based ${ML/T^{2} = (L^{3}/(MT^{2})) (M^2/L^{2})}$. The constant $G / c^4$ (the inverse Planck force) also appears in Einstein's field equations.}
\end{itemize}

The \textbf{equivalence of mass and rest energy} presents us with a redundancy. Keeping them both around and treating them as different concepts is kind of like reserving the word \emph{temperature} for thermometer-readings in Fahrenheit and \emph{hotness} for thermometer-readings in Celsius, and carrying on as though the distinction were physically meaningful. We certainly \emph{could} take a ``more the merrier" approach and use them both, but why should we when our purpose is to lay down the foundation of relativistic mechanics? Let's take a ``less is more" approach instead, and stick with $E_0$. We're kicking $m$ to the curb with $\vv v$. Of course, we can always replace $E_0$ and $\vvbeta$ with $mc^2$ and $\vv v/c$ if the need or inclination arises.


\subsection{Relativistic Momentum}\label{ssec:rm}

Using Equation \ref{eq:15} and $\vv v = \vvbeta c$, we can rewrite Equation \ref{eq:1} with $E_0$ and $\vvbeta$:
\begin{equation}\label{eq:16}
\vv p c \approx E_0 \vvbeta \quad \textrm{\footnotesize{ (for $\beta \ll 1$)}}.
\end{equation}
We've put the ``spare" $c$ on the left side so that the equation has the units of energy ($\vvbeta$ is dimensionless, remember). We'll find it convenient to express energy and momentum in the same unit---they're our additive-and-conserved scalar and vector!---so from now on it's always $\vv p c$ for us, never $\vv p$ alone.\footnote{We could go further and introduce a new \emph{symbol} that equals $\vv p c$, allowing us to carry around fewer $c$'s (like we already do with $\beta = v/c$). We could do the same for $ct$, moreover. Tempting as this is, time and momentum are so central that I think we'd better not. Besides, it's the $c^{-1}$'s that are the \emph{real} eyesores, and we'll generally only make ``new symbols" for the purpose of hiding those (we'll make one for power in Section \ref{sssec:ff}).}

Now, we'd like to find a definition of $\vv p c$ that works for \emph{arbitrary} $\vvbeta$. If we're being good epistemologists, we must admit that we don't yet know whether such a universal quantity even exists! Assuming it does, it will help to ask ourselves, ``What \emph{is} momentum, anyway?" First and foremost, it's our motion-related additive-and-conserved vector, and we'll insist that it remain so as we try to generalize it. Consequently, we can't posit an upper limit on the magnitude of $\vv p c$ like we already have with $\beta$. To see why, consider what additivity entails: a system of two particles with identical momenta must have a magnitude of momentum twice that of either particle alone; and if we double the number of particles with identical momenta in the system, then the system's magnitude of momentum doubles also, ad infinitum. So Equation \ref{eq:16} will have to change to allow $p c$ to grow arbitrarily large as $\beta \rightarrow 1$. Something must replace the invariant $E_0$, because the current limitation $pc < E_0$ precludes additivity.

Good. What else do we know about momentum?

In Newtonian mechanics, momentum is velocity scaled by mass, which acts as the \emph{resistance to change} in velocity. Equation \ref{eq:16} expresses the same relationship in different units: in the classical limit, momentum (in energy units) is (normalized) velocity $\vvbeta$ scaled by rest energy $E_0$. Because the tendency to resist change in velocity is called \textbf{inertia}, we might say that rest energy (or mass) is the measure of inertia when $\beta \ll 1$, though ``measure of inertia" isn't really a technical term.

What about when $\beta \centernot{\ll} 1$? Can rest energy be the measure of inertia at \emph{all} relative speeds? If by ``measure of inertia" one indeed means ``resistance to change in velocity," then the answer is \emph{no}! Were resistance to change in velocity invariant, raising a system's $\beta$ from $0$ to $0.001$ would raise it from $0.999$ to $1$ in some other frame. Since nothing can be accelerated to the speed of light, that's a nonstarter.

Along similar lines, whatever takes the place of $E_0$ in the generalization of Equation \ref{eq:16} must \emph{increase without bound} as $\beta \rightarrow 1$. Otherwise we'd have superluminal velocities at sufficiently large $p c$. So this $E_0$-replacement reduces to $E_0$ in the classical limit but is also some monotonic function of speed. That being the case, \emph{momentum and velocity cannot be directly proportional like they are in Newtonian mechanics}.

It follows, by the way, that a \emph{change} in momentum over time (``force") does \emph{not} imply a simple proportional change in velocity as it does when $\beta \ll 1$. We won't actually do that calculus for a while (see Sections \ref{sssec:pf} and \ref{ssec:in}), but already we see that special relativity complicates the relationship between force and acceleration. For now we'll withhold judgment on whether the mystery $E_0$-replacement we're looking for can reasonably be called the ``measure of inertia" for arbitrary $\vvbeta$.

As for the matter at hand, we \emph{can} make an educated guess about our mystery quantity. After all, we've already encountered a (very important additive-and-conserved) monotonic function of speed that both reduces to $E_0$ in the classical limit and increases without bound as $\beta \rightarrow 1$: total energy $E$ (see Equation \ref{eq:12}). If our educated guess is correct, then relativistic momentum is $\vv p c = E \vvbeta = \gamma E_0 \vvbeta$. Are we on the right track? Well, $E \vvbeta$ certainly reduces to Equation \ref{eq:16} when $\beta \ll 1$, because then $\gamma \approx 1$. Explicitly:
\begin{equation*}\begin{split}
\vv p c &\stackrel{?}{=}\gamma E_0 \vvbeta \\
&= E_0 \mkern.5mu \beta \hatbeta \left( 1 + \dfrac{1}{2} \, \beta ^2 +  \dfrac {3}{8} \, \beta ^4 + \dots  \right) \\[3pt]
&= E_0 \hatbeta \left( \beta + \dfrac{1}{2} \, \beta^3 + \dots \right)
\end{split}\end{equation*}
(where $\hatbeta$ is the unit vector $\vvbeta / \beta$). Since $\beta \gg \beta^3$ when $\beta \ll 1$, we ignore everything but the first term and recover Equation \ref{eq:16}.

Okay, but is the vector $E \vvbeta$ conserved? There's only one way to know for sure: put it to the test.\footnote{If it's \emph{assumed} that there exists an additive-and-conserved vector quantity that reduces to Equation \ref{eq:16} in the classical limit, then it's possible to show that that vector is $E \vvbeta$ (or, in ``momentum" units, $E \vvbeta/c = \gamma m \vv v$), and not some other vector that happens to behave correctly at the limiting values of $\beta$. Some textbooks do this via the ``Lewis and Tolman" thought experiment involving an elastic collision. But conservation laws are ultimately \emph{empirical} results. That goes for the conservation of total energy, too: we derived Equations \ref{eq:12} and \ref{eq:13} by \emph{assuming} that energy is conserved in special relativity. (Don't worry, it is.)} Easier said than done! Fortunately, we don't have to do it ourselves; particle accelerators run such experiments daily, with values of $\beta$ that approach one. The result? $E \vvbeta$ is conserved every time! So let's make it official:
\begin{equation}\label{eq:17}
\boxed{\vv p c = \gamma E_0 \vvbeta}  \quad \textrm{\footnotesize{ (for $E_0 \neq 0$)}},
\end{equation}
or
\begin{equation}\label{eq:18}
\boxed{\vv p c = E \vvbeta} \, ,
\end{equation}
where $E$ reduces to $E_0$ in the classical limit (cf.\ Equation \ref{eq:16}).\footnote{Notice that Equation \ref{eq:18} has no $E_0 \neq 0$ stipulation? Be suspicious but read on.}

We can apply what we've learned to Einstein's thought experiment. Equation \ref{eq:17} tells us that in the moving observer's frame, the body's velocity can remain unchanged if and only if its $\Delta p c = \gamma \beta \Delta E_0$ ($= \gamma v c \mkern.5mu \Delta m$). That must be what happens. Both light waves carry away momentum, but the momentum the body loses is all $E_0$ (or $m$), \emph{even though neither wave has rest energy}. If the body's speed were low enough for us to use Equation \ref{eq:16} (Equation \ref{eq:1}), we'd have $\Delta p c \approx \beta \Delta E_0$ ($ = v c \mkern.5mu \Delta m$), and the takeaway would still be that the rest energy of a system isn't equal to the sum of the rest energies of its constituents. Rest energy (mass) isn't additive. \emph{Total} energy is.

We're making progress! Now we have an equation relating our additive-and-conserved scalar $E$ to our additive-and-conserved vector $\vv p c$. And Equation \ref{eq:18} gives us a new perspective on momentum: constants aside, it's the velocity vector scaled by total energy. The conservation of momentum is nothing but the conservation of energy-times-velocity. Both $E$ and $E \vvbeta$ are additive and conserved.

Finally, we can view Equation \ref{eq:18} as an expression for $\vvbeta$:
\begin{equation*}
\vvbeta= \dfrac{\vv p c}{E}.
\end{equation*}
So velocity is momentum over total energy. That's the additive-and-conserved vector over the additive-and-conserved scalar. \emph{Constant} velocity being displacement $\Delta \vv r$ per time elapsed $c \mkern.5mu \Delta t$, we find a curious relationship among momentum, energy, space, and time:
\begin{equation*}
\dfrac {\vv p c}{E} = \dfrac{\Delta \vv r}{c \mkern.5mu \Delta t} \quad \textrm{\footnotesize{ (for $\vvzeta \equiv \dot{\vvbeta} = \vv 0$)}}.\footnote{Remember that we're using the dot to indicate $ct$-derivatives, in keeping with $\vvbeta = \dot{\vv r}$ for the normalized velocity. So $\vvzeta=\dd \vvbeta / \dd (ct) = \vv a / c^2$, and we'll call this the ``normalized" acceleration. Note, though, that it isn't dimensionless and its magnitude $\zeta$ can exceed one---hence the square quotes.}
\end{equation*}
To generalize for instantaneous velocity when the ``normalized" acceleration $\vvzeta \neq \vv 0$, we use infinitesimal changes in $\vv r$ and $ct$ (or take the limit as $c \mkern.5mu \Delta t \rightarrow 0$):
\begin{equation}\label{eq:19}
\vvbeta = \dfrac {\vv p c}{E} =  \dfrac{\dd \vv r}{\dd (ct)} .
\end{equation}


\subsection{The Energy--Momentum Relation}\label{ssec:em}

As for a relation between $E$ and $\vv p c$, we can go a step further than Equation \ref{eq:18}. The trick is to combine Equations \ref{eq:12} and \ref{eq:17} into a single expression that relates these additive-and-conserved quantities to each other by way of the invariant $E_0$. First we square our equations to liberate $\beta^2$ from the radical sign (within $\gamma$):
\begin{equation*}
E^2 = (\gamma E_0)^2
\end{equation*}
and
\begin{equation*}
(\vv p c)^2 = (\gamma E_0 \vvbeta)^2.
\end{equation*}
Then by Equation \ref{eq:20}, we see that all we need to do is subtract one square from the other:
\begin{equation*}
\begin{split}
E^2 - (\vv p c)^2 &= (\gamma E_0)^2 - (\gamma E_0 \vvbeta)^2 \\[2pt]
&= (\gamma E_0)^2 \, (1 - \beta^2) \\[2pt]
&= E_0^2.
\end{split}
\end{equation*}
Voil\`a:
\begin{equation}\label{eq:21}
\boxed{E^2=E_0^2+(\vv p c)^2} \, .
\end{equation}

Mission accomplished! That's the \textbf{energy--momentum relation}, and it's one of the most important equations in physics. Perhaps you noticed that it evinces a Pythagorean relationship: as illustrated in Figure \ref{f:1}, a system's total energy can be represented as the hypotenuse of a right triangle whose shorter legs represent the system's rest energy and magnitude-of-momentum.\footnote{This Pythagorean interpretation has some elegance and it's okay for now, but don't get too attached to it. Later we'll view the energy--momentum relation from another perspective that will prove more fruitful yet!} For a system at rest, the momentum leg shrinks to zero and our triangle becomes a straight line, reducing Equation \ref{eq:21} to Equation \ref{eq:6} ($E=E_0$).

\begin{figure}[h]
\centering
\caption{Equation \ref{eq:21} as a right triangle}
\label{f:1}
\vspace{10pt}
\begin{tikzpicture}[thick]
\coordinate (O) at (0,0);
\coordinate (A) at (4,0);
\coordinate (B) at (0,2);
\coordinate (K) at (.25, 1.9);
\draw (O)--(A)--(B)--cycle;
%\draw[help lines] (0,0) grid (4,3);
\tkzLabelSegment[below](O,A){\textit{$E_0$}}
\tkzLabelSegment[left](O,B){\textit{$p c$}}
\tkzLabelSegment[xshift =.1cm, yshift=.6cm](A,B){\textit{$E$}}
\tkzMarkRightAngle[size=0.5,opacity=.4](A,O,B)% square angle here
\end{tikzpicture}
\end{figure}

What about when $E_0 = 0$? We haven't really addressed this head-on yet, but earlier we mentioned in passing that some systems are massless or ``restless" (Footnote \ref{fn:rl})---i.e., they have no rest energy because they have no rest frame. What does it mean for a system to have no rest frame? It means that no frame of reference exists for which the momenta of the system's constituents sum to zero. Put differently, all observers agree that the system's $E \vvbeta \neq \vv 0$ (Equation \ref{eq:18}). Assuming that $E > 0$ (wouldn't be much of a system, otherwise!), this implies that the system's $\beta > 0$ for \emph{everyone}. Nobody can catch up to it. And special relativity tells us that that can only be true if the system travels at the universal speed limit, $\beta = 1$. \emph{A ``restless" (massless) system travels at the speed of light}, and vice versa.\footnote{In Section \ref{sssec:sy}, we remarked that a system's velocity is the velocity of its rest frame relative to an observer. Obviously that's nonsense if the system is ``restless" (massless) and \emph{has} no rest frame. In this special case, \emph{every constituent of the system} travels at $\beta = 1$ in the same direction.} By Equation \ref{eq:18}, then, $\vv p c = E \hatbeta$ when $E_0 = 0$ (where the $\hatbeta$ is a unit vector).

Turning our attention back to Figure \ref{f:1}, we see that for $c$-faring things like light that have no rest frame, the rest-energy leg shrinks to zero, the triangle again collapses to a straight line, and Equation \ref{eq:21} reduces to
\begin{equation}\label{eq:22}
E=p c \quad \textrm{\footnotesize{ (for $E_0=0$)}},
\end{equation}
which is the magnitude of our vectorial relation $\vv p c = E \hatbeta$.

``Hold your horses!" you say. ``What of Equations \ref{eq:12} and \ref{eq:17}? Shouldn't $E_0 = 0$ mean that $E = 0$ and $\vv p c = \vv 0$?"

Well, that would be consistent with Equation \ref{eq:22}, but the answer is no: when we remember to plug $\beta = 1$ into the Lorentz factor in Equations \ref{eq:12} and \ref{eq:17}, we end up not with $E = p c = 0$, but rather with $E = 0/0$ and $\vv p c = \vv 0 / 0$, which are indeterminate forms. So Equations \ref{eq:12} and \ref{eq:17} tell us nothing about ``restless" (massless) systems, but they also don't rule out the \emph{possibility} that such systems have energy and momentum, provided that $\beta = 1$.

``Fair enough," you concede, ``but you're still pulling the wool over my eyes! You're applying Equations \ref{eq:18} and \ref{eq:21} to systems with $E_0 = 0$, but we \emph{derived} those equations using Equations \ref{eq:12} and \ref{eq:17}, which \emph{don't} apply to such systems!"

You've got me there. Guilty as charged.

My defense is a three-parter.

First, \emph{if} Equations \ref{eq:18} and \ref{eq:21} cover the $E_0 = 0$ case, then they must do so according to the constraint established by Equations \ref{eq:12} and \ref{eq:17}: $\beta = 1$. All we've done to Equation \ref{eq:18} is impose that limitation on it, making $\vvbeta$ a unit vector.

Second, it works! Equation \ref{eq:22} is the same result that Maxwell's equations lead to for electromagnetic radiation, which we know to be massless. It's the full version of Equation \ref{eq:3}---we only needed the proportionality at the time---and it's the relation that quantum mechanics gives for massless particles like photons. So yes, the logic we used was unsound, but Equations \ref{eq:18} and \ref{eq:21} \emph{do} return the right answer for $E_0 = 0$. It's a happy bonus, and the universal applicability of these two equations makes them treasures.\footnote{Of course, if $E = p c$ is all we've got, then it isn't very useful. We need some additional information to determine the \emph{value} of a ``restless" system's $E$ and $p c$. The conservation principle often comes in handy here (as in Einstein's thought experiment), as do classical electromagnetism and quantum mechanics, both of which relate $E$ and $p c$ to other quantities (amplitude in classical electromagnetism; wavelength and frequency in quantum mechanics).}

Third, it's not \emph{really} just a happy bonus. We've been constructing relativistic dynamics piecemeal, altering Newtonian equations as we go. But with a more sophisticated top-down approach, Equation \ref{eq:21} can be derived in a more general way, leaving no doubt that it applies when $E_0 = 0$.

A word of caution: traveling at $\beta = 1$ is perfectly compatible with being a \emph{constituent} of a system with a rest frame. We already know this from Einstein's thought experiment, where we treated the body and the two light waves as one closed system with rest energy (and kinetic energy for moving observers). The key is that a system's rest frame is the inertial frame in which its aggregate momentum is zero. In the thought experiment, neither individual light wave has such a frame, because each travels at $\beta = 1$ in a single direction. Since nobody can catch up to it, nobody can say its momentum is zero. In a sense, nobody can even \emph{start} to catch up to it. Its speed is $c$ for everybody. But the two light waves together regarded as a system \emph{do} have a rest frame, because momentum is additive, meaning that a system's momentum is the vector sum of the momenta of its constituents---magnitude and direction. As long as the light waves weren't emitted in exactly the same direction, there exists an inertial frame in which their momenta cancel.



\subsection{Vectorless Magnitudes: The Tail Wagging the Dog}

If we rearrange Equation \ref{eq:21} like so,
\begin{equation}\label{eq:23}
E_0^2=E^2-(\vv p c)^2,
\end{equation}
we notice something marvelous: the invariance of the left side---the squared rest energy---guarantees the invariance of the right side. Inertial observers in relative motion will disagree on a system's momentum and total energy measured separately, but they'll agree on the difference of their squares, $E^2-(\vv p c)^2$. That's an \emph{incredibly} powerful tool for analyzing a system's dynamics. It allows us to crunch the numbers for whichever frame most simplifies the math (often the rest frame, where $\vv p c = \vv 0$), and then use the invariant Equation \ref{eq:23} to ``convert" to other frames as needed.

You should be reminded of a similar tool:
\begin{equation*}
(\Delta s)^2 = (c \mkern.5mu \Delta t)^2 - (\Delta \vv r)^2,
\end{equation*}
where $\Delta s$ is the invariant spacetime interval between two events (Equation \ref{eq:sti}). Clearly, the familiar equation for the squared interval has something in common with our new Equation \ref{eq:23}. Inertial observers in relative motion will disagree on the distance and elapsed time between events, but they'll agree on the difference of their squares, $(c \mkern.5mu \Delta t)^2 - (\Delta \vv r)^2$. There's a pattern here, and this isn't the first time we've seen an intriguing connection between $\vv p c$ and $E$ on the one hand and $\vv r$ and $ct$ on the other (Equation \ref{eq:19}). To get to the bottom of this, we'll need the \emph{infinitesimal} spacetime interval, $\dd s$.

\subsubsection{The Spacetime Distance-Analogue}\label{sssec:ds}

It's important to understand that proper time $ct_0$ isn't restricted to inertial travelers. Proper time is the wristwatch time of \emph{any} traveler.\footnote{\label{fn:tr}In this discussion, we model an accelerating ``traveler" as a point particle.} To reckon the proper time interval $c \mkern.5mu \Delta t_0$ for a non-inertial journey, we'd use calculus. First we'd chop up the traveler's path through spacetime (the traveler's \textbf{world line}) into infinitely small segments, each constituting an ``instantaneous" journey with its own \emph{infinitesimal} timelike interval, as above but with d's instead of deltas:
\begin{equation}\label{eq:24}
\boxed{(\dd s)^2 =(c \, \dd t)^2 - (\dd \vv r)^2} \, .
\end{equation}
Corresponding to each instantaneous journey is an inertial frame of reference for which the traveler is momentarily at rest. We call this an \textbf{instantaneous rest frame}.\footnote{This is sometimes called a \textbf{momentarily comoving frame}. Note that the concept is just as applicable to classical mechanics as it is to special relativity!} In every such frame, $\dd \vv r = \vv 0$, so that $\dd s = c \,\dd t = c \, \dd t_0$, where we've introduced the infinitesimal proper time interval $c \, \dd t_0$. To find $c \mkern.5mu \Delta t_0$, then, we'd just add up all the $c \, \dd t_0$'s using integration: $c \mkern.5mu \Delta t_0 = \int c \, \dd t_0$.

But we don't need $c \mkern.5mu \Delta t_0$ right now. It's the infinitesimals we're interested in. This $\dd s$, the infinitesimal spacetime interval, is analogous to an infinitesimal distance in space $\Vert \dd \vv r \Vert$. Like a spatial distance, $\dd s$ doesn't have direction. It's an infinitesimal scalar quantity. Unlike $\Vert \dd \vv r \Vert$, however, $\dd s$ is invariant with respect to observer velocity (i.e., under a Lorentz boost). Different inertial observers will disagree on the traveler's $c \, \dd t$ and $\dd \vv r$, but they'll agree on the traveler's $(c \, \dd t)^2 - (\dd \vv r)^2$.

\subsubsection{The (Normalized) Spacetime Speed-Analogue}\label{sssec:b}

In light of this spacetime distance-analogue, a question arises naturally: can we assign the traveler a \emph{rate} of spacetime traversal, analogous to (normalized) speed? Mind, the name of the game is maintaining invariance---that's what makes the interval and the energy--momentum relation so useful---so any spacetime analogue we fashion must have a value that all inertial observers agree on. Dividing $\dd s$ by infinitesimal coordinate time $c \, \dd t$, for example, wouldn't work, since coordinate time is a relative measurement. Still, the answer is yes, we \emph{can} construct an invariant spacetime $\beta$-analogue. We can divide $\dd s$ by the invariant infinitesimal \emph{proper} time interval, $c \, \dd t_0$:\footnote{(that is, differentiate $s$ with respect to $ct_0$)}
\begin{equation*}
\left(\dfrac{\dd s}{\dd (ct_0)} \right)^2 = \left(\dfrac{\dd (ct)}{\dd (c t_0)} \right)^2 - \left(\dfrac{\dd \vv r}{\dd (c t_0)} \right)^2.
\end{equation*}

Before simplifying this, let's make sure we understand all the terms. We interpret the three derivatives as ratios. So $\dd s/\dd (c t_0)$, our $\beta$-analogue, represents the infinitesimal ``distance" of spacetime a traveler covers per infinitesimal duration of the traveler's own wristwatch time, and all inertial observers agree on this rate. On the far right, $\dd \vv r/\dd (c t_0)$ is the traveler's infinitesimal displacement as measured by an inertial observer---a frame-dependent vector quantity---divided by the infinitesimal passage of time according to the traveler's watch (the invariant proper time). And the middle term? It's the frame-dependent infinitesimal passage of time according to synchronized clocks in an inertial observer's rest frame (i.e., the observer's coordinate time), divided by the invariant infinitesimal passage of time according to the \emph{traveler} (again, proper time). In other words, it's the time-dilation ratio, which we know is just the Lorentz factor! And didn't we already establish that $\dd s = c \, \dd t_0$? We can work with this. First, by the chain rule we have:
\begin{equation*}
\left(\dfrac{\dd s}{\dd (c t_0)} \right)^2 = \left(\dfrac{\dd (ct)}{\dd (c t_0)} \right)^2 - \left( \dfrac{\dd \vv r}{\dd (ct)} \, \dfrac{\dd (ct)}{\dd (c t_0)} \right)^2,
\end{equation*}
and subbing in $1 = \dd s / \dd (c t_0)$, $\gamma = \dd (ct) / \dd (c t_0)$, and $\vvbeta = \dd \vv r / \dd (ct)$:\footnote{\label{fn:cr}Observe here that since $\gamma = \dd (ct) / \dd (c t_0)$, the chain rule lets us rewrite any $c t_0$-derivative as $\gamma$ times the corresponding $ct$-derivative. Now recall that we're using Newton's overdot notation for $ct$-derivatives and a little circle for $c t_0$-derivatives. Thus, e.g., $\mathring{\vv r} = \gamma \dot{\vv r}$.}
\begin{equation}\label{eq:25}
\boxed{1 = \gamma^2 - \left(\gamma \vvbeta \right)^2} \, .
\end{equation}

Now \emph{that's} rather interesting. Our spacetime $\beta$-analogue is, well, one! That's the invariant and \emph{constant} $c t_0$-rate of all ``motion" through spacetime. And from the perspective of any given inertial observer, the time and space ``contributions" to that rate are, respectively, $\dd (ct) / \dd (c t_0) =\gamma$ and $\dd \vv r / \dd (c t_0) = \gamma \vvbeta$. The difference of their squares equals the squared rate, $1^2 = 1$. When $\beta \ll 1$, inertial observers calculate a time contribution $\gamma$ of about one, and a space contribution $\gamma \vvbeta$ of about zero---exactly one and zero in the traveler's instantaneous rest frame. What happens as $\beta$ gets larger? Perhaps you've encountered the unfortunately common pop-sci factoid that the time contribution should then decrease, that ``speed through time" must diminish to accommodate a greater ``speed through space" so that they always sum to one (or to $c$ in different units). As Equation \ref{eq:25} shows, that's a white lie at best (and ``speed through time" is gibberish). Yes, the $ct_0$-rate of spacetime-traversal is always one, and yes, the magnitude of $\gamma \vvbeta$ obviously increases with $\beta$. But look at the time contribution. It doesn't decrease; it blows up to infinity as $\beta$ approaches one! The space contribution does, too, but it can never catch up to the time contribution, since $1>\beta$. So no, the contributions aren't inversely related, and they don't balance to one (or $c$ if unnormalized). The \emph{difference of their squares} balances to $1^2=1$.

Note that Equation \ref{eq:25} doesn't permit setting $\beta=1$ (leads to division by zero in the Lorentz factors), consistent with the fact that proper time is only defined for timelike journeys. No spacelike or lightlike intervals allowed here. To speak of ``restless" (massless) systems ``experiencing" time is meaningless.

\subsubsection{The Spacetime Magnitude-of-Momentum-Analogue}\label{sssec:pc}

So we have a spacetime distance-analogue $\dd s$ and a (normalized) spacetime speed-analogue $\dd s/\dd (c t_0)=1$, both invariant and the latter constant. We could keep going. Taking a $c t_0$-derivative of the $\beta$-analogue, for instance, would lead us to a spacetime $\zeta$-analogue (magnitude-of-``normalized"-acceleration).\footnote{Recall that we've dubbed $\vvzeta = \dot{\vvbeta} = \vv a / c^2$ the ``normalized" acceleration.} Taking yet another would get us an analogue of ``normalized" jerk. The calculus gets more involved, but the principle is the same. We won't do either of those here, though. Instead, we'll find a spacetime magnitude-of-momentum-analogue to bring this section to a close.\footnote{``Spacetime magnitude-of-momentum-analogue"? Oy. Surely we can do better than this. Read on!}

How do we obtain our $p c$-analogue? Well, we already have a $\beta$-analogue, and we know from Equation \ref{eq:18} that momentum is $\vvbeta$ times $E$. But multiplying by $E$ won't do; it isn't invariant. How about rest energy $E_0$? That's the traveler's total energy $E$ as measured in the traveler's instantaneous rest frame, and it's definitely invariant. Yes, that will work, and so our spacetime $p c$-analogue is $\dd s/\dd (c t_0)$ times $E_0$. By Equation \ref{eq:25}, we have:
\begin{equation*}
\begin{split}
\left[ \left( \dfrac{\dd s}{\dd (c t_0)} \right) E_0 \right]^2 &= \left[ \left( \dfrac{\dd (ct)}{\dd (c t_0)} \right) E_0\right]^2 - \left[ \left( \dfrac{\dd \vv r}{\dd (c t_0)} \right)E_0\right]^2 \\[5pt]
\bigl[ (1)E_0 \bigr]^2 &=  \bigl[ (\gamma) E_0 \bigr] ^2 - \bigl[ (\gamma \bm{\upbeta}) E_0 \bigr] ^2 ,
\end{split}
\end{equation*}
and by Equations \ref{eq:12} and \ref{eq:17}:
\begin{equation}\label{eq:26}
\boxed{E_0^2 = E^2 - (\vv p c)^2} \, .
\end{equation}

Remind you of anything? (Ahem \dots Equation \ref{eq:23}.) We've come full circle! Here, the space ``contribution" is the traveler's momentum, the time ``contribution" is the traveler's total energy, and the spacetime $p c$-analogue itself is the traveler's invariant rest energy. Now consider this: Earlier we left unanswered the question of whether total energy $E$---a function of $\beta$, really---qualifies as the ``measure of inertia" or ``resistance to change in velocity" for arbitrary $\vvbeta$ (see Section \ref{ssec:rm}). We know that it does when it reduces to the simple invariant scalar $E_0$ in the classical limit, but until we discuss force we'll have to withhold judgment for the general case. In our steps for Equation \ref{eq:26}, however, we constructed our spacetime $p c$-analogue by multiplying the $\beta$-analogue not by a function like $E$, but rather by the invariant $E_0$:
\begin{equation*}
p c \textrm{-analogue} = E_0 \times \beta \textrm{-analogue}.
\end{equation*}
This situation mirrors that of the low-$\beta$ Equation \ref{eq:16}, where $E_0$ is straightforwardly the resistance to change in motion through space. For constant $E_0$, a change in our spacetime $p c$-analogue would imply a strictly proportional change in our spacetime $\beta$-analogue, with $E_0$ as the constant of proportionality. Rest energy, it seems, is the resistance to change in ``motion" \emph{through spacetime}! But wait a minute. Our $\beta$-analogue \emph{can't} change. It's always one; that's why the $p c$-analogue ended up equaling $E_0$. If only there were some aspect of it that \emph{could} change, something that we're overlooking, then it would make sense to speak of $E_0$ as the measure of ``spacetime inertia." Hmm. We'll come back to this.


\subsubsection{Celerity (Not Proper Velocity)}\label{sssec:ce}

Hidden in our steps for Equation \ref{eq:26} is a pair of relations worth making explicit:
\begin{equation}\label{eq:27}
\dfrac{E}{E_0} = \dfrac{\dd (ct)}{\dd (c t_0)}
\end{equation}
and
\begin{equation}\label{eq:28}
\dfrac{\vv p c}{E_0} = \dfrac{\dd \vv r}{\dd (c t_0)}
\end{equation}
Constants aside, total energy and momentum are to invariant rest energy as coordinate time and displacement are to invariant proper time. A comparison with Equation \ref{eq:19} is instructive (it's Equation \ref{eq:28} divided by Equation \ref{eq:27}). Of course, Equation \ref{eq:27} becomes Equation \ref{eq:12} if we sub in $\gamma$ for $\dd (ct) / \dd (c t_0)$. Additionally, we can extract from Equation \ref{eq:28} a ``new" expression for momentum:
\begin{equation*}
\vv p c = E_0 \, \dfrac{\dd \vv r}{\dd (c t_0)} = E_0 \mkern.5mu \mathring{\vv r},
\end{equation*}
or introducing the symbol $\vvomega \equiv \mathring{\vv r} = \gamma \dot{\vv r} = \gamma \vvbeta$ (see Footnote \ref{fn:cr}):
\begin{equation}\label{eq:29}
\boxed{\vv p c = E_0 \vvomega} \quad \textrm{\footnotesize{ (for $E_0 \neq 0$)}}.
\end{equation}
Looks identical to our low-$\beta$ approximation for momentum but with a \emph{proper}-time derivative instead of a coordinate-time derivative (cf.\ Equation \ref{eq:16}). It goes without saying that Equation \ref{eq:29} is another way of writing Equation \ref{eq:17}; it's only a matter of where we put the Lorentz factor.

We'll call the vector $\vvomega = \gamma \vvbeta$ the (``normalized") \textbf{celerity}.\footnote{The ``unnormalized" celerity $\gamma \vv v$ is usually notated $\vv w$, so $\vvomega = \vv w / c$.} The scare quotes around \emph{normalized} are there because although $\vvomega$ is indeed dimensionless like $\vvbeta$, it has no upper limit (its magnitude $\omega = \gamma \beta$ can exceed one). Celerity often goes by the name ``proper velocity," but that's a confusing misnomer. We'll reserve the word \emph{proper} for rest-frame ``versions" of frame-dependent quantities.\footnote{\label{fn:pr}A competing convention prepends \emph{proper} to $t_0$-derivatives (hence ``proper velocity").} For instance, proper energy $E_0$ is a traveler's total energy $E$ as measured by an observer at rest relative to the traveler. Likewise, a proper time interval $c \mkern.5mu \Delta t_0$ (or $c \, \dd t_0$) is the coordinate time interval $c \mkern.5mu \Delta t$ (or $c \, \dd t$) in the traveler's (instantaneous) rest frame, where $\beta = 0$ and $\gamma = 1$. Proper energy and proper time intervals have \emph{invariant} values, as do all proper quantities; celerity does not. (For proper quantities that are vectors, it's the magnitude that's invariant.) Granted, proper velocity defined ``properly" would be a quantity of little interest---a zero vector---so the term is arguably up for grabs. Still, for the sake of consistency, we won't use it. Further, all our proper quantities will get the naught subscript, like $E_0$ and $c \mkern.5mu \Delta t_0$. The next one we'll encounter is the (``normalized") proper acceleration $\vvzeta_{\mkern1mu 0}$ (Section \ref{sssec:pa}).

Anyway, we've uncovered a definite pattern: we started with the equation for the (squared) invariant infinitesimal interval, with its (squared) scalar ``time contribution" and its (squared) vector ``space contribution," and we've multiplied (or differentiated) it by (squared) invariants to create new invariants, all (squared) spacetime analogues of familiar vector magnitudes. In the following sections, we'll look more closely at our analogy and find a much more convenient way to express it.


\clearpage

\section{Four-Vectors: The Elephant in the Room}
%
%\subsection{A Pythagorean Diversion}
%
%Our mission is to discover what Equations \ref{eq:24}, \ref{eq:25}, and \ref{eq:26} \emph{really} have in common. We've already put our finger on some of it, but there's more.
%
%We begin by noting that the Pythagorean relationship we previously identified with the energy--momentum relation holds for our $\Vert \dd \vv r \Vert$- and $\beta$-analogues, as well. This is illustrated in Figure \ref{f:2}. For a given angle $\theta$, we have $\sin \theta = (\Vert \dd \vv r \Vert / \dd t)/c = \beta = p c / E$, which bears a passing resemblance to Equation \ref{eq:19}. So $\theta = \sin^{-1} {\beta}$, a function of relative velocity. And if you've taken calculus recently, you'll know that the derivative of $\sin^{-1} {\beta}$ is $(1-\beta^2)^{-1/2}$. In other words, the rate of change of $\theta$ with respect to $\beta$ is $\gamma$.
%
%\begin{figure}[h]
%\caption{Equations \ref{eq:24}, \ref{eq:25}, and \ref{eq:26} as right triangles}
%\label{f:2}
%\vspace{10pt}
%\begin{subfigure}[b]{0.32\textwidth}
%\centering
%\resizebox{\linewidth}{!}{
%\begin{tikzpicture}[thick]
%\coordinate (O) at (0,0);
%\coordinate (A) at (4,0);
%\coordinate (B) at (0,2);
%\coordinate (K) at (.25, 1.9);
%\draw (O)--(B)--(A);
%\draw[thick,->] (.5,2.1) -- (.3,1.9);
%%\draw[help lines] (0,0) grid (4,3);
%\tkzMarkRightAngle[size=0.5,opacity=.4](A,O,B)% square angle here
%% Draw the arc which center is (2,1)
%\draw[thick,red] ([shift=(180:4cm)]4,0) arc (180:150:3.5cm);
%\tkzLabelSegment[below,color=red](O,A){\textit{$\dd s = c \, \dd t_0$}}
%\tkzLabelSegment[left](O,B){\textit{$\Vert \dd \vv r \Vert$}}
%\tkzLabelSegment[xshift =.2cm, yshift=0cm](A,B){\textit{$c \, \dd t$}}
%\tkzLabelPoint[xshift=.15cm, yshift=.62cm, color=black](K){\textit{\footnotesize $c \, \dd t_0(\gamma -1)$}}
%\draw[thick,red] (O)--(A);
%\tkzLabelAngle[pos = 1](B,A,O){$\theta$}
%\end{tikzpicture}
%}
%\subcaption{Equation \ref{eq:24}}
%\end{subfigure}
%\begin{subfigure}[b]{0.32\textwidth}
%\centering
%\resizebox{\linewidth}{!}{
%\begin{tikzpicture}[thick]
%\coordinate (O) at (0,0);
%\coordinate (A) at (4,0);
%\coordinate (B) at (0,2);
%\coordinate (K) at (.25, 1.9);
%\draw (O)--(B)--(A);
%\draw[thick,->] (.5,2.1) -- (.3,1.9);
%%\draw[help lines] (0,0) grid (4,3);
%\tkzMarkRightAngle[size=0.5,opacity=.4](A,O,B)% square angle here
%% Draw the arc which center is (2,1)
%\draw[thick,red] ([shift=(180:4cm)]4,0) arc (180:150:3.5cm);
%\tkzLabelSegment[below,color=red](O,A){\textit{$1_{\phantom{0}}$}}
%\tkzLabelSegment[left](O,B){\textit{$\gamma \beta$}}
%\tkzLabelSegment[xshift =.2cm, yshift=0cm](A,B){\textit{$\gamma$}}
%\tkzLabelPoint[xshift=.15cm, yshift=.62cm, color=black](K){\textit{\footnotesize $\gamma -1 \phantom{(E_0)}$}}
%\draw[thick,red] (O)--(A);
%\tkzLabelAngle[pos = 1](B,A,O){$\theta$}
%\end{tikzpicture}
%}
%\subcaption{Equation \ref{eq:25}}
%\end{subfigure}
%\begin{subfigure}[b]{0.32\textwidth}
%\centering
%\resizebox{\linewidth}{!}{
%\begin{tikzpicture}[thick]
%\coordinate (O) at (0,0);
%\coordinate (A) at (4,0);
%\coordinate (B) at (0,2);
%\coordinate (K) at (.25, 1.9);
%\draw (O)--(B)--(A);
%\draw[thick,->] (.5,2.1) -- (.3,1.9);
%%\draw[help lines] (0,0) grid (4,3);
%\tkzMarkRightAngle[size=0.5,opacity=.4](A,O,B)% square angle here
%% Draw the arc which center is (2,1)
%\draw[thick,red] ([shift=(180:4cm)]4,0) arc (180:150:3.5cm);
%\tkzLabelSegment[below,color=red](O,A){\textit{$E_0$}}
%\tkzLabelSegment[left](O,B){\textit{$p c$}}
%\tkzLabelSegment[xshift =.2cm, yshift=0cm](A,B){\textit{$E$}}
%\tkzLabelPoint[xshift=.15cm, yshift=.62cm, color=black](K){\text{\footnotesize $E_{\mkern.5mu \textrm{k}}=E_0(\gamma -1)$}}
%\draw[thick,red] (O)--(A);
%\tkzLabelAngle[pos = 1](B,A,O){$\theta$}
%\end{tikzpicture}
%}
%\subcaption{Equation \ref{eq:26}}
%\end{subfigure}
%\end{figure}
%
%The red legs are our invariants, and their length relative to the $\theta$-dependent hypotenuses is shown by the red circular arcs. The $(\gamma - 1)$ segments indicated by the arrows are the length of the hypotenuses minus the length of the invariant legs. In Figure \ref{f:2}(a), the $(\gamma - 1)$ segment has length $c \, \dd t_0(\gamma -1)$ and represents time dilation. In (c), its length is $E_0(\gamma-1)$ and it represents kinetic energy. In (b), it's the increase in the ``time contribution" of our spacetime $\beta$-analogue that comes with a greater ``space contribution" $\gamma \beta$ (remember that they both blow up to infinity as $\beta$ approaches one).
%
%When $\beta \ll 1$, $\theta$ approaches zero, the red arcs approximate straight lines, and the hypotenuses are about the same length as the invariant legs. In this ``non-relativistic" zone, the $(\gamma - 1)$ segments contribute negligibly to the length of the hypotenuses. As $\beta$ starts to get bigger, those segments remain negligible for a while. Eventually, though, the red arcs start to look arc-ish, and then the segments get appreciably larger. And if you imagine the triangles getting taller and taller and the arcs approaching (but never reaching) a quarter-circle, you'll see that $\gamma$ increases without bound and approximates $(\gamma - 1)$.
%
%All of this is fine, but in a sense we're barking up the wrong tree: the Pythagorean theorem treats the \emph{hypotenuse} as the special quantity---the Euclidean distance---whereas the quantities of most interest to us are represented by the red catheti in Figure \ref{f:2}. Next we'll bring these invariants to the center of our attention. Instead of studying how, say, $E_0$ and $\vv p c$ combine to make $E$ (informative in its own right), we'll focus on how $E$ and $\vv p c$ combine to make $E_0$.


\subsection{The Basics}

\subsubsection{Invariant Magnitudes, Transformable Components}

What do Equations \ref{eq:24}, \ref{eq:25}, and \ref{eq:26} \emph{really} have in common? We've already put our finger on some of it, but there's more.

Our instinct might be to note that the Pythagorean relationship we previously identified with the energy--momentum relation holds for our $\Vert \dd \vv r \Vert$- and $\beta$-analogues, as well. We wouldn't be wrong, but in a sense we'd be barking up the wrong tree. The Pythagorean theorem treats the \emph{hypotenuse} as the special quantity. Given a unit circle centered on the origin, the hypotenuse of a right triangle is associated with the distance between the origin and a point on the circle, whereas the other legs (the catheti) are associated with the Cartesian coordinates of the point. Under a rotation of the coordinate axes, it's the \emph{hypotenuse}---the Euclidean distance---that remains unchanged. That's why it's special. But in the context of a Lorentz transformation, the quantities that \emph{look like} ``hypotenuses" in Equations \ref{eq:24}, \ref{eq:25}, and \ref{eq:26} ($c \, \dd t$, $\gamma$, and $E$) are not the ones that remain unchanged! If we embrace a geometric mindset, which we'll soon justify (later we'll even learn to regard a Lorentz boost as a ``rotation" of sorts), then our special quantities are invariants like spacetime intervals and rest energy. Let's bring these invariants to the center of our attention. Instead of studying how, say, $E_0$ and $\vv p c$ combine to make $E$, we'll focus on how $E$ and $\vv p c$ combine to make $E_0$. The payoff will be enormous.

So here again are our Equations \ref{eq:24}, \ref{eq:25}, and \ref{eq:26}, unsquared this time and with the vectors broken down into their Cartesian components:
\begin{align*}
\dd s&=\sqrt{(c \, \dd t)^2 - \Bigl[ (\dd x)^2 + (\dd y)^2 + (\dd z)^2 \Bigr]} \tag{$\Vert \dd \vv r \Vert$-analogue} \\[10pt]
\mathring{s} = \dfrac{\dd s}{\dd (c t_0)} =1&=\sqrt{\gamma^2 - \Bigl[ (\gamma \beta_x)^2 + (\gamma \beta_y)^2 + (\gamma \beta_z)^2 \Bigr]} \tag{$\beta$-analogue} \\[10pt]
E_0 \mkern1mu \mathring{s} = E_0 &=\sqrt{E^2 - \Bigl[ (cp_x)^2 + (cp_y)^2 + (cp_z)^2 \Bigr] } \tag{$p c$-analogue}
\end{align*}
And here's what we've already acknowledged about them:
\begin{itemize}
\item They're all analogous to familiar vector magnitudes (infinitesimal distance, normalized speed, and magnitude-of-momentum).
\item They're all \emph{invariant} scalars, thanks to our strict procedure for generating the $\beta$- and $p c$-analogues from the timelike $\dd s$.
\item Each has a scalar ``time contribution" with an embedded $c\, \dd t$ factor.
\item Each has a vector ``space contribution" with an embedded $\dd \vv r$ factor.
\item The square of the space contribution is always subtracted from the square of the time contribution.
\end{itemize}
Now let's go deeper.

Each equation takes four frame-dependent quantities in the same unit and fuses them into an invariant scalar. This is achieved by combining their squares in a particular way.

You have experience with something quite like this. In fact, you were looking at an example of it just a moment ago. Focus on the vector components in one of those equations---say, the $\vv p c$-components---and consider what we're doing to them. We're summing their squares, which is to say that we're taking the dot product $\vv p c \cdot \vv p c= (cp_x)^2 + (cp_y)^2 + (cp_z)^2 = (p c)^2$. If the energy term (and minus sign) weren't there, that dot product would be the entire radicand, and the square root would be $p c$, the magnitude of the momentum vector $\vv p c$. When we use this component-squaring method to find a self--dot product or a vector's magnitude in three-dimensional Euclidean space, we're combining three numbers that aren't invariant into a scalar with the same unit that \emph{is} invariant, though not in the sense we've been using the word in this paper (i.e., not under a boost; see Footnote \ref{fn:inv}). Vector components depend on your chosen origin and axes, but a vector's magnitude (and its square) stays the same if you arbitrarily rotate or translate (shift) your coordinate system.\footnote{\label{fn:pv}The position vector $\vv r$, which always has its tail at the origin, is an exception when it comes to translation. Its magnitude is invariant under \emph{rotation}, but if you shift your axes then you end up with a new origin, and consequently an entirely new position vector. Not so for $\Delta \vv r$ or its infinitesimal counterpart $\dd \vv r$; the magnitude of a displacement or separation vector is indeed invariant under translation.} Within the context of that three-dimensional space, the magnitude and squared magnitude of your run-of-the-mill vector are invariant scalars, while the vector's components transform under rotation or translation according to some linear formulas.

The invariant quantities \emph{we've} been dealing with (like $E_0$) are invariant under boosts in the context of the four-dimensional spacetime we inhabit. They're \textbf{\emph{Lorentz}-invariant}, and this brings us to one more thing our analogues have in common: the four (unsquared) quantities on the right side of Equations \ref{eq:24}, \ref{eq:25}, and \ref{eq:26} all obey the Lorentz transformation. Why? Because we started with the infinitesimals $c\, \dd t$, $\dd x$, $\dd y$, and $\dd z$ (Equation \ref{eq:24}), which we know transform Lorentzianly, and all we did to them was multiply by or differentiate with respect to invariant scalars ($ct_0$ and $E_0$), quantities whose values all observers agree on. Since the Lorentz transformation is linear, and since multiplication and differentiation are linear operations, the results must \emph{also} obey the Lorentz transformation! Under standard configuration, then, momentum and energy (Equation \ref{eq:26}) transform between frames like this:
\begin{equation}\label{eq:fm}
\begin{aligned}
E^\prime &= \gamma \left( E - \beta cp_x \right) & E &= \gamma \left( E^\prime + \beta cp_x^\prime \right) \\
cp_x^\prime &= \gamma \left( cp_x - \beta E \right) & \qquad cp_x &= \gamma \left( cp_x^\prime + \beta E^\prime \right) \\
cp_y^\prime &=cp_y & cp_y &= cp_y^\prime \\
cp_z^\prime &=cp_z & cp_z &= cp_z^\prime
\end{aligned}
\end{equation}
(we've just substituted energy and momentum-components for duration and separation-components in Equations \ref{eq:lt2}). For the transformation of $\gamma$ and $\gamma \vvbeta$ [$= \vvomega$] (Equation \ref{eq:25}), it helps to reintroduce the ``rel" subscript for the relative speed between the primed and unprimed frames, as well as for the corresponding Lorentz factor, so as not to confuse them with the primed and unprimed measurements of some third party's velocity and Lorentz factor: 
\begin{equation}\label{eq:fv}
\begin{aligned}
\gamma^\prime &= \gamma_{\textrm{rel}} \left( \gamma - \beta_{\textrm{rel}} \mkern1mu \gamma \beta_x \right) & \gamma &= \gamma_{\textrm{rel}} \left( \gamma^\prime + \beta_{\textrm{rel}} \mkern1mu \gamma^\prime \beta_x^\prime \right) \\
\gamma^\prime \beta_x^\prime &= \gamma_{\textrm{rel}} \left( \gamma \beta_x - \beta_{\textrm{rel}} \mkern1mu \gamma \right) & \qquad \gamma \beta_x &= \gamma_{\textrm{rel}} \left( \gamma^\prime \beta_x^\prime + \beta_{\textrm{rel}} \mkern1mu \gamma^\prime \right) \\
\gamma^\prime \beta_y^\prime &= \gamma \beta_y & \gamma \beta_y &= \gamma^\prime \beta_y^\prime \\
\gamma^\prime \beta_z^\prime &= \gamma \beta_z & \gamma \beta_z &= \gamma^\prime \beta_z^\prime.
\end{aligned}
\end{equation}
A bit of algebra leads directly back to Equations \ref{eq:vt}.

We're finally ready to put it all together. Our spacetime analogues really are just like vector magnitudes. They're invariant, and their squares are found by combining the squares of quantities that aren't invariant but which transform together according to a family of linear formulas. What's different is the \emph{method} of combination. Instead of adding all the squares like we're used to doing with vector components in Euclidean geometry (Pythagorean theorem), we subtract the squares of the space contributions from the square of the time contribution. This difference reflects the fact that although space and time are intimately connected, they're not the same thing.


\subsubsection{Four-Displacement and the Minkowski Dot Product}\label{sssec:fdi}

We're missing one ingredient---direction! This is easily remedied. In what direction does a traveler journey through spacetime? The question answers itself: in the direction the traveler happens to be going. Now, direction is relative, even in three-dimensional Euclidean space, and so it must be specified in relation to something. Strictly speaking, a whole coordinate system isn't necessary; it's enough to have two events (two points in spacetime) to specify the direction from one to the other, from ``here-and-now" to ``there-and-then." Coordinate systems have their advantages, though. Once an inertial observer has adopted one, ``now" and ``then" can be expressed as coordinate times, and ``here" and ``there" can be expressed as spatial coordinates in reference to the observer's chosen spatial origin $(x,y,z)=(0,0,0)$. But the observer can also put ``from now to then" on an equal footing with ``from here$_{x, y, z}$ to there$_{x, y, z}$," and catalog all events a traveler reaches in reference to a chosen \emph{spacetime} origin, $(ct,x,y,z)=(0,0,0,0)$. That way, the \emph{spacetime} direction from one event on the traveler's world line to the ``next" (infinitesimally close to the first) can be thought of as an arrow pointing from the temporally earlier event ($ct$, $x$, $y$, $z$) to the temporally later event ($ct + c \, \dd t$, $x + \dd x$, $y + \dd y$, $z + \dd z$). In the traveler's instantaneous rest frame, $\dd \vv r = \vv 0$, and the spacetime direction is entirely temporal. In all other frames, however, observers say that the traveler's spacetime direction is both temporal and spatial. And they're all correct! Different inertial observers have different ideas of how far apart ``here" and ``there" are, and how much time passes between ``now" and ``then," but they agree on the spacetime interval between events, and they agree on the \emph{chronological order} of timelike- (and lightlike-) separated events.

One more step. We take this notion of a traveler's spacetime direction, and we pair it with the magnitude $\dd s$, the infinitesimal spacetime interval between ``adjacent" events on the traveler's world line. The resulting object is an infinitesimal displacement vector, analogous to our old friend $\dd \vv r$ that dwells in three-dimensional Euclidean space. Since this new vector dwells in four-dimensional spacetime, we say that it's a \textbf{four-vector}, and now we'll use the term \textbf{three-vector} to refer to $\dd \vv r$ and its associates. The \emph{components} of our new (infinitesimal) \textbf{four-displacement} are what we've hitherto called the time and space ``contributions" to $\dd s$: $c \, \dd t$, $\dd x$, $\dd y$, and $\dd z$. It bears repeating that they obey the Lorentz transformation, and (equivalently) that the vector's magnitude is Lorentz-invariant. These two properties are so important that we make them sine qua nons for any future four-vectors we define. \emph{By definition, four-vectors have Lorentz-invariant magnitudes and Lorentz-transformable components.} Fortunately, these conditions are satisfied automatically if we use the same procedure for generating new four-vectors that we used earlier to generate our spacetime analogues: take an existing four-vector, and either multiply it by or differentiate it with respect to an invariant.\footnote{More generally, \emph{any} four quantities that transform together Lorentzianly are the components of a four-vector.} In the case of our spacetime $\beta$- and $p c$-analogues, we've already done the work, and we'll see that they are the magnitudes of four-vectors, just as our spacetime $\Vert \dd \vv r \Vert$-analogue ($\dd s$) is the magnitude of four-displacement.

We'll need some notation. To distinguish the four-displacement from the three-displacement $\dd \vv r$, let's use uppercase for the former:
\begin{equation}\label{eq:30}
\boxed{\dd \vv R \equiv \langle c \, \dd t, \, \dd \vv r \rangle = \langle c\, \dd t, \, \dd x, \, \dd y, \, \dd z \rangle} \, .
\end{equation}
And defining a dot-product for four-vectors, we have $\dd \vv R$'s magnitude:
\begin{equation*}
\begin{split}
\Vert \dd \vv R \Vert &= \sqrt{\dd \vv R \cdot \dd \vv R}\\
&= \sqrt{(c \, \dd t)(c \, \dd t) - (\dd \vv r \cdot \dd \vv r)}\\
&= \sqrt{(c \, \dd t)^2 - (\dd x)^2 - (\dd y)^2 - (\dd z)^2}\\
&= \dd s,
\end{split}
\end{equation*}
the invariant interval (timelike, but let's allow lightlike too). We already knew that, but now we see explicitly that the dot product of two four-vectors differs from the dot product of two three-vectors by the signs of the products of the components: instead of ($+$$+$$+$), we have ($+$$-$$-$$-$), with the product of the time components in first position.\footnote{\label{fn:sc}It works just as well to use the convention ($-$$+$$+$$+$), or even to multiply the time component by the imaginary unit $\mathrm{i}$ and use ($+$$+$$+$$+$).} Why? Because of the equation for the spacetime interval (Equation \ref{eq:24}). That's just how it is in our \textbf{Minkowski spacetime}.\footnote{The mathematician Hermann Minkowski developed the geometric approach to special relativity we're exploring. He used the ($+$$+$$+$$+$) convention mentioned in the previous footnote, by the way, but it's fallen out of favor (it causes problems in \emph{general} relativity).} For constant velocity, we can of course use a finite four-displacement $\Delta \vv R = \langle c \mkern.5mu \Delta t, \Delta \vv r \rangle$.\footnote{\label{fn:sp}We can speak more abstractly of a \textbf{four-separation} $\Delta \vv R = \langle c \mkern.5mu \Delta t, \Delta \vv r \rangle$ that ``points" from one event to another. They needn't be on a traveler's world line, and the ``from" event needn't be chronologically earlier than the ``to" event. Indeed, the events may be spacelike-separated, in which case they don't even \emph{have} a fixed chronological order, and the vector's magnitude is then a \emph{spacelike} interval (an imaginary number---see Section \ref{sssec:sti2}).}

You may be wondering if our new \textbf{Minkowski dot product} has a geometric definition like the Euclidean dot product does ($\vv q \cdot \vv w = qw \cos \theta$). The answer is a qualified yes: for \emph{qualifying} four-vectors $\vv Q$ and $\vv W$, the analogous relation is $\vv Q \cdot \vv W = QW \cosh \phi$. Here we have the hyperbolic cosine function and its argument $\phi$, which in this context can be thought of as the \emph{hyperbolic angle} between $\vv Q$ and $\vv W$ (itself a concept that needs qualifying). We'll forgo the details until Section \ref{ssec:ra}. For now, it's enough to know that in Minkowski spacetime, \textbf{orthogonal vectors} remain those whose dot product with each other is zero, and \textbf{parallel vectors} remain those that are scalar multiples of each other (\textbf{codirectional} if the scalar is positive, \textbf{contradirectional} if it's negative).\footnote{The words \emph{parallel} and \emph{antiparallel} are often used instead of \emph{codirectional} and \emph{contradirectional}, but then there's no word left to mean ``codirectional or contradirectional." For us, \emph{parallel} means ``codirectional or contradirectional" and gets the symbol $\parallel$. So if $\vv Q \parallel \vv W$, then $\vv Q$ and $\vv W$ are either codirectional or contradirectional. (Same for three-vectors.)}


\subsubsection{Four-Position?}

We could have started with a \emph{position} four-vector, but the \textbf{four-position} $\vv R = \langle ct, \vv r \rangle$ has the same limitation as the position three-vector mentioned in Footnote \ref{fn:pv}---namely, its magnitude isn't invariant under translation because its tail is always at the (spacetime) origin. Different origin? Different position vector. With standard configuration, this is no problem, since the frames share a spacetime origin and are merely ``boosted" relative to each other along their coincident $x$-axes. But all the other four-vectors we'll discuss have magnitudes that are invariant even under translation. In the jargon, these vector magnitudes are \textbf{Poincar\'e-invariant}. The magnitude of the four-position is \emph{Lorentz}-invariant like the others, but not Poincar\'e-invariant. For that reason, we take the \emph{displacement} four-vector as our prototype, rather than the four-position.


\subsubsection{(Normalized) Four-Velocity}

Next, we define the normalized \textbf{four-velocity} by replicating our steps for obtaining the spacetime $\beta$-analogue from the $\Vert \dd \vv r \Vert$-analogue (cf.\ Section \ref{sssec:b}). Again, we use uppercase (we'll always use uppercase for four-vectors and lowercase for three-vectors):
\begin{equation}\label{eq:31}
\boxed{
\begin{aligned}
\vv B&\equiv \mathring{\vv R} = \dfrac{\dd \vv R}{\dd (c t_0)} \\[3pt]
&= \langle \gamma, \gamma \vvbeta \rangle = \langle \gamma, \mkern1mu \vvomega \rangle \\[4pt]
&= \gamma \mkern1.5mu \big \langle 1, \, \beta_x, \, \beta_y, \, \beta_z \big \rangle
\end{aligned}
} \, .
\end{equation}
Its magnitude is $B = \sqrt{\vv B \cdot \vv B} = \gamma \sqrt{1 - (\vvbeta \cdot \vvbeta)} = 1$, a \emph{constant} invariant. As we could have guessed from Equation \ref{eq:25}, $\vv B$ is a unit vector. Note the similarity between Equation \ref{eq:31}'s $\vv B = \mathring{\vv R}$ and the relation $\vvbeta = \dot{\vv r}$.\footnote{Recall that we're using the circle for $c t_0$-derivatives and the dot for $ct$-derivatives. Also, the unnormalized four-velocity is $\vv V = \dd \vv R / \dd t_0 = \vv B c$, just as $\vv v = \vvbeta c$.}

Because proper time is undefined for lightlike intervals (or, if you prefer, because $\gamma$ is undefined for $\beta = 1$), $\vv B$ is undefined for light and anything else without a rest frame.


\subsubsection[Four-Momentum: One Conservation Law to Rule Them All]{Four-Momentum:\\ One Conservation Law to Rule Them All}

By the same logic, we can construct a \textbf{four-momentum} in terms of the normalized four-velocity, replicating our steps for obtaining the spacetime $p c$-analogue from the $\beta$-analogue (cf.\ Section \ref{sssec:pc}). We'll define it so that it has the dimension of energy:\footnote{Many authors use $\vv P \equiv \langle E/c , \, \vv p \rangle = m \vv V$ instead, giving it the dimension of momentum.}
\begin{equation}\label{eq:32}
\boxed{
\begin{aligned}
\vv P &= E_0 \vv B \\
\vv P &\equiv \langle E, \vv p c \rangle = \langle E, \, c p_x, \, c p_y, \, c p_z \rangle
\end{aligned}
} \, .
\end{equation}
Its magnitude is $P = \sqrt{\vv P \cdot \vv P} = \sqrt{E^2 - (\vv p c \cdot \vv p c)} = E_0$, the invariant rest energy. Note the similarity between Equation \ref{eq:32}'s $\vv P = E_0 \vv B$ and the relation $\vv p c = E \vvbeta$ (Equation \ref{eq:18}).

Unlike the four-velocity $\vv B$, the four-momentum $\vv P$ \emph{is} defined for a ``restless" (massless) system, but not in the form $E_0 \vv B$. (That's why we reserved the ``$\equiv$" symbol for the second line in Equation \ref{eq:32}.) What's more, Equation \ref{eq:22} tells us that $E=p c$ in that case. The magnitude of the four-momentum is then $P = \sqrt{E^2 - (p c)^2} = 0$, as it must be when $E_0 = 0$. So the four-momentum is then a \textbf{null vector}, even though its components aren't zero. That runs counter to our Euclidean intuition. In Minkowski spacetime, a four-vector is null if the absolute value of its scalar time component equals the magnitude of its three-vector spatial component. The \textbf{zero four-vector} (we'll use the symbol \mbox{\boldmath$\emptyset$} to distinguish it from the null/zero three-vector $\vv 0$) is a null vector that indeed has only zeros for components; it has the special property of being orthogonal to all four-vectors.

Speaking of components, energy and three-momentum are special because they're our additive-and-conserved quantities. And since the \emph{components} of four-momentum are additive and conserved, four-momentum \emph{itself} is additive and conserved.\footnote{Careful: this doesn't mean that the \emph{magnitude} of four-momentum is additive and conserved. The magnitude is $E_0$, which we know to be conserved but \emph{not} additive!} We've combined two additive conservation laws into one---the \textbf{additivity and conservation of four-momentum}. Fancy that!

Let's briefly see this law in action. Suppose a closed system consists only of particle $a$ and particle $b$. If they collide elastically, then $\vv P_{\mathrm{system}} = \vv P_a + \vv P_b$ before and after the collision,\footnote{In these types of problems, it's usually assumed that there's no potential energy or field momentum to worry about. If, however, field contributions aren't negligible, then we must use $\vv P_{\mathrm{system}} = \vv P_a + \vv P_b + \vv P_{\mathrm{field}}$. Naturally, conservation still holds.} and the system four-momentum before the collision (subscript i for \emph{initial}) equals the system four-momentum after the collision (subscript f for \emph{final}):
\begin{equation*}
\begin{aligned}
\vv P_{a, \mkern1mu \mathrm{i}} + \vv P_{b, \mkern1mu \mathrm{i}} &= \vv P_{a, \mkern1mu \mathrm{f}} + \vv P_{b, \mkern1mu \mathrm{f}}\\
\langle E, \vv p c \rangle_{a, \mkern1mu \mathrm{i}} + \langle E, \vv p c \rangle_{b, \mkern1mu \mathrm{i}} &= \langle E, \vv p c \rangle_{a, \mkern1mu \mathrm{f}} + \langle E, \vv p c \rangle_{b, \mkern1mu \mathrm{f}},
\end{aligned}
\end{equation*}
i.e.:
\begin{equation*}
\begin{split}
E_{a, \mkern1mu \mathrm{i}} + E_{b, \mkern1mu \mathrm{i}} &= E_{a, \mkern1mu \mathrm{f}} + E_{b, \mkern1mu \mathrm{f}}\\
cp_{x\,a, \mkern1mu \mathrm{i}} + cp_{x\,b, \mkern1mu \mathrm{i}} &= cp_{x\,a, \mkern1mu \mathrm{f}} + cp_{x\,b, \mkern1mu \mathrm{f}}\\
cp_{y\,a, \mkern1mu \mathrm{i}} + cp_{y\,b, \mkern1mu \mathrm{i}} &= cp_{y\,a, \mkern1mu \mathrm{f}} + cp_{y\,b, \mkern1mu \mathrm{f}}\\
cp_{z\,a, \mkern1mu \mathrm{i}} + cp_{z\,b, \mkern1mu \mathrm{i}} &= cp_{z\,a, \mkern1mu \mathrm{f}} + cp_{z\,b, \mkern1mu \mathrm{f}}.
\end{split}
\end{equation*}
Or say particle $d$ decays into particles $e$ and $g$. Then we have:
\begin{equation*}
\begin{split}
\vv P_d &= \vv P_e + \vv P_g\\
\langle E, \vv p c \rangle_{d} &= \langle E, \vv p c \rangle_{e} + \langle E, \vv p c \rangle_{g},
\end{split}
\end{equation*}
which works even if the decay products are ``restless" (massless) particles, as when a pion decays into two photons.

Because the conservation of four-momentum holds for any inertial observer, many problems in relativistic dynamics are best solved by considering the rest frame of either the system or one of its constituents. There, the only non-zero component of $\vv P_{\mathrm{system}}$ (or of $\vv P_{\mathrm{constituent}}$) is the time component, $E_{\mathrm{system}}$ ($E_{\mathrm{constituent}}$), which equals the system's (constituent's) rest energy. Since rest energy is invariant, one can then use that result in conjunction with the conservation of four-momentum to find unknowns in other frames.


\subsubsection{Four-Force, Three-Force, and Power}\label{sssec:ff}

In Section \ref{sssec:pc}, we puzzled over the notion that rest energy is the measure of ``spacetime inertia." Now with Equation \ref{eq:32} we can understand what that means. Although $\vv B$'s magnitude is invariant and fixed at one, its \emph{direction} is certainly changeable. A system's rest energy must be its resistance to change in its direction of ``motion" through \emph{spacetime}.

With all this talk about resistance to change in velocity, it's about time we introduced a relativistic concept of force. But before we do, let's reiterate the stipulation we made in Footnote \ref{fn:tr}: for now, we model any accelerating system (or ``traveler") as a point particle. Otherwise we'd need to account for the fact that the whole system doesn't accelerate ``as one" under the influence of a (non-gravitational) force, and we'd have to allow for different parts of the system to have different rest frames. Let's also remind ourselves of a related issue we encountered back in Section \ref{ssec:re}: if the transfer of rest energy into or out of a composite system occurs at more than one location on the system's bounds, then the relativity of simultaneity makes it difficult if not impossible to consider the rest energy ``invariant." If the energy-transfer occurs at a single location on the system's bounds, or if the system is modeled as a point particle, then the relativity of simultaneity presents no such issue.

On to force. In Newtonian mechanics, force is $\vv f_{\mathrm{classical}} \equiv \dd \vv p_{\mathrm{classical}} / \dd t$.\footnote{\label{fn:cf}We regard $\vv f_{\mathrm{classical}} = m \vv a$ as incidental, valid only in the ``special case" that $\dot{m} = 0$. It's probably more common to \emph{define} classical force as $m \vv a$, so that its magnitude is \emph{always} invariant under a Galilean boost (even in a variable-mass situation). The definition we're using falls short in that regard (the chain rule produces a frame-dependent $(\dd m / \dd t) \vv v$ term), but it has two advantages for us: first, it emphasizes a conserved quantity (momentum); and second, it makes the transition to force in special relativity rather straightforward.} Let's take this as our cue and define a \textbf{four-force}---sometimes called the \textbf{Minkowski force}---in terms of the four-momentum (we'll use the chain rule with $\dd (ct) / \dd (c t_0) = \gamma$):
\begin{equation}\label{eq:33}
\boxed{
\begin{aligned}
\vv F &\equiv \mathring{\vv P} \\[2pt]
&= \langle \mathring{E}, \mkern.5mu \mathring{\vv p} c \rangle \\[3pt]
&= \gamma \mkern1.5mu \big \langle \dot{E}, \mkern.5mu \dot{\vv p} c \big \rangle \\[3pt]
&=\gamma \mkern1.5mu \big \langle \mathcal{P}, \mkern.5mu \vv f \big \rangle
\end{aligned}
} \, ,
\end{equation}
where $\mathcal{P} \equiv \dot{E} = \dd E / \dd (ct)$ is the ``normalized" \textbf{relativistic power}, and where ${\vv f \equiv \dot{\vv p} c = \dd (\vv p c) / \dd (ct)}$ is the \textbf{relativistic three-force}. If we wanted to be dimensionally consistent with classical physics we could instead write ${\vv F = \gamma \langle P/c, \mkern.5mu \vv f \rangle}$ (where $P = \mathcal{P}c$ is power in traditional units, not $\Vert \vv P \Vert$), but I'm allergic to unnecessary $c^{-1}$'s.\footnote{Setting $c = 1$ renders all of this moot, but we're not doing that. My strategy is: make symbols for ``normalized" quantities to hide $c^{-1}$ ($\mathcal P$, $\vvbeta$, $\vvzeta$), admit $c$ sometimes by ``attaching" it to certain quantities ($\vv p c$, $ct$), and always take $ct$-derivatives (never $t$-derivatives).} Like the four-velocity, the four-force is defined as a $ct_0$-derivative and is therefore inapplicable when $E_0 = 0$.

So the greater a system's rest energy $E_0$, the greater its resistance to change in (direction of) four-velocity $\vv B$ under the influence of an acceleration-causing four-force $\vv F$. We specify ``acceleration-causing" because a system whose $E_0$ changes while its $\vvbeta$ (and hence $\vv B$) doesn't---like the light-emitting body in Einstein's thought experiment---is under the influence of a four-force, too.\footnote{\label{fn:fo}Unlike the classical force, whose magnitude isn't Galilean-invariant in a variable-mass scenario because of the frame-dependent $(\dd m / \dd t) \vv v$ term mentioned in Footnote \ref{fn:cf}, the four-force has a magnitude that's \emph{always} Lorentz-invariant (provided we model the system in question as a point particle). The chain rule does produce an analogous $\mathring{E}_0 \vv B$ term ($\vv F = \mathring{\vv P} = \mathring{E}_0 \vv B + E_0 \mathring{\vv B}$), but $\vv B$ is itself a four-vector with a Lorentz-invariant magnitude, whereas $\vv v$'s magnitude varies under a Galilean boost. \emph{The four-velocity is \emph{not} to the Lorentz transformation as the three-velocity is to the Galilean transformation.}}

In the common scenario that a system's $E_0$ doesn't change, the relativistic power $\mathcal{P} = \dot{E} = \dot{E}_{\mkern.5mu \textrm{k}}$, and we're inclined to ask ourselves whether the \textbf{work--kinetic energy principle} from Newtonian mechanics holds in special relativity: does $\dd E_{\mkern.5mu \textrm{k}} = \vv f \cdot \dd \vv r$ in this case? Or equivalently, does $\dot{E}_{\mkern.5mu \textrm{k}} = \vv f \cdot \vvbeta$? It does, as we discover if we take the $ct$-derivative of Equation \ref{eq:21}:
\begin{equation}\label{eq:rp}
\begin{split}
\frac{\dd}{\dd (ct)} \Big[ E^2 \Big] &= \frac{\dd}{\dd (ct)} \Big[ E_0^2 + (\vv p c)^2 \Big] \\[5pt]
2E \dot{E} &= 2 E_0 \dot{E}_0 + 2 \vv p c \cdot \dot{\vv p} c\\[5pt]
\dot{E} &= \dfrac{E_0}{E} \, \dot{E}_0 + \dot{\vv p} c \mkern1mu \cdot \dfrac{\vv p c}{E} \\[5pt]
\mathcal{P} &= \dfrac{\mathring{E}_0}{\gamma^2} + \vv f \cdot \vvbeta
\end{split}
\end{equation}
(by Equations \ref{eq:12} and \ref{eq:19}), which becomes $\mathcal{P} = \dot{E}_{\mkern.5mu \textrm{k}} = \vv f \cdot \vvbeta$ when $\dot{E}_0 = 0$.\footnote{The quantity $\mathring{E}_0 = \gamma \dot{E}_0$ that we introduced in Footnote \ref{fn:fo} and Equation \ref{eq:rp} is invariant (the proper-time derivative of the proper energy)---provided, of course, that no relativity-of-simultaneity issues throw a wrench in the works. Later we'll dub it the proper power.} Equation \ref{eq:33} then takes the sometimes useful form:
\begin{equation}\label{eq:fb}
\vv F = \gamma \mkern1.5mu \big \langle \vv f \cdot \vvbeta, \, \vv f \big \rangle \quad \textrm{\footnotesize{ (for $\mathring{E}_0 = 0$)}}.
\end{equation}

\subsubsection{Invariance of the Minkowski Dot Product}

Now, we've seen what happens when we take the dot product of a four-vector with itself: we get its (squared) magnitude, which is invariant. But what happens when we take the Minkowski dot product of one four-vector with another? A four-momentum and a four-displacement, say:
\begin{equation*}
\vv P \cdot \dd \vv R = (E)(c \, \dd t) - (\vv p c \cdot \dd \vv r) .
\end{equation*}
Is \emph{that} invariant? Well, how do things look in another frame?
\begin{equation*}
\vv P^\prime \cdot \dd \vv R^\prime = (E^\prime)(c \, \dd t^\prime) - (\vv p c ^\prime \cdot \dd \vv r ^\prime) .
\end{equation*}
We want to know whether $\vv P \cdot \dd \vv R = \vv P^\prime \cdot \dd \vv R^\prime$. We can put the frames in standard configuration without losing generality, and then the question becomes: does $(E^\prime)(c \, \dd t^\prime) - (cp_x^\prime)(\dd x^\prime)$ equal the unprimed $(E)(c \, \dd t) - (cp_x)(\dd x)$? To find out, we'll use the Lorentz transformation. For the components of four-momentum, that's Equations \ref{eq:fm}; for the components of four-displacement, it's Equations \ref{eq:lt2} (with $\dd$'s instead of $\Delta$'s). We'll also use Equation \ref{eq:20}:
\begin{equation*}
\begin{split}
(E^\prime)(c \, \dd t^\prime) - (cp_x^\prime)(\dd x^\prime)&=\Bigl[ \gamma(E-\beta cp_x) \Bigr] \Bigl[ \gamma(c \, \dd t - \beta \, \dd x) \Bigr]\\
&\quad \quad \quad - \Bigl[ \gamma(cp_x - \beta E) \Bigr] \Bigl[ \gamma(\dd x - \beta \mkern1mu c \, \dd t) \Bigr]\\[5pt]
&=\gamma^2 \bigl( E \mkern1mu c \, \dd t - \cancel{E \beta \, \dd x} - \bcancel{\beta c p_x \, c \, \dd t} + \beta^2 c p_x \, \dd x \bigr) \\
&\quad \quad \quad - \gamma^2 \bigl( cp_x \, \dd x - \bcancel{c p_x \beta \, c \, \dd t} - \cancel{\beta E \, \dd x} + \beta^2 E \mkern1mu c \, \dd t \bigr) \\[5pt]
&=\gamma^2 \Bigl[ (E \mkern1mu c \, \dd t - \beta^2 E \mkern1mu c \, \dd t) - (cp_x \, \dd x - \beta^2 cp_x \, \dd x) \Bigr]\\[5pt]
&=\gamma^2 \Bigl[ (E \mkern1mu c \, \dd t)(1-\beta^2) - (cp_x \, \dd x)(1-\beta^2) \Bigr]\\[5pt]
&=(E)(c \, \dd t) - (cp_x) (\dd x).
\end{split}
\end{equation*}
Wonderful! The dot product of \emph{any} two four-vectors is Lorentz-invariant\footnote{(Poincar\'e-invariant, even, as long as there are no position four-vectors involved)} as a direct consequence of the Lorentz-transformability of their components. Frankly, if that weren't the case, this four-vector business would be pointless.

Here is a quick example of the utility---nay, necessity---of this result. Imagine a perfectly inelastic collision between particles $h$ and $j$ with known rest energies $E_{0 \mkern.5mu h}$ and $E_{0j}$. Say that particle $h$ travels with velocity $\vvbeta$ (and Lorentz factor $\gamma$) in particle $j$'s rest frame before the collision. Given that information, how could we find the rest energy of the resulting composite ``blob"? By the additivity and conservation of four-momentum, we have:
\begin{equation*}
\vv P_h + \vv P_j = \vv P_{\textrm{blob}}.
\end{equation*}
Since $\vv P \cdot \vv P = P^2 = E_0^2$, we square the equation to get our desired unknown on the right side:
\begin{equation*}
\begin{split}
(\vv P_h + \vv P_j) \cdot (\vv P_h + \vv P_j) &= \vv P_{\textrm{blob}} \cdot \vv P_{\textrm{blob}} \\[2pt]
E_{0 \mkern.5mu h}^2 + 2 (\vv P_h \cdot \vv P_j) + E_{0j}^2 &= E_{0 \, \textrm{blob}}^2.
\end{split}
\end{equation*}
If the dot product of two four-vectors weren't invariant, we'd be stuck here! Fortunately, it is, and we can evaluate $\vv P_h \cdot \vv P_j$ in any inertial frame we choose. Let's make it easy on ourselves and choose the rest frame of particle $j$, where $\vv P_j = \langle E_{0j}, \vv 0 \rangle$ and $\vv P_h = \langle E, \, \vv p c \rangle_h =  \langle \gamma E_{0 \mkern.5mu h}, \, \gamma E_{0 \mkern.5mu h} \vvbeta \rangle$. Now we obtain $\vv P_h \cdot \vv P_j = (\gamma E_{0 \mkern.5mu h})(E_{0j}) - 0$. Plugging that in:
\begin{equation*}
E_{0 \, \textrm{blob}}^2 = E_{0 \mkern.5mu h}^2 + 2 (\gamma E_{0 \mkern.5mu h} E_{0j}) + E_{0j}^2,
\end{equation*}
and
\begin{equation*}
E_{0 \, \textrm{blob}} = \sqrt{E_{0 \mkern.5mu h}^2 + 2 \gamma E_{0 \mkern.5mu h} E_{0j} + E_{0j}^2}.
\end{equation*}
This solution is also a nice illustration of the non-additivity of rest energy (mass): $E_{0 \, \textrm{blob}} \neq E_{0 \mkern.5mu h} + E_{0j}$, except in the classical limit when the approximation $\gamma \approx 1$ suffices. Note that $E_{0 \, \textrm{blob}}$ was equal to the rest energy of the closed \emph{system} the whole time; it's just that before the collision, some of it came from the kinetic energy of particles $h$ and $j$.


\subsubsection{Manifest Covariance}\label{sssec:mc}

Having established that the Minkowski dot product is invariant, we see that an equation written in terms of only four-vectors, their dot products, and Lorentz-invariants necessarily has the same form in all frames. Such an equation is said to be \textbf{manifestly covariant} or, informally, just \textbf{covariant}. (More generally, an equation written in terms of only four-\emph{tensors} is manifestly covariant, but we needn't worry about that for now.) In this context, the word \emph{covariant} means ``having the same form in all frames," and \emph{manifestly} means ``self-evidently." It's possible for an equation to be ``non-manifestly" Lorentz-covariant---for instance, $E_0^2 = E^2 - (\vv p c)^2$ holds for every inertial frame---but an equation like ${E_0^2 = \vv P \cdot \vv P}$ that's written in terms of four-vectors and Lorentz-invariants is \emph{manifestly} covariant. All laws of physics can be expressed in a manifestly covariant form.

Here's a ``trick": given an arbitrary inertial frame and an arbitrary four-vector, it's always possible to write a manifestly covariant expression for the value that any one of the vector's components takes in that frame. Say we're interested in the time component of a particle's four-momentum $\vv P$ (i.e., the particle's total energy $E$). The method is to define a four-vector $\vv E _{ct}$ whose components \emph{in the frame in question} are $\langle 1, 0, 0, 0 \rangle$ (where the $ct$-component gets the $1$ because it's the $ct$-component of $\vv P$ that we care about), and then just take the Minkowski dot product of the two vectors: ${\vv P \cdot \vv E_{ct} = (E)(1) - (cp_x)(0) - (cp_y)(0) - (cp_z)(0) = E}$. Note that $\vv E_{ct}$ is nothing but the frame's normalized four-velocity $\vv B$, since $\vv B = \langle \gamma, \gamma \vvbeta \rangle$ reduces to $\langle 1, \vv 0 \rangle$ in one's own rest frame. If it were one of $\vv P$'s \emph{spatial} components that we wanted---the $z$-component, say---then we'd define a vector $\vv E_z$ whose components are $\langle 0, 0, 0, 1 \rangle$ in the relevant frame, and we'd again take the dot product, though this time with a negative sign: $- \vv P \cdot \vv E_z = c p_z$.\footnote{Perhaps you've already pieced together that $\vv E_{ct}$, $\vv E_{x}$, $\vv E_{y}$, and $\vv E_z$ are \textbf{basis vectors}.}

\subsubsection[Orthogonality, (Normalized) Four-Acceleration]{Orthogonality, (``Normalized") Four-Acceleration}

So we have the four-displacement $\dd \vv R$, the normalized four-velocity $\vv B$, the four-momentum $\vv P$, and the four-force $\vv F$. Question: in what direction do they point? We began by \emph{defining} $\dd \vv R$ as tangent to a traveler's world line, pointing in whatever spacetime direction the traveler happens to be going. Common sense says that the same is true of the traveler's $\vv B$ and $\vv P$, but let's take a moment to think about what that means mathematically. Two vectors are codirectional if one is obtained by scaling the other by a positive number. We know that $\vv B$ and $\vv P$ are codirectional, since the latter is simply the former scaled by the traveler's $E_0$. Can we make a similar argument for $\dd \vv R$ and $\vv B$, and say that the latter is simply the former ``scaled" by the infinitesimal $1/(c \, \dd t_0)$? We can, but we need to be careful---we're \emph{not} saying that $\vv B$ is codirectional with $\vv R$! We're ``scaling" $\dd \vv R$, but \emph{differentiating} $\vv R$.

With that in mind, consider what happens when we take the $c t_0$-derivative of the four-velocity $\vv B$ to find the (``normalized") \textbf{four-acceleration}, $\vv Z$, analogous to the ``normalized" three-acceleration $\vvzeta=\dd \vvbeta / \dd (ct)$:\footnote{Like $\vvzeta$, $\vv Z$ isn't \emph{really} normalized (it isn't dimensionless, and its magnitude $Z$ can exceed one). It inherits the label (with scare quotes) to clarify that it isn't the ``unnormalized" four-acceleration $\vv A = \dd \vv V / \dd t_0 = \vv Z c^2$, which we have no use for.}
\begin{equation}\label{eq:34}
\boxed{
\begin{aligned}
\vv Z &\equiv \mathring{\vv B} \\[2pt]
&= \big \langle \mathring{\gamma}, \mathring{\vvomega} \big \rangle \\[3pt]
&= \gamma \mkern1.5mu \big \langle \dot{\gamma}, \dot{\vvomega} \big \rangle = \gamma \mkern1.5mu \big \langle \dot{\gamma}, \, \gamma \vvzeta + \dot{\gamma} \vvbeta \big \rangle
\end{aligned}
} \, ,
\end{equation}
and it's apparent that $\vv Z$ and $\vv B$ are \emph{not} codirectional. In fact, they're orthogonal! It must be so: $\vv B$ is always a unit vector, so only its direction can change.\footnote{If you don't see why constant $B$ means that $\vv B$ and $\vv Z$ are orthogonal, then think of the velocity and acceleration three-vectors in the familiar case of uniform circular motion.} And we can easily confirm it, because if two vectors are orthogonal then their dot product is zero. To test that, let's start by evaluating the $ct$-derivative of $\gamma$ (remembering that $\beta^2$ is actually $\vvbeta \cdot \vvbeta$):
\begin{equation}\label{eq:gd}
\begin{split}
\dot{\gamma} &= \dfrac{\dd}{\dd (ct)} \left[ \left(1-\beta^2 \right)^{-1/2} \right] \\[4pt]
&= \dfrac{\dd}{\dd ( \beta^2 )} \left[ \left( 1 - \beta^2 \right)^{-1/2} \right] \dfrac{\dd (\beta^2)}{\dd (ct)} \\[5pt]
&= \frac{1}{2} \left( 1 - \beta ^2 \right)^{-3/2} \, \dfrac{\dd}{\dd (ct)} \bigl( \vvbeta \cdot \vvbeta \bigr) \\[4pt]
&= \frac{\gamma^3}{2} \left[ ( \vvbeta \cdot \dot{\vvbeta} ) + ( \dot{\vvbeta} \cdot \vvbeta ) \right] \\[5pt]
\dot{\gamma} &= \gamma^3 (\vvbeta \cdot \vvzeta).
\end{split}
\end{equation}
Then we sub that result into Equation \ref{eq:34}:
\begin{equation}\label{eq:35}
\vv Z =\gamma \mkern1.5mu \big \langle \gamma^3 (\vvbeta \cdot \vvzeta), \, \gamma \vvzeta + \gamma^3 (\vvbeta \cdot \vvzeta) \vvbeta \big \rangle.
\end{equation}
And now we note that in a traveler's instantaneous rest frame, the three-velocity $\vvbeta = \vv 0$, which means that the spatial components of $\vv B$ are zero (Equation \ref{eq:31}) and the time component of $\vv Z$ is zero (Equation \ref{eq:35}). So the (invariant!) dot product $\vv B \cdot \vv Z$ is $(B^{ct})(0) - (0)(Z^x) - (0)(Z^y) - (0)(Z^z) = 0$. QED.\footnote{It's standard practice to use superscript for the components of a four-vector.}

Confession: we could have demonstrated that orthogonality more quickly in a manifestly covariant way:
\begin{equation*}
\begin{split}
\dfrac{\dd}{\dd (ct_0)} \left( \vv B \cdot \vv B \right) &= \dfrac{\dd}{\dd (c t_0)} \, B^2 \\[2pt]
2 \, \big( \mathring{\vv B} \cdot \vv B \big) &= \mathring{1} \\[3pt]
\vv Z \cdot \vv B &= 0 .
\end{split}
\end{equation*}
But our circuitous route was a good excuse to derive some useful equations.


\subsubsection{Proper Acceleration}\label{sssec:pa}

Equation \ref{eq:35} isn't quite so messy when the three-acceleration is parallel to the three-velocity. The time component doesn't look much different: ${Z^{ct} = \gamma^4 (\vvbeta \cdot \vvzeta) = \pm \, \gamma^4 \beta \zeta}$ (positive if $\vvbeta$ and $\vvzeta$ are codirectional, negative if they're contradirectional). The spatial component, however, gets much cleaner. Recognizing that parallel unit vectors can differ only by sign, we use $\vvzeta = \pm \, \zeta \hatbeta$ (where $\hatbeta = \vvbeta / \beta$ is the unit vector corresponding to velocity):
\begin{equation*}
\begin{split}
\gamma^2 \vvzeta + \gamma^4 (\vvbeta \cdot \vvzeta)\vvbeta &= \pm \, \gamma^2 \zeta \hatbeta + ( \pm \, \gamma^4 \beta \zeta) \beta \hatbeta \\
&= \pm \, \gamma^4 \zeta \hatbeta \left(\gamma^{-2} + \beta^2 \right)\\
&= \gamma^4 \vvzeta
\end{split}
\end{equation*}
(by Equation \ref{eq:20}), and so:
\begin{equation}\label{eq:fa}
\vv Z = \gamma^4 \langle \pm \, \beta \zeta, \vvzeta \rangle \quad \textrm{\footnotesize{ (for $\vvbeta \parallel \vvzeta$)}}.
\end{equation}
Then the magnitude of the four-acceleration is:
\begin{equation}\label{eq:36}
\begin{split}
Z &= \sqrt{\gamma^8 \beta^2 \zeta^2 - \gamma^8 \zeta^2} \\[2pt]
&= \gamma^3 \zeta \, \sqrt{\gamma^2 (\beta^2 - 1)} \\[2pt]
&= \gamma^3 \zeta \, \sqrt{\frac{\beta^2 - 1}{1 - \beta^2}} \\[2pt]
Z &= \gamma^3 \zeta \mkern1mu \mathrm{i} \quad \textrm{\footnotesize{ (for $\vvbeta \parallel \vvzeta$)}}.
\end{split}
\end{equation}
Ignore the $\mathrm{i}$ for now. Since $Z$ is Lorentz-invariant (it's the magnitude of a four-vector) and reduces to $\zeta$ in each of the traveler's successive instantaneous rest frames (where $\gamma = 1$), it must be true that Equation \ref{eq:36} allows \emph{all} qualifying inertial observers (those for whom the traveler's $\vvbeta \parallel \vvzeta$) to calculate the magnitude of acceleration measured in the traveler's frame by using their \emph{own} coordinate measurements of the traveler's $\gamma^3 \zeta$. For this reason, we call $Z / \mathrm{i}$ the magnitude of the (``normalized") \textbf{proper acceleration}. Proper acceleration is the three-acceleration a traveler actually experiences and measures with an on-board accelerometer. It's the traveler's acceleration as measured in the traveler's instantaneous rest frame. In keeping with our practice with other ``proper" quantities, we'll use the naught subscript for this one: $\vvzeta_{\mkern1mu 0}$.\footnote{``Unnormalized" proper acceleration is $\vv a_0 = \vvzeta_{\mkern1mu 0} c^2$. We won't use it.} Equation \ref{eq:36} tells us that when $\vvbeta \parallel \vvzeta$, the (``normalized") proper acceleration is $\vvzeta_{\mkern1mu 0} = \gamma^3 \vvzeta$, and its invariant magnitude is $\zeta_0 = \gamma^3 \zeta$ (where, again, $\gamma$ and $\zeta$ are coordinate measurements made by a qualifying inertial observer).

For \emph{arbitrary} angle $\theta$ between $\vvbeta$ and $\vvzeta$, computing $Z$ (and thus $\zeta_0$, we'll see) is more tedious, but not difficult. From Equation \ref{eq:35} (and Equation \ref{eq:20}):
\begin{equation}\label{eq:pa2}
\begin{split}
Z &= \sqrt{\vv Z \cdot \vv Z} = \sqrt{ \left[ \gamma^4 (\vvbeta \cdot \vvzeta) \right]^2 - \left[ \gamma^2 \vvzeta + \gamma^4 (\vvbeta \cdot \vvzeta) \vvbeta \right]^2}\\[2pt]
&= \sqrt{\gamma^8(\vvbeta \cdot \vvzeta)^2 - \gamma^4 \zeta^2 - 2\gamma^6 (\vvbeta \cdot \vvzeta)^2 - \gamma^8 (\vvbeta \cdot \vvzeta)^2 \beta^2 }\\[2pt]
&= \sqrt{(\vvbeta \cdot \vvzeta)^2 \Big[ \gamma^8 (1 - \beta^2) - 2\gamma^6 \Big] - \gamma^4 \zeta^2} \\[2pt]
Z &= \mathrm{i} \, \sqrt{\gamma^6 (\vvbeta \cdot \vvzeta)^2 + \gamma^4 \zeta^2} .
\end{split}
\end{equation}
That's one way to write it. Let's find a more intuitive way, starting like this:\footnote{The derivation that follows is a variation of an answer on Stack Exchange contributed by the user Pulsar: \url{https://physics.stackexchange.com/q/66853}.}
\begin{equation*}
Z = \gamma^3 \zeta \mkern1mu \mathrm{i} \, \sqrt{\frac{(\vvbeta \cdot \vvzeta)^2}{\zeta^2} + \frac{1}{\gamma^2}}.
\end{equation*}
Looks awkward, but we can make good sense of that with a little trigonometry. By the geometric definition of the Euclidean dot product, we know that $\vvbeta \cdot \vvzeta = \beta \zeta \cos{\theta}$, so we can substitute $\beta^2 \cos^2{\theta}$ for $(\vvbeta \cdot \vvzeta)^2 / \zeta^2$, and we have:
\begin{equation*}
\begin{split}
Z &= \gamma^3 \zeta \mkern1mu \mathrm{i} \, \sqrt{\beta^2 \cos^2{\theta} + 1 - \beta^2}\\
&= \gamma^3 \zeta \mkern1mu \mathrm{i} \, \sqrt{\beta^2 (\cos^2{\theta} - 1) + 1}\\
&= \gamma^3 \zeta \mkern1mu \mathrm{i} \, \sqrt{1 - (\beta \sin{\theta})^2}.
\end{split}
\end{equation*}
Since $(\beta \sin{\theta})$ is just the component of $\vvbeta$ that's perpendicular to $\vvzeta$ ($\beta_{\perp \vvzeta}$):
\begin{equation*}
Z = \gamma^3 \zeta \mkern1mu \mathrm{i} \, \sqrt{1 - \beta_{\perp \vvzeta}^2}.
\end{equation*}
And if we define $\gamma_{\perp} \equiv (1 - \beta_{\perp \vvzeta}^2)^{-1/2}$:
\begin{equation}\label{eq:37}
Z = \frac{\gamma^3 \zeta}{\gamma_{\perp}} \, \mathrm{i}.
\end{equation}
That's $\zeta \mkern1mu \mathrm{i}$ in the traveler's instantaneous rest frame, so we can now say that the (``normalized") proper acceleration for arbitrary $\theta$ is:
\begin{equation}\label{eq:pa4}
\vvzeta_{\mkern1mu 0} = \dfrac{Z}{\mathrm{i}} \, \hatzeta ,
\end{equation}
with invariant magnitude $\zeta_0 = [ \gamma^6 (\vvbeta \cdot \vvzeta)^2 + \gamma^4 \zeta^2 ]^{-1/2} = \gamma^3 \zeta / \gamma_{\perp}$ (by Equations \ref{eq:pa2} and \ref{eq:37}). When $\vvbeta \parallel \vvzeta$, Equation \ref{eq:37} reduces to Equation \ref{eq:36} (because then $\gamma_{\perp} = 1$). Another important special case is when $\vvbeta \perp \vvzeta$, as in uniform circular motion. Then $\gamma_{\perp} = \gamma$, and we get $\vvzeta_{\mkern1mu 0} = \gamma^2 \vvzeta$.


\subsubsection{Classifying Four-Vectors}

We see now that the magnitude $Z$ of the four-acceleration \emph{must} contain the imaginary unit. Why $\mathrm{i}$? Because four-acceleration is orthogonal to four-velocity, and $\mathrm{i}$ \emph{never} appears in the magnitude of four-velocity.

That warrants some fleshing out.

The $\mathrm{i}$ is physically meaningless, an artifact of the ($+$$-$$-$$-$) sign convention we've adopted (see Footnote \ref{fn:sc}). What's physically meaningful is that four-acceleration has something in common with spacelike intervals---namely, the sign of its squared magnitude (negative for us). Likewise, the squared magnitude of the four-velocity (Equation \ref{eq:31}) is \emph{positive} under our sign convention, which is the distinguishing characteristic of \emph{timelike} intervals. And the squared magnitude of the four-displacement (Equation \ref{eq:30}) or the four-momentum (Equation \ref{eq:32}) can be either positive ($\beta < 1$) or zero ($\beta = 1$), like a timelike or lightlike interval.\footnote{The magnitude of a four-displacement \emph{is} a spacetime interval! More broadly, the magnitude of a four-\emph{separation} is a spacetime interval (an imaginary number if the interval is spacelike)---see Footnote \ref{fn:sp}.} A way of classifying four-vectors suggests itself:
\begin{itemize}
\item A four-vector is spacelike if the magnitude of its spatial three-vector component is greater than the absolute value of its time component.
\item A four-vector is timelike if the opposite is true.
\item A four-vector is lightlike (null) if the magnitude of its spatial three-vector component is equal to the absolute value of its time component.
\end{itemize}
Here are some rules about four-vector orthogonality:
\begin{itemize}
\item The zero four-vector \mbox{\boldmath $\emptyset$} is the lightlike vector with zeros for components, orthogonal to all four-vectors (dot product with it is always zero).
\item A non-zero four-vector that's orthogonal to a timelike vector is necessarily spacelike. This is easily seen by choosing the frame in which the timelike vector has zeros for its \emph{spatial} components (such a frame exists for every timelike vector); any second vector that could yield zero when dotted with the first must have a zero \emph{time} component in that frame. That's why the four-acceleration has an imaginary magnitude---it's spacelike because it's orthogonal to the timelike four-velocity, and spacelike vectors have imaginary magnitudes under our sign convention.
\item A non-zero four-vector that's orthogonal to a spacelike vector can be timelike (see above), but also spacelike or lightlike. For instance, the spacelike vectors $\langle 0, 1, 0, 0 \rangle$ and $\langle 0, 0, 1, 0 \rangle$ are orthogonal, and the lightlike vector $\langle 1, 0, 0, 1 \rangle$ is orthogonal to both of them.
\item A non-zero four-vector that's orthogonal to a non-zero lightlike vector must be either spacelike (see above) or lightlike (e.g., $\langle 1, 0, 0, 1 \rangle$ and $\langle 2, 0, 0, 2 \rangle$).
\end{itemize}

What about the four-force (Equation \ref{eq:33})? Spacelike, timelike, or lightlike? In the special case that $E_0$ changes but $\vv B$ doesn't (as with the light-emitting body in Einstein's thought experiment), $\vv F$ is timelike because it's parallel to the timelike $\vv B$:
\begin{equation}\label{eq:38}
\begin{split}
\vv F &= \mathring{\vv P}\\
&= \frac{\dd}{\dd (c t_0)} \left( E_0 \mkern.5mu \vv B \right) \\[4pt]
\vv F &= \mathring E_0 \mkern.5mu \vv B \quad \textrm{\footnotesize{ (for $\vv Z = \textrm{\mbox{\boldmath $\emptyset$}}$)}}.
\end{split}
\end{equation}
But in most applications it's $E_0$ that remains constant while $\vv B$ changes. In that case $\vv F$ is spacelike (codirectional with $\mathring{\vv B} = \vv Z$, orthogonal to $\vv B$):
\begin{equation}\label{eq:39}
\vv F = E_0 \mkern.5mu \vv Z \quad \textrm{\footnotesize{ (for $\mathring{E}_0 = 0$)}}.
\end{equation}
This says symbolically what we've already put into words: rest energy is the measure of ``spacetime inertia," the resistance to change in (direction of) four-velocity $\vv B$.\footnote{Equivalently, $\vv F = m \vv A$ when $\mathring{m} = 0$ (since $E_0 = mc^2$ and $\vv Z = \vv A / c^2$).}

If \emph{both} $E_0$ and $\vv B$ change (the general case), then the product rule gives us $\vv F = \mathring E_0 \mkern.5mu \vv B + E_0 \mkern.5mu \vv Z$. Such a four-force might be spacelike, timelike, or even lightlike, depending on whether the magnitude of the spatial component (three-force) is greater than, less than, or equal to the absolute value of the time component (power).

\subsubsection{Proper Power, Proper Force}\label{sssec:pf}

When we discussed the four-force in Section \ref{sssec:ff}, we didn't bother finding its magnitude $F$. We could have ``plug-and-chugged" with the temporal and spatial components (like we later did to calculate the four-acceleration's magnitude), but that would have been a chore. Now that we've covered the four-acceleration $\vv Z$ and the proper acceleration $\vvzeta_{\mkern1mu 0}$ (with invariant magnitude $Z/\mathrm{i}$), we have a better option, manifestly covariant:
\begin{equation}\label{eq:40}
\begin{split}
F &= \sqrt{\vv F \cdot \vv F} = \sqrt{\mathring{\vv P} \cdot \mathring{\vv P}} \\[2pt]
&= \sqrt{ \left( \mathring{E}_0 \mkern.5mu \vv B + E_0 \mkern.5mu \vv Z \right) \cdot \left( \mathring{E}_0 \mkern.5mu \vv B + E_0 \mkern.5mu \vv Z \right) }\\[2pt]
&= \sqrt{ \mathring{E}_0^2 \left( \vv B \cdot \vv B \right) + 2 E_0 \mathring{E}_0 \left( \vv B \cdot \vv Z \right) + E_0^2 \left( \vv Z \cdot \vv Z \right) } \\[2pt]
F &= \sqrt{ \mathring{E}_0^2 - \left( E_0 \vvzeta_{\mkern1mu 0} \right)^2 }
\end{split}
\end{equation}
because $\vv B$ and $\vv Z$ are orthogonal, $\vv B \cdot \vv B = B^2 = 1$, and $Z^2 = - (\vvzeta_{\mkern1mu 0} \cdot \vvzeta_{\mkern1mu 0})$.\footnote{Because the four-force concept is inapplicable to ``restless" (massless) systems (see Section \ref{sssec:ff}), we don't lose generality by working with the rest energy. Whether the concepts of power and three-force are likewise inapplicable to such systems is a thornier question. The simple answer is yes, they're inapplicable. A more complete answer would wrestle with optical phenomena like reflection and refraction. For instance, should we regard a reflected light wave as the same system before and after reflection? (Probably not.) And does it even make sense to label an electromagnetic wave ``massless" when it's traveling at $\beta < 1$ through a medium? (Probably not, and such a wave is better described as a \emph{polarization} wave.) There's also gravitational redshift/blueshift and gravitational lensing, but these are topics for general relativity, and it would be misleading to naively use the words \emph{power} and \emph{force} here.}

All right, but that four-vector magnitude is less straightforward than the others we've seen. What does it tell us? First, note that it accords with Equations \ref{eq:38} and \ref{eq:39}: when $\vv B$ is constant ($\vvzeta_{\mkern1mu 0} = \vv 0$), $\vv F = \mathring{E}_0 \vv B$ and $F = \mathring{E}_0 B = \mathring{E}_0$; when $E_0$ is constant, $\vv F = E_0 \vv Z$ and $F = E_0 Z = E_0 \mkern.5mu \zeta_0 \mkern1mu \mathrm{i}$. And what \emph{are} these quantities $\mathring{E}_0$ and $E_0 \mkern.5mu \zeta_0$? They're certainly both invariant---the first is the derivative of the invariant rest energy with respect to the invariant $c t_0$, and the second is the invariant rest energy times the invariant magnitude of the proper acceleration.\footnote{This is a good time to remind ourselves of the caveats we mentioned at the beginning of Section \ref{sssec:ff}. First, we're still modeling any accelerating system as a point particle. Second, the relativity of simultaneity means that for an open composite system, it's not always possible to speak of the rest energy (or its proper-time derivative!) as ``invariant."} But they also represent the power and three-force (magnitude) that the traveler ``feels," or that an observer in any of the traveler's successive instantaneous rest frames measures. Why? For power:
\begin{equation}\label{eq:po}
\begin{split}
\mathcal{P} &= \dot{E}\\
&= \dot{\gamma} E_0 + \gamma \dot{E}_0\\
\mathcal{P} &= \gamma^3 (\vvbeta \cdot \vvzeta) E_0 + \mathring{E}_0
\end{split}
\end{equation}
(by Equation \ref{eq:gd}), where only the second term remains in the traveler's instantaneous rest frame ($\vvbeta = \vv 0$).\footnote{Same argument could be made for Equation \ref{eq:rp}, which is equivalent to Equation \ref{eq:po}.} For three-force:
\begin{equation}\label{eq:41}
\begin{split}
\vv f &= \dot{\vv p} c\\
&= \dot{\gamma} E_0 \vvbeta + \gamma \dot{E}_0 \vvbeta + \gamma E_0 \dot{\vvbeta}\\
\vv f &=\gamma^3 (\vvbeta \cdot \vvzeta) E_0 \vvbeta + \mathring{E}_0 \vvbeta + E \vvzeta,
\end{split}
\end{equation}
where, again, only the last term remains in the traveler's instantaneous rest frame, but its magnitude there is $E_0 \mkern.5mu \zeta_0$, our desired invariant.

So inertial observers can use their own coordinate measurements of the traveler's $\mathring{E}_0 = \gamma \dot{E}_0$ and $E_0 \mkern.5mu \zeta_0 = E_0 \gamma^3 \zeta / \gamma_{\perp}$ (Equation \ref{eq:37}) to calculate the traveler's power and three-force (magnitude) as measured in the traveler's instantaneous rest frame. These are therefore ``proper" quantities: the \textbf{proper power} $\mathcal{P}_0 \equiv \mathring{E}_0$, and the \textbf{proper force} $\vv f_0 \equiv E_0 \vvzeta_{\mkern1mu 0}$ ($= [mc^2][\vv a_0 / c^2] = m \vv a_0$).\footnote{Beware that another convention defines proper power and proper force as something else entirely---$\dd E / \dd t_0$ and $\dd \vv p / \dd t_0$, in keeping with defining proper velocity ``improperly" as $\dd \vv r / \dd t_0$ (see Footnote \ref{fn:pr} in Section \ref{sssec:ce}).} Equation \ref{eq:40} can now be expressed more intelligibly:
\begin{equation}\label{eq:42}
F = \sqrt{\mathcal{P}_0^2 - (\vv f_0 \cdot \vv f_0)} .
\end{equation}

Now, about this proper force: we've got $\vv f_0 = E_0 \vvzeta_{\mkern1mu 0}$, which in the traveler's instantaneous rest frame is simply $\vv f = E_0 \vvzeta$ ($= m \vv a$). Is the point of all this merely to restate the mundane fact that the relativistic three-force reduces to the Newtonian three-force in the classical limit? No! The point is that the magnitude $f_0 = E_0 \mkern.5mu \zeta_0$ ($= m a_0$) is an invariant whose value is calculable by all inertial observers. In $\vv f_0 = E_0 \vvzeta_{\mkern1mu 0}$ $(= m \vv a_0)$, then, we have something like a relativistic version of Newton's $\vv f_{\mathrm{classical}} = m \vv a$, but valid even when $\dot{E}_0 \neq 0$ ($\dot{m} \neq 0$).\footnote{In principle, the concept of proper force is applicable to Newtonian mechanics, too, provided one has defined classical force as the time-derivative of momentum (rather than as $m \vv a$---see Footnote \ref{fn:cf}). But it could only be relevant in a variable-mass situation. That's because acceleration-magnitude is invariant in the classical limit. Proper acceleration differs appreciably from ordinary coordinate acceleration only when $\beta \centernot{\ll} 1$.}


\subsection[Measure of Inertia Revisited]{``Measure of Inertia" Revisited}\label{ssec:in}

We see in Equation \ref{eq:41} that $\vv f \neq E_0 \vvzeta$ $( = m \vv a)$, even when $\dot{E}_0 = 0$ ($\dot{m} = 0$ being the condition under which $\vv f_\textrm{classical} = m \vv a$ is true). That's not surprising; back in Section \ref{ssec:rm} we ruled out the possibility that $E_0$ is the resistance to change in three-velocity for arbitrary $\vvbeta$. More interesting is that our \emph{candidate} for the relativistic measure of inertia, total energy $E$, likewise cannot be unambiguously described as the resistance to change in velocity under the influence of a three-force. Yes, the last term in Equation \ref{eq:41} is $E \vvzeta$, and the middle term disappears when $\dot{E}_0 = 0$, but there's still that pesky first term that doesn't vanish unless $\vvbeta = \vv 0$:
\begin{equation}\label{eq:43}
\vv f = \gamma^2 E \mkern1mu (\vvbeta \cdot \vvzeta) \vvbeta + E \vvzeta \quad \textrm{\footnotesize{ (for $\dot{E}_0 = 0$)}}.
\end{equation}

To gain a bit more insight into the relationship between force and acceleration, let's isolate $\vvzeta$ on one side of the equation so that we can compare the result with the Newtonian $\vvzeta = \vv f_{\mathrm{classical}} / E_0$. First we'll need to factor out that $(\vvbeta \cdot \vvzeta)$ on the right side, which we accomplish by taking the dot product of Equation \ref{eq:43} with the normalized three-velocity:
\begin{equation*}
\begin{split}
\vv f \cdot \vvbeta &= \left( \gamma^2 E \mkern1mu (\vvbeta \cdot \vvzeta) \vvbeta + E \vvzeta \right) \cdot \vvbeta \\
&= \gamma^2 E \mkern1mu (\vvbeta \cdot \vvzeta)(\beta^2 + \gamma^{-2}) \\
&= \gamma^2 E \mkern1mu (\vvbeta \cdot \vvzeta)
\end{split}
\end{equation*}
(by Equation \ref{eq:20}). Subbing that back into Equation \ref{eq:43}, we find:
\begin{equation}\label{eq:44}
\begin{split}
\vv f &= (\vv f \cdot \vvbeta) \vvbeta + E \vvzeta \\[5pt]
\vvzeta &= \frac{\vv f - (\vv f \cdot \vvbeta) \vvbeta}{E} \quad \textrm{\footnotesize{ (for $\dot{E}_0 = 0$)}}.
\end{split}
\end{equation}
Total energy $E$, we see, is the resistance to change in velocity not just under the influence of a three-force, but rather under the influence of said force \emph{given a particular velocity}. Fix the $E$ and $\vv f$, and the acceleration $\vvzeta$ still depends on the magnitude and direction of the three-velocity $\vvbeta$.

Does this mean that there is no universal ``measure of inertia"? That depends on how we define ``measure of inertia." There are at least five reasonable(-ish) possibilities:
\begin{enumerate}
\item If we define it as the scalar relating three-force $\vv f$ and three-acceleration $\vvzeta$ (when $\dot{E}_0 = 0$), then no, it doesn't exist beyond the classical limit.
\item If we define it as the scalar relating three-momentum $\vv p c$ and three-velocity $\vvbeta$, then the measure of inertia is total energy $E = \gamma E_0$, a function of speed (\emph{not} a constant of proportionality!).\footnote{\label{fn:rm}Some who take this view prefer in certain contexts to express $E$ in units of mass and call it the \textbf{relativistic mass}, $m_{\textrm{r}} \equiv E / c^2$ ($= \gamma m$). That way, $\vv p = m_{\textrm{r}} \vv v$, which jibes with the classical Equation \ref{eq:1}. Some people even reserve the term \emph{mass} and the symbol $m$ for this frame-dependent quantity, and then use the term \textbf{rest mass} (or \textbf{proper mass}, or \textbf{invariant mass}) and the symbol $m_0$ to refer to the invariant scalar that we know as $m$. Under that convention, $m = E / c^2$ ($ = \gamma m_0$), and $\vv p = m \vv v$ (jibes even better). It's become unfashionable to express total energy in mass units, but the practice still has its adherents.}
\item If we define it as the constant of proportionality relating proper force $\vv f_0$ and proper acceleration $\vvzeta_{\mkern1mu 0}$, then it's rest energy $E_0$ (or mass), as it is in Newtonian mechanics.
\item If we define it as the constant of proportionality relating $\vv f$ and $\dot{\vvomega}$ (the $ct$-derivative of \emph{celerity}), then again it's $E_0$. To see this, differentiate Equation \ref{eq:29} with respect to $ct$ (for $\dot{E}_0 = 0$).
\item If we define it as the constant of proportionality relating four-force $\vv F$ and four-acceleration $\vv Z$ (or indeed relating four-momentum $\vv P$ and four-velocity $\vv B$), then yet again it's $E_0$, as we've seen, though we've called this the measure of ``\emph{spacetime} inertia" (Equation \ref{eq:39}).
\end{enumerate}

But enough semantics!

Have another look at Equation \ref{eq:44}, and note that the force and acceleration three-vectors \emph{aren't even parallel}, unless the force happens to be either perpendicular or parallel to the three-velocity $\vvbeta$. If $\vv f \perp \vvbeta$, then $(\vv f \cdot \vvbeta) \vvbeta = \vv 0$:
\begin{equation}\label{eq:45}
\vvzeta = \frac{\vv f}{E} \quad \textrm{\footnotesize{ (for $\dot{E}_0 = 0$ and $\vv f \perp \vvbeta$)}}.
\end{equation}
If $\vv f \parallel \vvbeta$, then $\vv f \cdot \vvbeta = \pm \, f \beta$, and $\vvbeta = \pm \, \beta \mkern1mu \vv{\hat{f}}$ (where $\vv{\hat{f}}$ is the unit vector $\vv f / f$):
\begin{equation*}
\begin{split}
\vv f - (\vv f \cdot \vvbeta) \vvbeta &= f \mkern1mu \vv{\hat{f}} - (\pm \, f \beta) (\pm \, \beta \mkern1mu \vv{\hat{f}}) \\[2pt]
&= \vv f \mkern1mu (1 - \beta^2) \\[2pt]
&= \frac{\vv f}{\gamma^2},
\end{split}
\end{equation*}
such that
\begin{equation}\label{eq:46}
\vvzeta = \frac{\vv f}{\gamma^2 E} \quad \textrm{\footnotesize{ (for $\dot{E}_0 = 0$ and $\vv f \parallel \vvbeta$)}}.
\end{equation}
Recall from Equation \ref{eq:36} that the proper acceleration $\vvzeta_{\mkern1mu 0}$ is $\gamma^3 \vvzeta$ when $\vvzeta \parallel \vvbeta$, a condition that holds here. So Equation \ref{eq:46} gives:
\begin{equation*}
\vv f = \gamma^3 E_0 \vvzeta = E_0 \vvzeta_{\mkern1mu 0} = \vv f_0 ,
\end{equation*}
meaning that when $\dot{E}_0 = 0$, the magnitude of three-force is invariant among observers for whom $\vv f \parallel \vvbeta$ (not all observers!).

If $\vv f$ is neither perpendicular nor parallel to $\vvbeta$, then Equations \ref{eq:45} and \ref{eq:46} point us toward the \emph{component} vectors of $\vvzeta$ relative to $\vvbeta$, and with a minor notational adjustment we can express $\vvzeta$ as their sum:
\begin{equation}\label{eq:47}
\vvzeta = \frac{\vv f_{\perp \vvbeta}}{E} + \frac{\vv f_{\parallel \vvbeta}}{\gamma^2 E} \quad \textrm{\footnotesize{ (for $\dot{E}_0 = 0$)}},
\end{equation}
where $\vv f_{\perp \vvbeta}$ and $\vv f_{\parallel \vvbeta}$ are the component vectors of $\vv f$ perpendicular and parallel to $\vvbeta$. Equation \ref{eq:47} shows us exactly how inertia depends on direction. By a factor of $\gamma^2$, an object resists acceleration along its axis of motion more than it does against it.\footnote{By $\vvzeta = \vv a / c^2$ and $E = \gamma mc^2$, Equation \ref{eq:47} is equivalently $\vv a = \vv f_{\perp \vv v} / (\gamma m) + \vv f_{\parallel \vv v} / (\gamma^3 m)$. In this context, the quantities $\gamma m$ and $\gamma^3 m$ are sometimes respectively called the \textbf{transverse mass} and the \textbf{longitudinal mass}, to emphasize the dependence of inertia on direction. Like the relativistic mass (see Footnote \ref{fn:rm}), these concepts have generally fallen out of favor. Today most physicists speak only of one mass: Newton's invariant $m$, which Einstein showed is non-additive and equal to $E_0 / c^2$.}

Finally, we can take advantage of matrix notation to relate $\vv f$ and $\vvzeta$ in a more compact and quasi-familiar way. First we rewrite Equation \ref{eq:43}:
\begin{equation*}
\begin{split}
\vv f &= \gamma^2 E \vvbeta \mkern1mu (\vvbeta \cdot \vvzeta) + E \vvzeta \\
[\vv f] &= \gamma^2 E [\vvbeta] [\vvbeta]^{\mathrm{T}} [\vvzeta] + E [\vvzeta] ,
\end{split}
\end{equation*}
where square brackets around a vector indicate its column-matrix representation (Cartesian components), and the superscript $\mathrm{T}$ indicates the transpose (so $[\vvbeta]^{\mathrm{T}}$ is a row matrix and $[\vvbeta]^{\mathrm{T}} [\vvzeta]$ represents the dot product $\vvbeta \cdot \vvzeta$). Now, because the matrix product is distributive over matrix addition, we can simply ``factor out" $[\vvzeta]$ (but only to the right---the matrix product is \emph{not} commutative):
\begin{equation*}
[\vv f] = (\gamma^2 E [\vvbeta] [\vvbeta]^{\mathrm{T}} + E I_3) [\vvzeta]
\end{equation*}
($I_3$ is the identity matrix). What remains in the parentheses is a 3-by-3, $[\vvbeta] [\vvbeta]^{\mathrm{T}}$ being the matrix representation of the \textbf{dyadic product} $\vvbeta \otimes \vvbeta$.\footnote{We'll examine the dyadic product more closely in Section \ref{sec:dy}.} Label it $\mathcal{E} \equiv E(\gamma^2 [\vvbeta] [\vvbeta]^{\mathrm{T}} + I_3)$. Then Equations \ref{eq:43} and \ref{eq:44} can be notated:
\begin{equation}\label{eq:48}
[\vv f] = \mathcal{E} [\vvzeta] \quad \textrm{\footnotesize{ (for $\dot{E}_0 = 0$)}}
\end{equation}
and
\begin{equation}\label{eq:49}
[\vvzeta] = \mathcal{E}^{-1} [\vv f] \quad \textrm{\footnotesize{ (for $\dot{E}_0 = 0$)}},
\end{equation}
where $\mathcal{E}^{-1} = (I_3 - [\vvbeta] [\vvbeta]^{\mathrm{T}})/E$ is the matrix inverse of $\mathcal{E}$.\footnote{Those who prefer to work with $m$, $\vv v$, and $\vv a$ can instead use:
\begin{equation*}
[\vv f] = \mathcal{M} [\vv a] \qquad \qquad [\vv a] = \mathcal{M}^{-1} [\vv f],
\end{equation*}
where $\mathcal{M} \equiv \mathcal{E} / c^2 = \gamma m (\gamma^2 [\vv v] [\vv v]^{\mathrm{T}}/c^2 + I_3) =  m_{\textrm{r}}(\gamma^2 [\vv v] [\vv v]^{\mathrm{T}}/c^2 + I_3)$ (see Footnote \ref{fn:rm}).} In the classical limit, $\mathcal{E} \approx E_0 I_3$, giving $[\vv f] \approx E_0 I_3 [\vvzeta] = E_0 [\vvzeta]$ $(= m [\vv a])$, as expected.

Equations \ref{eq:48} and \ref{eq:49} are nice! Perhaps the matrices $\mathcal{E}$ and $\mathcal{E}^{-1}$ are a sixth candidate for ``measure of inertia."\footnote{Or better, the \emph{geometric quantities that the matrices represent} (see Section \ref{sec:dy}).}



\subsection{Lorentz Transformation Revisited}

We've spent a lot of time on invariant ``proper" quantities that allow observers to figure out what's happening in a traveler's (instantaneous) rest frame. That was time well spent, but what if an observer is less interested in a traveler's rest frame than in some other observer's measurements of the traveler's observables? Or what if there isn't a traveler at all, and an observer just wants to know what the components of some four-vector are in someone else's rest frame?\footnote{Most of the four-vectors we've considered pertain directly to something that's moving, but that's not a requirement. For instance, we're free to construct a four-separation that ``connects" a pair of spacelike-separated events and whose magnitude is the spacetime interval between them (an imaginary number in this case)---see Footnote \ref{fn:sp}.}

We already have the answer: four-vector components obey the Lorentz transformation. Let's now focus on this for a bit. We'll stick with standard-configuration Lorentz boosts. Matrix notation served us well at the end of the previous section, and as we apply the Lorentz transformation to four-vectors we'll find it convenient to use matrix notation once again.

Naturally, we begin with the transformation of the components of instantaneous four-displacement, which is just the infinitesimal version of Equations \ref{eq:lt2} (inverse transformation on the right side):
\begin{equation}\label{eq:fd}
\begin{aligned}
c \, \dd t^{\prime} &= \gamma \left( c \, \dd t - \beta \, \dd x \right) & c \, \dd t &= \gamma \left( c \, \dd t^\prime + \beta \, \dd x^\prime \right) \\
\dd x^{\prime} &= \gamma \left( \dd x - \beta \mkern1mu c \, \dd t \right) & \qquad  \dd x &= \gamma \left( \dd x^\prime + \beta \mkern1mu c \, \dd t^\prime \right) \\
\dd y^{\prime} &= \dd y& \dd y&= \dd y^\prime \\
\dd z^{\prime} &= \dd z& \dd z&= \dd z^\prime .
\end{aligned}
\end{equation}
Each of these systems of equations can be expressed as the matrix product of a 4-by-4 matrix and a four-displacement notated as a column:
\begin{equation*}
\begin{bmatrix}
c \, \dd t^\prime \\ \dd x^\prime \\ \dd y^\prime \\ \dd z^\prime
\end{bmatrix}
=
\begin{bmatrix}
\gamma & -\omega & 0 & 0 \\
- \omega & \gamma & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
c \, \dd t \\ \dd x \\ \dd y \\ \dd z
\end{bmatrix}
\qquad
\begin{bmatrix}
c \, \dd t \\ \dd x \\ \dd y \\ \dd z
\end{bmatrix}
=
\begin{bmatrix}
\gamma & \omega & 0 & 0 \\
\omega & \gamma & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
c \, \dd t^\prime \\ \dd x^\prime \\ \dd y^\prime \\ \dd z^\prime
\end{bmatrix}
\end{equation*}
(using the celerity $\omega = \gamma \beta$ just to keep it fresh in our minds). We call the 4-by-4 on the left the \textbf{Lorentz boost matrix} (for a boost along the $x$-axis), and we give it the symbol $\Lambda$:
\begin{equation}\label{eq:bm}
\Lambda =
\begin{bmatrix}
\gamma & -\gamma \beta & 0 & 0 \\
- \gamma \beta & \gamma & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix} .
\end{equation}
The other 4-by-4 is its inverse (Equation \ref{eq:20} is helpful for verifying this), the \textbf{inverse Lorentz boost matrix}:
\begin{equation}\label{eq:im}
\Lambda^{-1} =
\begin{bmatrix}
\gamma & \gamma \beta & 0 & 0 \\
\gamma \beta & \gamma & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix} .
\end{equation}
We can now write the Lorentz transformation of the components of a four-displacement $\dd \vv R$ (and its inverse) in a single line:
\begin{equation*}
[\dd \vv R^\prime] = \Lambda \, [\dd \vv R] \qquad \qquad [\dd \vv R] = \Lambda^{-1} \, [\dd \vv R^\prime] .
\end{equation*}
Indeed, we can do the same for an arbitrary four-vector $\vv Q$:
\begin{equation*}
[\vv Q^\prime] = \Lambda \mkern1mu [\vv Q] \qquad \qquad [\vv Q] = \Lambda^{-1} \mkern1mu [\vv Q^\prime] .
\end{equation*}
As we'll see, the details of a transformation can be an eyesore, so these one-line matrix equations are a fine thing.

Care is required when the four-vector's components contain speeds or Lorentz factors (e.g., four-velocity and four-acceleration). We mustn't confuse these $\beta$'s and $\gamma$'s with the $\beta$'s and $\gamma$'s in the boost matrices. Our solution, remember, is to add a ``rel" subscript to the $\beta$'s and $\gamma$'s that correspond to the relative motion of the primed and unprimed frames---that is, we notate the boost parameters $\beta_{\mathrm{rel}}$ and $\gamma_{\mathrm{rel}}$.

\subsubsection{Acceleration Transformed}

We've already explicitly written out the Lorentz transformation for the components of four-displacement (Equations \ref{eq:lt2} and \ref{eq:fd}), four-velocity (Equations \ref{eq:fv}), and four-momentum (Equations \ref{eq:fm}). Let's go ahead and do the same for the components of four-acceleration (Equation \ref{eq:34}).
\begin{equation*}
[\vv Z^\prime] = \Lambda \mkern1mu [\vv Z] \qquad \qquad [\vv Z] = \Lambda^{-1} \mkern1mu [\vv Z^\prime] ,
\end{equation*}
i.e.,
\begin{equation*}
\begin{bmatrix}
Z^{\prime \mkern1.5mu ct} \\[.15 em]
Z^{\prime \mkern1.5mu x} \\[.15 em]
Z^{\prime \mkern1.5mu y} \\[.15 em]
Z^{\prime \mkern1.5mu z}
\end{bmatrix}
=
\begin{bmatrix}
\gamma_{\textrm{rel}} & -\gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & 0 & 0 \\[.15 em]
- \gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & \gamma_{\textrm{rel}} & 0 & 0 \\[.15 em]
0 & 0 & 1 & 0 \\[.15 em]
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
Z^{ct} \\[.15 em]
Z^{x} \\[.15 em]
Z^{y} \\[.15 em]
Z^{z}
\end{bmatrix}
\end{equation*}
and
\begin{equation*}
\begin{bmatrix}
Z^{ct} \\[.15 em]
Z^{x} \\[.15 em]
Z^{y} \\[.15 em]
Z^{z}
\end{bmatrix}
=
\begin{bmatrix}
\gamma_{\textrm{rel}} & \gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & 0 & 0 \\[.15 em]
\gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & \gamma_{\textrm{rel}} & 0 & 0 \\[.15 em]
0 & 0 & 1 & 0 \\[.15 em]
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
Z^{\prime \mkern1.5mu ct} \\[.15 em]
Z^{\prime \mkern1.5mu x} \\[.15 em]
Z^{\prime \mkern1.5mu y} \\[.15 em]
Z^{\prime \mkern1.5mu z}
\end{bmatrix}
\end{equation*}
(we use superscript for four-vector components, recall). Equivalently:
\begin{equation*}
\begin{bmatrix}
\gamma^\prime \dot{\gamma}^\prime \\[.25 em]
\gamma^{\prime} \dot{\omega}_x^{\prime} \\[.25 em]
\gamma^{\prime} \dot{\omega}_y^{\prime} \\[.25 em]
\gamma^{\prime} \dot{\omega}_z^{\prime} \\[.1 em]
\end{bmatrix}
=
\begin{bmatrix}
\gamma_{\textrm{rel}} & -\gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & 0 & 0 \\[.25 em]
- \gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & \gamma_{\textrm{rel}} & 0 & 0 \\[.25 em]
0 & 0 & 1 & 0 \\[.25 em]
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
\gamma \dot{\gamma} \\[.25 em]
\gamma \dot{\omega}_x \\[.25 em]
\gamma \dot{\omega}_y \\[.25 em]
\gamma \dot{\omega}_z \\[.1 em]
\end{bmatrix}
\end{equation*}
and
\begin{equation*}
\begin{bmatrix}
\gamma \dot{\gamma} \\[.25 em]
\gamma \dot{\omega}_x \\[.25 em]
\gamma \dot{\omega}_y \\[.25 em]
\gamma \dot{\omega}_z \\[.1 em]
\end{bmatrix}
=
\begin{bmatrix}
\gamma_{\textrm{rel}} & \gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & 0 & 0 \\[.25em]
\gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & \gamma_{\textrm{rel}} & 0 & 0 \\[.25 em]
0 & 0 & 1 & 0 \\[.25 em]
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
\gamma^\prime \dot{\gamma}^\prime \\[.25 em]
\gamma^{\prime} \dot{\omega}_x^{\prime} \\[.25 em]
\gamma^{\prime} \dot{\omega}_y^{\prime} \\[.25 em]
\gamma^{\prime} \dot{\omega}_z^{\prime} \\[.1 em]
\end{bmatrix}
\end{equation*}
(where a dot over anything primed signifies its $ct^\prime$-derivative). Dropping the matrix notation and ``breaking up" the celerity components into Lorentz factors and velocity/acceleration components (remember: $\vvzeta = \dot{\vvbeta}$), that's:
\begin{equation*}
\begin{aligned}
\gamma^\prime \dot{\gamma}^\prime &= \gamma_{\textrm{rel}} \mkern1mu \gamma \left[ \dot{\gamma} - \beta_{\textrm{rel}} \bigl( \gamma \zeta_x + \dot{\gamma} \beta_x \bigr) \right] \\
\gamma^\prime \left( \gamma^{\prime} \zeta^{\prime}_x + \dot{\gamma}^\prime \beta_x^\prime \right) &= \gamma_{\textrm{rel}} \mkern1mu \gamma \left( \gamma \zeta_x + \dot{\gamma} \beta_x - \beta_{\textrm{rel}} \mkern1.mu \dot{\gamma} \right) \\
\gamma^\prime \left( \gamma^{\prime} \zeta^{\prime}_y + \dot{\gamma}^\prime \beta_y^\prime \right) &= \gamma \left( \gamma \zeta_y + \dot{\gamma} \beta_y \right) \\
\gamma^\prime \left( \gamma^{\prime} \zeta^{\prime}_z + \dot{\gamma}^\prime \beta_z^\prime \right) &= \gamma \left( \gamma \zeta_z +  \dot{\gamma} \beta_z \right)
\end{aligned}
\end{equation*}
and
\begin{equation*}
\begin{aligned}
\gamma \dot{\gamma} &= \gamma_{\textrm{rel}} \mkern1mu \gamma^\prime \left[ \dot{\gamma}^\prime + \beta_{\textrm{rel}} \bigl( \gamma^{\prime} \zeta^\prime_x +  \dot{\gamma}^\prime \beta^\prime_x \bigr) \right] \\
\gamma \left( \gamma \zeta_x + \dot{\gamma} \beta_x \right) &= \gamma_{\textrm{rel}} \mkern1mu \gamma^\prime \left( \gamma^{\prime} \zeta^\prime_x + \dot{\gamma}^\prime \beta^\prime_x + \beta_{\textrm{rel}} \mkern1.mu \dot{\gamma}^\prime \right) \\
\gamma \left( \gamma \zeta_y + \dot{\gamma} \beta_y \right) &= \gamma^\prime \left( \gamma^{\prime} \zeta^{\prime}_y + \dot{\gamma}^\prime \beta_y^\prime \right) \\
\gamma \left( \gamma \zeta_z +  \dot{\gamma} \beta_z \right) &= \gamma^\prime \left( \gamma^{\prime} \zeta^{\prime}_z + \dot{\gamma}^\prime \beta_z^\prime \right) .
\end{aligned}
\end{equation*}
Not pretty! Some torturous algebra (using Equations \ref{eq:fv} for substitutions as necessary) leads to the transformation of the Lorentz factor's time-derivative and of the three-acceleration, which we could also obtain by taking the $ct$- ($ct^\prime$-) derivatives of Equations \ref{eq:vt}:
\begin{equation}\label{eq:at}
\begin{aligned}
\dot{\gamma}^\prime &= \dfrac{\dot{\gamma} - \dot{\omega}_x \mkern1mu \beta_{\textrm{rel}} }{1 - \beta_x \mkern1mu \beta_{\textrm{rel}}} \\[3pt]
\zeta_x^\prime &= \dfrac{\zeta_x}{\gamma_{\textrm{rel}}^3 \left( 1 - \beta_x \mkern1mu \beta_{\textrm{rel}} \right)^3 } \\[3pt]
\zeta_y^\prime &= \dfrac{\zeta_y}{\gamma_{\textrm{rel}}^2 \left( 1 - \beta_x \mkern1mu \beta_{\textrm{rel}} \right)^2 } + \dfrac{\beta_y \mkern1mu \beta_{\textrm{rel}} \mkern1mu \zeta_x}{\gamma_{\textrm{rel}}^2 \left( 1 - \beta_x \mkern1mu \beta_{\textrm{rel}} \right)^3} \\[4pt]
\zeta_z^\prime &= \dfrac{\zeta_z}{\gamma_{\textrm{rel}}^2 \left( 1 - \beta_x \mkern1mu \beta_{\textrm{rel}} \right)^2 } + \dfrac{\beta_z \mkern1mu \beta_{\textrm{rel}} \mkern1mu \zeta_x}{\gamma_{\textrm{rel}}^2 \left( 1 - \beta_x \mkern1mu \beta_{\textrm{rel}} \right)^3} \\[2 em]
\dot{\gamma} &= \dfrac{\dot{\gamma}^\prime + \dot{\omega}^\prime_x \mkern1mu \beta_{\textrm{rel}} }{1 + \beta^\prime_x \mkern1mu \beta_{\textrm{rel}}} \\[3pt]
\zeta_x &= \dfrac{\zeta_x^\prime}{\gamma_{\textrm{rel}}^3 \left( 1 + \beta^{\prime}_x \mkern1mu \beta_{\textrm{rel}} \right)^3 } \\[3pt]
\zeta_y &= \dfrac{\zeta_y^\prime}{\gamma_{\textrm{rel}}^2 \left( 1 + \beta_x^\prime \mkern1mu \beta_{\textrm{rel}} \right)^2 } - \dfrac{\beta_y^\prime \mkern1mu \beta_{\textrm{rel}} \mkern1mu \zeta_x^\prime}{\gamma_{\textrm{rel}}^2 \left( 1 + \beta^{\prime}_x \mkern1mu \beta_{\textrm{rel}} \right)^3} \\[5pt]
\zeta_z &= \dfrac{\zeta_z^\prime}{\gamma_{\textrm{rel}}^2 \left( 1 + \beta_x^\prime \mkern1mu \beta_{\textrm{rel}} \right)^2 } - \dfrac{\beta_z^\prime \mkern1mu \beta_{\textrm{rel}} \mkern1mu \zeta_x^\prime}{\gamma_{\textrm{rel}}^2 \left( 1 + \beta^{\prime}_x \mkern1mu \beta_{\textrm{rel}} \right)^3} .
\end{aligned}
\end{equation}
We can use these ugly equations if we must, but transforming to the \emph{traveler's} instantaneous rest frame using the proper acceleration is clearly preferable. In fact, if an observer's (unprimed) rest frame is in standard configuration with the traveler's (primed) instantaneous rest frame (which is just a matter of choosing the right coordinate system), then $\beta_x^\prime = \beta_y^\prime = \beta_z^\prime = 0$, and Equations \ref{eq:at} give the Cartesian components of the traveler's proper acceleration:
\begin{equation*}
\begin{aligned}
\qquad \qquad \qquad & \zeta_{0 \mkern1.5mu x} = \zeta_x^\prime  = \gamma_{\textrm{rel}}^3 \mkern1mu \zeta_x = \gamma^3 \zeta_x \\[2pt]
& \zeta_{0 \mkern1.5mu y} = \zeta_y^\prime  = \gamma^2  \zeta_y  \qquad \qquad \qquad \textrm{\footnotesize{ (for standard configuration)}} \\[2pt]
& \zeta_{0 \mkern1.5mu z} = \zeta_z^\prime  = \gamma^2 \zeta_z .
\end{aligned}
\end{equation*}
This accords with our results from Section \ref{sssec:pa}: if $\vvbeta \parallel \vvzeta$, then $\vvzeta_{\mkern1mu 0} = \gamma^3 \vvzeta$; and if $\vvbeta \perp \vvzeta$, then $\vvzeta_{\mkern1mu 0} = \gamma^2 \vvzeta$. In general, an observer can split the proper acceleration into a sum of two component vectors, one parallel and the other perpendicular to the traveler's three-velocity:
\begin{equation}\label{eq:pa}
\vvzeta_{\mkern1mu 0} = \gamma^3 \vvzeta_{\parallel \vvbeta} + \gamma^2 \vvzeta_{\perp \vvbeta}.
\end{equation}

\subsubsection{Force Transformed}\label{sssec:ft}

That wasn't pleasant. Transforming the components of four-force won't be pleasant either, but it won't be quite as bad. First:
\begin{equation*}
[\vv F^\prime] = \Lambda \mkern1mu [\vv F] \qquad \qquad [\vv F] = \Lambda^{-1} \mkern1mu [\vv F^\prime] ,
\end{equation*}
which is (Equation \ref{eq:33}):
\begin{equation*}
\begin{bmatrix}
\gamma^\prime \mathcal{P}^\prime \\[.15 em]
\gamma^\prime f_x^\prime \\[.15 em]
\gamma^\prime f_y^\prime \\[.15 em]
\gamma^\prime f_z^\prime\\[.1 em]
\end{bmatrix}
=
\begin{bmatrix}
\gamma_{\textrm{rel}} & -\gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & 0 & 0 \\[.25 em]
- \gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & \gamma_{\textrm{rel}} & 0 & 0 \\[.25 em]
0 & 0 & 1 & 0 \\[.25 em]
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
\gamma \mathcal{P} \\[.15 em]
\gamma f_x \\[.15 em]
\gamma f_y \\[.15 em]
\gamma f_z \\[.1 em]
\end{bmatrix}
\end{equation*}
and
\begin{equation*}
\begin{bmatrix}
\gamma \mathcal{P} \\[.15 em]
\gamma f_x \\[.15 em]
\gamma f_y \\[.15 em]
\gamma f_z \\[.1 em]
\end{bmatrix}
=
\begin{bmatrix}
\gamma_{\textrm{rel}} & \gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & 0 & 0 \\[.25 em]
\gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & \gamma_{\textrm{rel}} & 0 & 0 \\[.25 em]
0 & 0 & 1 & 0 \\[.25 em]
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
\gamma^\prime \mathcal{P}^\prime \\[.15 em]
\gamma^\prime f_x^\prime \\[.15 em]
\gamma^\prime f_y^\prime \\[.15 em]
\gamma^\prime f_z^\prime\\[.1 em]
\end{bmatrix}
\end{equation*}
(remember that we've defined the relativistic power like $\mathcal{P} = \dot{E} = \dd E / \dd (ct)$, so that it has force-units). That's:
\begin{equation*}
\begin{aligned}
\gamma^\prime \mathcal{P}^\prime &= \gamma_{\textrm{rel}} \mkern1mu \gamma \left( \mathcal{P} - \beta_{\textrm{rel}} \mkern1mu f_x \right) \qquad & \gamma \mathcal{P} &= \gamma_{\textrm{rel}} \mkern1mu \gamma^\prime \left( \mathcal{P}^\prime + \beta_{\textrm{rel}} \mkern1mu f^\prime_x \right) \\[1pt]
\gamma^\prime f_x^\prime &= \gamma_{\textrm{rel}} \mkern1mu \gamma \left( f_x - \beta_{\textrm{rel}} \mkern1mu \mathcal{P} \right)  & \gamma f_x &= \gamma_{\textrm{rel}} \mkern1mu \gamma^\prime \left( f^\prime_x + \beta_{\textrm{rel}} \mkern1mu \mathcal{P}^\prime \right) \\[2pt]
\gamma^\prime f_y^\prime &= \gamma f_y & \gamma f_y &= \gamma^\prime f_y^\prime \\[2pt]
\gamma^\prime f_z^\prime &= \gamma f_z & \gamma f_z &= \gamma^\prime f_z^\prime.
\end{aligned}
\end{equation*}
After some algebra (again using Equations \ref{eq:fv} for substitutions as needed), we obtain the transformation of power and three-force:
\begin{equation}\label{eq:ft}
\begin{aligned}
\mathcal{P}^\prime &= \dfrac{\mathcal{P} - \beta_{\textrm{rel}} \mkern1mu f_x}{1 - \beta_{\textrm{rel}} \mkern1mu \beta_x} & \mathcal{P} &= \dfrac{\mathcal{P}^\prime + \beta_{\textrm{rel}} \mkern1mu f_x^\prime}{1 + \beta_{\textrm{rel}} \mkern1mu \beta_x^\prime} \\[6pt]
f_x^\prime &= \dfrac{f_x - \beta_{\textrm{rel}} \mkern1mu \mathcal{P} }{1 - \beta_{\textrm{rel}} \mkern1mu \beta_x} & f_x &= \dfrac{ f^\prime_x + \beta_{\textrm{rel}} \mkern1mu \mathcal{P}^\prime }{1 + \beta_{\textrm{rel}} \mkern1mu \beta^\prime_x} \\[5pt]
f_y^\prime &= \dfrac{f_y}{\gamma_{\textrm{rel}} \left( 1 - \beta_{\textrm{rel}} \mkern1mu \beta_x \right) } \qquad \quad & f_y &= \dfrac{f^\prime_y}{\gamma_{\textrm{rel}} \left( 1 + \beta_{\textrm{rel}} \mkern1mu \beta^\prime_x \right) } \\[5pt]
f_z^\prime &= \dfrac{f_z}{\gamma_{\textrm{rel}} \left( 1 - \beta_{\textrm{rel}} \mkern1mu \beta_x \right) } & f_z &= \dfrac{f^\prime_z}{\gamma_{\textrm{rel}} \left( 1 + \beta_{\textrm{rel}} \mkern1mu \beta^\prime_x \right) } .
\end{aligned}
\end{equation}
As with acceleration, we can use the power and force transformations if we must, but it's nicer to transform to the \emph{traveler's} instantaneous rest frame using proper power and proper force. And again, if an (unprimed) observer chooses a coordinate system that's in standard configuration with a (primed) traveler's instantaneous rest frame, then $\vvbeta^\prime = \vv 0$, $\mathcal{P}^\prime = \mathcal{P}_0$ (invariant proper power), and Equations \ref{eq:ft} give the Cartesian components of the proper force acting on the traveler:
\begin{equation}\label{eq:pf}
\begin{aligned}
\qquad \qquad \qquad & f_{0 \mkern1.5mu x} = f_x^\prime = f_x - \mathcal{P}_0 \mkern1mu \beta_x \\[2pt]
& f_{0 \mkern1.5mu y} = f_y^\prime = \gamma f_y \qquad \qquad \quad \textrm{\footnotesize{ (for standard configuration)}} \\[2pt]
& f_{0 \mkern1.5mu z} = f_z^\prime = \gamma f_z ,
\end{aligned}
\end{equation}
since now $\gamma_{\textrm{rel}} = \gamma$ and $\beta_{\textrm{rel}} = \beta = \beta_x$ (standard configuration means $\beta_x > 0$).

There are two notable special cases for Equations \ref{eq:ft} and \ref{eq:pf}. The first is when the four-force induces no acceleration but causes the traveler to lose or gain rest energy (as with the light-emitting body in Einstein's thought experiment). Then $\vv F = \mathcal{P}_0 \mkern.5mu \vv B$ (Equation \ref{eq:38}), $\mathcal{P} = \mathcal{P}^\prime = \mathcal{P}_0$ (Equation \ref{eq:po} with zero acceleration), and $\vv f^{[\prime]} = \mathcal{P}_0 \vvbeta^{[\prime]}$ (Equation \ref{eq:41} with zero acceleration), so that the three-force transformation is just the proper power times the three-velocity transformation (Equations \ref{eq:vt}):
\begin{equation*}
\begin{aligned}
f_x^{\prime} &= \mathcal{P}_0 \, \frac{\beta_x - \beta_{\textrm{rel}}}{1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}}} & f_x &= \mathcal{P}_0 \, \frac{\beta_x^{\prime} + \beta_{\textrm{rel}}}{1 + \beta_x^{\prime} \mkern1mu \beta_{\textrm{rel}}}\\[5pt]
f_y^{\prime} &= \mathcal{P}_0 \, \frac{\beta_y}{\gamma_{\textrm{rel}} \left( 1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}} \right)}& \qquad f_y &= \mathcal{P}_0 \, \frac{\beta_y^{\prime}}{\gamma_{\textrm{rel}} \left( 1 + \beta_x^{\prime} \mkern1mu \beta_{\textrm{rel}} \right) } \qquad \textrm{\footnotesize{ (for $\vv Z = \textrm{\mbox{\boldmath $\emptyset$}}$)}}\\[5pt]
f_z^{\prime} &= \mathcal{P}_0 \, \frac{\beta_z}{\gamma_{\textrm{rel}} \left( 1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}} \right)}& f_z &= \mathcal{P}_0 \, \frac{\beta_z^{\prime}}{\gamma_{\textrm{rel}} \left( 1 + \beta_x^{\prime} \mkern1mu \beta_{\textrm{rel}} \right)}.
\end{aligned}
\end{equation*}
If the primed observer \emph{is} the traveler (standard configuration), then $\vvbeta^\prime = \vv 0$, giving $f_y = f_z = 0$ and $f_x = \mathcal{P}_0 \mkern1mu \beta_{\mathrm{rel}} = \mathcal{P}_0 \mkern1mu \beta_x$, and Equations \ref{eq:pf} yield $\vv f_0 = \vv 0$, as they must (with no acceleration, the proper force is a zero vector).

The other special case---the usual scenario---is when the four-force induces acceleration without changing the traveler's rest energy. Then $\vv F = E_0 \mkern.5mu \vv Z$ (Equation \ref{eq:39}), $\vv f^{[\prime]} = E_0 \dot{\vvomega}^{[\prime]}$ (Equation \ref{eq:29}), and $\mathcal{P}^{[\prime]} = \vv f^{[\prime]} \cdot \vvbeta^{[\prime]}$ (Equation \ref{eq:fb}, or Equation \ref{eq:rp} with zero proper power). The three-force transformation from Equations \ref{eq:ft} is then:
\begin{equation*}
\begin{aligned}
f_x^\prime &= \dfrac{f_x - \beta_{\textrm{rel}} \mkern1mu ( \vv f \cdot \vvbeta ) }{1 - \beta_{\textrm{rel}} \mkern1mu \beta_x} & f_x &= \dfrac{ f^\prime_x + \beta_{\textrm{rel}} \mkern1mu ( \vv f^\prime \cdot \vvbeta^\prime ) }{1 + \beta_{\textrm{rel}} \mkern1mu \beta^\prime_x} \\[5pt]
f_y^\prime &= \dfrac{f_y}{\gamma_{\textrm{rel}} \left( 1 - \beta_{\textrm{rel}} \mkern1mu \beta_x \right) } \qquad \quad & f_y &= \dfrac{f^\prime_y}{\gamma_{\textrm{rel}} \left( 1 + \beta_{\textrm{rel}} \mkern1mu \beta^\prime_x \right) } \qquad \textrm{\footnotesize{ (for $\mathcal{P}_0 = 0$)}} \\[5pt]
f_z^\prime &= \dfrac{f_z}{\gamma_{\textrm{rel}} \left( 1 - \beta_{\textrm{rel}} \mkern1mu \beta_x \right) } & f_z &= \dfrac{f^\prime_z}{\gamma_{\textrm{rel}} \left( 1 + \beta_{\textrm{rel}} \mkern1mu \beta^\prime_x \right) } .
\end{aligned}
\end{equation*}
And if the primed observer \emph{is} the traveler (standard configuration), then Equations \ref{eq:pf} become:
\begin{equation*}
\begin{aligned}
\qquad \qquad \qquad & f_{0 \mkern1.5mu x} = f_x^\prime = f_x \\[2pt]
& f_{0 \mkern1.5mu y} = f_y^\prime = \gamma f_y \qquad \textrm{\footnotesize{ (for standard configuration and $\mathcal{P}_0 = 0$)}} \\[2pt]
& f_{0 \mkern1.5mu z} = f_z^\prime = \gamma f_z .
\end{aligned}
\end{equation*}
That first result means that the component of three-force whose corresponding component vector is parallel to a traveler's three-velocity is always equal to the matching component of the traveler's \emph{proper} force (provided that the traveler's rest energy isn't changing); i.e., the component is invariant under a Lorentz boost along the axis of the traveler's motion (though that axis may change as the force acts!). This is a generalization of our earlier finding that $\vv f = \vv f_0$ when $\vv f \parallel \vvbeta$ (see the text after Equation \ref{eq:46}).


\subsection{Hyperbolic Angles}\label{ssec:ra}

\subsubsection{What and Why?}

Back in Section \ref{sssec:fdi}, we made passing reference to a special case in which the Minkowski dot product \emph{can} be expressed in a way that's analogous to the Euclidean relation $\vv q \cdot \vv w = qw \cos \theta$. The expression is $\vv Q \cdot \vv W = QW \cosh \phi$, where $\phi$ is the hyperbolic angle between the qualifying four-vectors $\vv Q$ and $\vv W$, but we noted that this equation---and indeed the very notion of an angle between four-vectors---is conditional.

Conditional on what? What's this special case?\footnote{Beyond the special case we'll discuss, there are actually a couple of others in which one might reasonably associate a hyperbolic argument with a pair of four-vectors. But they're unimportant for our purposes, and the particular relation $\vv Q \cdot \vv W = QW \cosh \phi$ doesn't hold for them.} What do hyperbolas have to do with anything, and where does that equation come from?

The special case is when the vectors $\vv Q$ and $\vv W$ are timelike and have time components with the same sign---either both positive (the vectors are \textbf{future-pointing}) or both negative (\textbf{past-pointing}).\footnote{\label{fn:nt}An example of a past-pointing vector is the four-force acting on the light-emitting body in Einstein's thought experiment. In the body's rest frame, the time component $\mathcal{P}_0$ is negative because the body loses rest energy, and the four-force is timelike because $\mathcal{P}_0^2 > f_0^2 = 0$. By the way, non-zero lightlike vectors are classified as future- or past-pointing, too.} We'll see why in a moment. As for hyperbolas, consider:
\begin{equation}\label{eq:50}
1 = \cosh^2 {\phi} - \sinh^2 {\phi},
\end{equation}
the hyperbolic equivalent of the better-known Pythagorean trigonometric identity $1 = \cos^2 {\theta} + \sin^2 {\theta}$. (In case you're not familiar with the hyperbolic functions, the three ``main" ones and their inverses were given in Equation \ref{eq:hf}.) Equation \ref{eq:50} immediately calls to mind Equation \ref{eq:25}:
\begin{equation*}
1 = \gamma^2 - \omega^2 ,
\end{equation*}
the squared magnitude of the normalized four-velocity $\vv B$. This is a striking similarity! They're both equations for the unit hyperbola. We take this as motivation to introduce the hyperbolic angle $\phi$ into our relativistic toolbox (or rather, to reintroduce it, since we used it to derive the Lorentz transformation in Section \ref{sssec:lt}):
\begin{equation}\label{eq:51}
\boxed{
\begin{aligned}
\cosh{\phi} &= \gamma \\
\sinh{\phi} &= \omega = \gamma \beta \\
\tanh{\phi} &= \beta
\end{aligned}
} \, ,
\end{equation}
by the identity $\tanh{\phi} = \sinh{\phi} \, / \cosh{\phi}$.

Now we can derive $\vv Q \cdot \vv W = QW \cosh \phi$. Start with the four-velocities $\vv B_1$ and $\vv B_2$ of two travelers, and calculate their dot product---say, in the frame where $\vv B_1 = \langle 1, \vv 0 \rangle$ and $\vv B_2 = \langle \gamma, \vvomega \rangle = \langle \cosh{\phi}, \, \hatbeta \sinh{\phi} \rangle$ (by Equations \ref{eq:51}):
\begin{equation*}
\vv B_1 \cdot \vv B_2 = \gamma = \cosh{\phi}.
\end{equation*}
That's an invariant result: the dot product of the four-velocities of a pair of travelers is always their relative Lorentz factor, or $\cosh{\phi}$, where we interpret $\phi$ as the hyperbolic angle between the four-velocities.\footnote{\label{fn:ha}At the risk of stating the obvious: the \emph{relative} $\beta$, $\omega$, and $\gamma$ of two travelers---namely, the $\beta$, $\omega$, and $\gamma$ of one traveler as measured by the other---are trivially invariant. This doesn't mean that either traveler's $\beta$, $\omega$, and $\gamma$ are invariant (they obviously aren't), nor that \emph{differences} in the travelers' velocities and celerities are invariant (again, we know they are not, even under boosts along the axis of the travelers' relative motion). The quantity $\phi$, however, is the hyperbolic \emph{angle} between the travelers' four-velocities, and if it's to live up to its name it should possess some of the nice \emph{properties} of angles. We should expect (and will soon verify), for example, that under appropriate circumstances, differences in hyperbolic angles \emph{are} invariant. Specifically, the (trivially invariant) hyperbolic angle between two four-velocities should always equal the difference of the hyperbolic angles they make with a third \emph{coplanar} four-velocity, though we'll need to allow for \emph{signed} hyperbolic angles to make this work.} Next, recall that normalized four-velocity is a dimensionless timelike unit vector (with a positive time component). Any other timelike vector can therefore be expressed as the product (or negative product, if it's past-pointing) of its own magnitude and a parallel four-velocity. So if $\vv Q = \pm \, Q \vv B_1$ and $\vv W = \pm \, W \vv B_2$, with $Q$ and $W$ both positive or both negative:
\begin{equation}\label{eq:52}
\begin{split}
\vv Q \cdot \vv W &= \pm \, Q \vv B_1 \cdot \pm \, W \vv B_2 \\
&= QW (\vv B_1 \cdot \vv B_2) \\
\vv Q \cdot \vv W &= QW \cosh{\phi}.
\end{split}
\end{equation}
For any timelike $\vv Q$ and $\vv W$ that are both future-pointing or both past-pointing, then, Equation \ref{eq:52} holds, and $\phi$ can be interpreted as the hyperbolic angle between them. Further, $\cosh{\phi}$ is the relative Lorentz factor of a pair of travelers whose four-velocities are parallel to $\vv Q$ and $\vv W$.

\subsubsection{Rapidity}

Of course, the four-velocities that are parallel to $\vv Q$ and $\vv W$ in Equation \ref{eq:52} need not correspond with \emph{actual} travelers in a given scenario. For instance, the four-force acting on the light-emitting body in Einstein's thought experiment isn't parallel to the four-velocity \emph{of} anything that we care about (see Footnote \ref{fn:nt}); rather, the ``four-velocity" there is just a unit vector with no physical significance. If that four-force were dotted with another past-pointing timelike vector, we could identify the hyperbolic angle $\phi$ between them, but we'd probably have little interest in the associated relative $\gamma = \cosh{\phi}$ or the rest of Equations \ref{eq:51}, because, well, $\gamma$ and $\beta$ of \emph{what}?

But sometimes the travelers are real, as they'd be if $\vv Q$ and $\vv W$ were their four-displacements or four-momenta (or indeed their four-velocities!). When they are, Equations \ref{eq:51} are physically meaningful, and we call the hyperbolic angle between the travelers' four-velocities their relative \textbf{rapidity}. As an observer, each traveler can speak of the other's rapidity $\phi = \tanh^{-1}{\beta}$, magnitude of the three-vector $\vvphi \equiv \phi \hatphi = \phi \hatbeta$.

\emph{Any expression with a (subluminal) speed,\footnote{Like celerity and the Lorentz factor, the rapidity is undefined for $\beta = 1$.} celerity, or Lorentz factor can be rewritten in terms of the rapidity.} Why might we want to do so?

For starters, rapidity is simply an elegant concept. Consider that we can write the Lorentz boost matrix and its inverse (Equations \ref{eq:bm} and \ref{eq:im}) like this:
\begin{equation*}
\Lambda =
\begin{bmatrix}
\cosh{\phi} & -\sinh{\phi} & 0 & 0 \\
- \sinh{\phi} & \cosh{\phi} & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\qquad
\Lambda^{-1} =
\begin{bmatrix}
\cosh{\phi} & \sinh{\phi} & 0 & 0 \\
\sinh{\phi} & \cosh{\phi} & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix} .
\end{equation*}
This allows us to understand a Lorentz boost as a hyperbolic rotation of the $ct^{[\prime]}$- and $x^{[\prime]}$-axes through the observers' relative rapidity (the hyperbolic angle between their four-velocities). Isn't that something? It's a very close analogue of a spatial rotation. For example, here are the transformation matrices we'd use for a rotation of the $x^{[\prime]}$- and $y^{[\prime]}$-axes through a Euclidean angle $\theta$:
\begin{equation*}
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & \cos{\theta} & \sin{\theta} & 0 \\
0 & - \sin{\theta} & \cos{\theta} & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\qquad \qquad
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & \cos{\theta} & -\sin{\theta} & 0 \\
0 & \sin{\theta} & \cos{\theta} & 0 \\
0 & 0 & 0 & 1
\end{bmatrix} .
\end{equation*}

As a palpably geometric quantity---the hyperbolic angle between a traveler's and observer's four-velocities---the rapidity is perhaps more ``fundamental" than speed or celerity or the Lorentz factor. There's something satisfying about recasting relativistic mechanics in terms of four-vectors, invariants, and rapidities alone. Four-velocity (normalized):
\begin{equation*}
\vv B = \langle \cosh{\phi}, \, \hatphi \sinh{\phi} \rangle
\end{equation*}
($\hatphi = \hatbeta$); infinitesimal four-displacement:
\begin{equation*}
\dd \vv R = \vv B \, c \, \dd t_0 = c \, \dd t_0 \, \langle \cosh{\phi}, \, \hatphi \sinh{\phi} \rangle ;
\end{equation*}
four-momentum (for $E_0 \neq 0$):
\begin{equation*}
\vv P = E_0 \vv B = E_0 \, \langle \cosh{\phi}, \, \hatphi \sinh{\phi} \rangle ;
\end{equation*}
four-acceleration (``normalized"):\footnote{\label{fn:ph}By $\mathring{\phi}$ we mean the $ct_0$-derivative of the rapidity $\phi$, \emph{not} the magnitude of the vector $\mathring{\vvphi}$. They're equal only in the case of rectilinear motion (or when there's no acceleration):
\begin{equation*}
\left \Vert \mathring{\vvphi} \right \Vert = \left \Vert \dfrac{\dd}{\dd (c t_0)} \, ( \phi \hatphi ) \right \Vert = \left \Vert \mathring{\phi} \hatphi + \phi \mathring{\hatphi} \right \Vert,
\end{equation*}
which is just $\mathring{\phi}$ for rectilinear motion ($\mathring{\vvphi} \parallel \vvphi$), because then $\mathring{\hatphi} = \vv 0$.}
\begin{equation*}
\vv Z = \mathring{\vv B} = \langle  \mathring{\phi} \sinh{\phi} , \, \hatphi \mkern1.5mu \mathring{\phi} \cosh{\phi} + \mathring{\hatphi} \sinh{\phi} \rangle
\end{equation*}
(because the hyperbolic sine and hyperbolic cosine functions are derivatives of each other); and four-force:
\begin{equation*}
\vv F = \mathring{\vv P} = \mathcal{P}_0 \vv B + E_0 \vv Z
\end{equation*}
($\mathcal{P}_0 = \mathring{E}_0$ being the proper power in force-units), or
\begin{equation*}
\vv F = \langle \mathcal{P}_0 \cosh{\phi} + E_0 \mkern1.5mu \mathring{\phi} \sinh{\phi} , \, \hatphi ( \mathcal{P}_0 \sinh{\phi} + E_0 \mkern1.5mu \mathring{\phi} \cosh{\phi} ) + \mathring{\hatphi} E_0 \sinh{\phi} \rangle .
\end{equation*}
Admittedly, the components of four-acceleration and four-force are somewhat unwieldy when written out like this, but they're arguably even worse in terms of $\vvbeta$, $\vvomega$, and $\gamma$, and the user-friendly nature of hyperbolic functions is a plus. Just look how easy it is to obtain an expression for the squared proper acceleration using the rapidity (and Equation \ref{eq:pa4}):
\begin{equation}\label{eq:pa3}
\begin{split}
- \, Z^2 &= - \, \vv Z \cdot \vv Z = \left( \hatphi \mkern1.5mu \mathring{\phi} \cosh{\phi} + \mathring{\hatphi} \sinh{\phi} \right)^2 - \left( \mathring{\phi} \sinh{\phi} \right)^2 \\
\zeta_0^2 &= \mathring{\phi}^2 + (\mathring{\hatphi} \sinh{\phi})^2 
\end{split}
\end{equation}
because $\hatphi \cdot \mathring{\hatphi} = 0$ (unit vector is orthogonal to its time-derivative), $\hatphi \cdot \hatphi = 1$, and $\cosh^{2}{\phi} - \sinh^{2}{\phi} = 1$. It requires some algebra to show that the right side of Equation \ref{eq:pa3} is equivalent to the negative square of the right sides of Equations \ref{eq:pa2} and \ref{eq:37}, but it is, and it was child's play to derive. So in addition to its conceptual elegance, the rapidity offers much \emph{mathematical} elegance.

Equation \ref{eq:pa3} also gives us the appealing relation $\zeta_0 = | \mathring{\phi} |$ for $\mathring{\vvphi} \parallel \vvphi$.\footnote{In this case, $\mathring{\phi} = \Vert \mathring{\vvphi} \Vert$ (see Footnote \ref{fn:ph}), and $\vvzeta_{\mkern1mu 0} = \mathring{\vvphi}$.} When the traveler's motion is rectilinear, then, $| \mathring{\phi} |$ is the magnitude of the proper acceleration and therefore invariant under a Lorentz boost in the $\pm \, \hatphi$ direction. Same goes for $\dd \phi = \mathring{\phi} \, c \, \dd t_0$: incremental \emph{changes} in the traveler's rapidity are invariant among observers for whom the traveler's $\mathring{\vvphi} \parallel \vvphi$. Now we're getting somewhere! Note that because we've defined rapidity as a magnitude, we cannot extend this argument to say that \emph{differences} in the rapidities of two rectilinear travelers with parallel velocities are necessarily invariant among the would-be qualifying observers. If one traveler is moving to your left at the same speed that the other is moving to your right, then you'll say the difference in their rapidities is zero, but you'd give a different answer after a boost to your left or right (imagine boosting all the way to one of the travelers' rest frames---is the other traveler now at rest? no). Partly to remedy this ``shortcoming," we'll introduce a \emph{signed} rapidity-like parameter in the next section. Naively we might guess that this signed parameter will be the component of $\vvphi$ corresponding to the boost axis ($\phi_x$ if the boost is along the $x$-axis), but we'll see that that's not generally so.


\subsubsection{Longitudinal Rapidity}

Since the inverse hyperbolic tangent is an odd function with a domain of $(-1, 1)$, there's nothing stopping us from feeding it signed velocity \emph{components} and working with the identically signed rapidity-like quantities it outputs. The hyperbolic tangent function is likewise odd and also returns values that retain the sign of their inputs, so the process is ``reversible." Let's experiment with this. Say we've got two inertial observers whose rest frames are in standard configuration (boost along the $x^{[\prime]}$-axis), and say they're measuring the velocity $\vvbeta$ ($\vvbeta^\prime$) of some third party. By Equations \ref{eq:vt}, the velocity component corresponding to the boost axis transforms like this:
\begin{equation*}
\beta_x^{\prime} = \frac{\beta_x - \beta_{\textrm{rel}}}{1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}}} \qquad \qquad \beta_x = \frac{\beta_x^{\prime} + \beta_{\textrm{rel}}}{1 + \beta_x^{\prime} \mkern1mu \beta_{\textrm{rel}}}.
\end{equation*}
Now let's use Equations \ref{eq:51} to rewrite the relative speed of the frames $\beta_{\textrm{rel}}$ in terms of its rapidity equivalent $\phi_{\textrm{rel}}$, and let's also rewrite each velocity \emph{component} as the hyperbolic tangent of a rapidity-like parameter that we'll label $\varphi^{[\prime]}$ ($\varphi$ is the ``curly" version of the Greek letter phi [$\phi$]):
\begin{equation}\label{eq:th}
\tanh{\varphi^{\prime}} = \frac{\tanh{\varphi} - \tanh{\phi_{\textrm{rel}}}}{1 - \tanh{\varphi} \,  \tanh{\phi_{\textrm{rel}}}} \qquad \quad \tanh{\varphi} = \frac{\tanh{\varphi^{\prime}} + \tanh{\phi_{\textrm{rel}}}}{1 + \tanh{\varphi^{\prime}} \, \tanh{\phi_{\textrm{rel}}}} .
\end{equation}
The pertinent characteristic of $\varphi$ is that it's the inverse hyperbolic tangent of the velocity component \emph{corresponding to the axis of the given Lorentz boost} (the $x$-axis here). This new quantity is the \textbf{longitudinal rapidity}. Particle physicists usually label it $y$ and sometimes call it the ``particle rapidity" or, much to the confusion of outsiders, just the ``rapidity."\footnote{They also prefer to express it in terms of energy and momentum rather than velocity: $\tanh^{-1}{\beta_x} = \tanh^{-1}{(cp_x / E)} = \frac{1}{2} \ln{[(E + cp_x) / (E - cp_x) ]}$ (by Equations \ref{eq:19} and \ref{eq:hf}). We choose not to label it $y$ because that symbol is already doing Cartesian duties for us. Anyway, $\varphi$ kind of looks like $y$, and as a variant of the letter $\phi$ it seems fitting.}

So what does the longitudinal rapidity do for us? Quite a bit, if we happen to know the addition/subtraction rule for the hyperbolic tangent function:
\begin{equation*}
\tanh{(a \pm b)} = \dfrac{\tanh{a} \pm \tanh{b}}{1 \pm \tanh{a} \, \tanh{b}}.
\end{equation*}
That turns Equations \ref{eq:th} into:
\begin{equation*}
\tanh{\varphi^{\prime}} = \tanh{(\varphi - \phi_{\textrm{rel}})} \qquad \qquad \tanh{\varphi} = \tanh{(\varphi^\prime + \phi_{\textrm{rel}})},
\end{equation*}
meaning:
\begin{equation}\label{eq:lr}
\varphi^\prime = \varphi - \phi_{\textrm{rel}} \qquad \qquad \varphi = \varphi^\prime + \phi_{\textrm{rel}}.
\end{equation}
Under a boost along the relevant axis, the longitudinal rapidity transforms additively! Geometrically, this is to be expected: $\phi_{\textrm{rel}}$, $\varphi$, and $\varphi^\prime$ are all hyperbolic angles in the $x/ct$-plane ($\varphi$ and $\varphi^{\prime}$ being \emph{signed} angles), and coplanar angles \emph{should} be additive, as we mentioned in Footnote \ref{fn:ha}.

Another handy feature of longitudinal rapidity follows from the graceful way it transforms. Say that our primed and unprimed observers are measuring \emph{two} velocities instead of one (we'll use the subscripts 1 and 2). Maybe the velocities are those of a single traveler at two different proper-times, or maybe they're those of two different constant-velocity travelers. Either way, by Equations \ref{eq:lr}:
\begin{equation*}
\begin{aligned}
\varphi_{2}^\prime - \varphi_{1}^\prime &= \left( \varphi_{2} - \phi_{\textrm{rel}} \right) - \left( \varphi_{1} - \phi_{\textrm{rel}} \right) \\[4pt]
&= \varphi_{2} - \varphi_{1}.
\end{aligned}
\end{equation*}
In other words, \emph{changes} and \emph{differences} in longitudinal rapidity are invariant (under a boost along the axis in question). We know that this isn't true of \emph{velocity}: if one traveler recedes to your left at half the speed of light while another recedes to your right at the same speed, then you'll say their (normalized) ``separation speed" is ${1/2 - (-1/2) = 1}$, but neither traveler says the other recedes at the speed of light. Rather, by Equations \ref{eq:vt}, each says the other recedes at $\beta = 4/5$. Yet all three of you would agree on the travelers' ``separation rapidity": $\tanh^{-1}(1/2) - \tanh^{-1}(-1/2) = \tanh^{-1}(4/5)$.

Bear in mind that, in general, $\varphi \neq \phi_x$ ($x$ being the boost-axis). If $\theta$ is the angle between $\vvphi$ and the positive $x$-direction, then longitudinal rapidity is $\varphi = \tanh^{-1} \beta_x = \tanh^{-1}{(\beta \cos{\theta})}$, whereas the Cartesian component $\phi_x$ of $\vvphi$ is $\phi \cos{\theta} = (\tanh^{-1}{\beta}) (\cos{\theta})$. The latter isn't really a useful quantity, except in the special case that it's the \emph{only} non-zero component of $\vvphi$ and hence \emph{equal to} the former---i.e., if $\theta$ is $0$ or $\pi$, then $\cos{\theta} = \pm \, 1$, and consequently $\tanh^{-1} \beta_x = \phi_x$ (because inverse hyperbolic tangent is an odd function). Along the same lines, note that we cannot generally ``apply" Equations \ref{eq:51} to $\phi_x$ (or $\varphi$) to find $\gamma$ or the celerity component $\omega_x = \gamma \beta_x = \cosh{\phi} \tanh{\varphi}$. Again, the exception is when the velocity is directed along the $x$-axis, because then $[\varphi = \, ] \tanh^{-1} \beta_x = \phi_x = \pm \, \phi$ (and hyperbolic cosine is an even function):
\begin{equation}\label{eq:rx}
\begin{aligned}
\cosh{\phi _x} &= \gamma \\
\sinh{\phi_x} &= \omega_x \qquad \textrm{\footnotesize{ (for $\vvbeta = \langle \beta_x, 0, 0 \rangle $)}} .\\
\tanh{\phi_x} &= \beta_x
\end{aligned}
\end{equation}

Here's a neat application. Twice now---first after deriving Equation \ref{eq:46}, and then at the end of Section \ref{sssec:ft}---we've shown that among observers for whom a traveler's three-velocity and three-force are parallel, the magnitude of the three-force is equal to the magnitude of the proper force and thus invariant under a Lorentz boost in the $\pm \, \hatbeta$ direction (provided that the traveler's rest energy remains constant). More generally, we found that even if the three-force isn't parallel to the three-velocity, the \emph{component} of three-force whose corresponding component vector is (momentarily) parallel to the three-velocity is invariant under such a boost. Another demonstration of this conditional invariance, more attractive perhaps, uses longitudinal rapidity and begins with Equation \ref{eq:29}:
\begin{equation*}
\begin{split}
\vv f &= \dot{\vv p} c = E_0 \dot{\vvomega} =  \dfrac{E_0}{\gamma} \, \mathring{\vvomega} \\[3pt]
f_x & = \dfrac{E_0}{\gamma} \, \mathring{\omega}_x = \dfrac{E_0}{\cosh{\phi}} \, \mathring{\omega}_x .
\end{split}
\end{equation*}
The $x$-axis will be our boost axis, so longitudinal rapidity is $\varphi = \tanh^{-1}{\beta_x}$, and $\omega_x = \gamma \beta_x = \cosh{\phi} \mkern1mu \tanh{\varphi}$. Now impose the condition that the three-velocity is in the positive or negative $x$-direction. This gives $\varphi = \phi_x = \pm \, \phi$ and puts Equations \ref{eq:rx} into play, such that $\omega_x = \cosh{\varphi} \mkern1mu \tanh{\varphi} = \sinh{\varphi}$:
\begin{equation*}
\begin{split}
f_x &= \dfrac{E_0}{\cosh{\varphi}} \, \dfrac{\dd}{\dd (c t_0)} \left( \sinh{\varphi} \right)  \\[3pt]
f_x &= E_0 \mkern1.5mu \mathring{\varphi} .
\end{split}
\end{equation*}
Since $E_0$ and $c \, \dd t_0$ are Lorentz-invariant, and since $\dd \varphi$ (infinitesimal change in longitudinal rapidity) is invariant under a boost along the $x$-axis, the component $f_x = E_0 \, \dd \varphi / \dd (c t_0)$ must also be invariant under a boost along the $x$-axis---that is, under a boost in the $\pm \, \hatbeta$ direction.


\subsubsection{Pseudorapidity and the Ultrarelativistic Limit}

Particle physicists are keen on longitudinal rapidity (they often call it ``particle rapidity" or even just ``rapidity," and they label it $y$). That's because they deal with particles emitted from collisions in accelerators, and there's need to analyze this debris in the center-of-momentum frame of the colliding-particle system, which generally isn't the same as the lab frame. This puts a premium on quantities that are invariant under a Lorentz boost along the axis of the original beam (the $z$-axis, customarily), and differences in the corresponding longitudinal rapidities of ejecta fit the bill.

Unfortunately, longitudinal rapidities aren't always easy to measure directly in particle accelerators. But in the \textbf{ultrarelativistic limit} ($\beta \rightarrow 1$), there's another quantity called \textbf{pseudorapidity} that's an excellent approximation to longitudinal rapidity, and it's much easier to measure: all you need is the emitted particle's so-called polar angle $\theta$, defined as the angle between the beam axis (positive $z$-direction) and the particle's velocity vector.

Before getting into the pseudorapidity, let's take a moment to mull over the ultrarelativistic limit. Above, we characterized it as $\beta \rightarrow 1$, rather than $\beta \approx 1$. The reason is that we might care how close to the speed of light something is moving (i.e., $1 - \beta$).\footnote{Why $1 - \beta$ instead of just $\beta$? Because in the ultrarelativistic limit, $\beta$ will have too many leading nines for practical use (round 0.999999999 to three significant digits---that's $1.00$, and the important information is lost!). Much better to deal with leading zeros.} If not, $\beta \approx 1$ may suffice, but if so, it plainly won't. This is similar to a situation we dealt with back in Section \ref{ssec:re}, when we couldn't use $\gamma \approx 1$ to derive Equation \ref{eq:14} (would have given us a value of zero for kinetic energy in the classical limit).\footnote{If one already has Equation \ref{eq:21} in hand (we didn't), then one \emph{can} derive Equation \ref{eq:14} using $\gamma \approx 1$ (and Equations \ref{eq:12} and \ref{eq:17}):
\begin{equation*}
\begin{split}
(pc)^2 &= E^2 - E_0^2 = (E + E_0) (E - E_0) = (E + E_0) E_{\mkern.5mu \textrm{k}} \\[2pt]
E_{\mkern.5mu \textrm{k}} &= \dfrac{(pc)^2}{E + E_0} = \dfrac{(\gamma E_0 \beta)^2}{\gamma E_0 + E_0} = E_0 \beta^2 \, \dfrac{\gamma^2}{\gamma + 1} \\[2pt]
&\approx \dfrac{1}{2} \, E_0 \beta^2 .
\end{split}
\end{equation*}
} Our solution here is similar, too: we'll use the binomial theorem to get extra terms as needed.

Start with the energy--momentum relation (Equation \ref{eq:21}), and isolate $E$:
\begin{equation*}
E = \sqrt{(p c)^2 + E_0^2} .
\end{equation*}
Now, since $p c \gg E_0$ as $\beta \rightarrow 1$, let's factor out $p c$. That way our binomial expansion will involve increasing powers of $E_0 / p c$, which is small:\footnote{If we wanted to approximate $E$ in the classical limit, we'd factor out $E_0$ instead.}
\begin{equation*}
E = p c \left[ 1 + \left( \dfrac{E_0}{p c} \right)^2 \right]^{1/2}.
\end{equation*}
Recall the binomial series:
\begin{equation*}
(1+x)^n = 1 + nx + \frac{n(n-1)}{2!} \, x^2 + \frac{n(n-1)(n-2)}{3!} \, x^3 + \dots ,
\end{equation*}
convergent if $|x| < 1$. We've got $x = (E_0 / p c)^2 = \omega^{-2}$ (Equation \ref{eq:29}), and some algebra shows that $|\omega^{-2}|<1$ when $\beta > \sqrt{2}/2 \approx .707$, in which case:
\begin{equation*}
E = p c \left[ 1 + \dfrac{1}{2} \left( \dfrac{E_0}{p c} \right)^2 - \dfrac{1}{8} \left( \dfrac{E_0}{p c} \right)^4 + \dots \right] .
\end{equation*}
As planned, $1 \gg (E_0 / p c)^2 \gg (E_0 / p c)^4$, etc. For many purposes we can stop at the first term and use $E \approx p c$, but here we'll want the second one, too:
\begin{equation}\label{eq:ue}
E \approx p c \left[ 1 + \dfrac{1}{2} \left( \dfrac{E_0}{p c} \right)^2 \right]  \qquad \textrm{\footnotesize{ (for $\beta \rightarrow 1$)}}.
\end{equation}
Then if we want to know $1 - \beta$, we can use Equations \ref{eq:19} and \ref{eq:ue}:
\begin{equation}\label{eq:ub}
\begin{split}
1 - \beta &= 1 - \dfrac{p c}{E} \\[5pt]
1 - \beta & \approx 1 - \left[ 1 + \dfrac{1}{2} \left( \dfrac{E_0}{p c} \right)^2 \right]^{-1} \qquad \textrm{\footnotesize{ (for $\beta \rightarrow 1$)}}.
\end{split}
\end{equation}
We can make that easier on the eyes. One option is another binomial expansion, which yields the \emph{slightly} worse but still-excellent approximation $1 - \beta \approx .5 (E_0 / p c)^2$. By Equations \ref{eq:29} and \ref{eq:51}, $E_0 / p c = \omega^{-1} = 1 / \sinh{\phi}$, so that this slightly worse approximation is equivalently $.5 \, \text{csch}^2 \, \phi$ (where $\text{csch} \, \phi = 1 / \sinh{\phi}$ is the hyperbolic cosecant function). But we can also plug the rapidity into Equation \ref{eq:ub} directly, rendering it elegant without sacrificing accuracy at all. First:
\begin{equation*}
1 - \beta \approx 1 - \left[ 1 + \dfrac{1}{2 \sinh^2{\phi}} \right]^{-1} .
\end{equation*}
Now we use the ``double-angle" hyperbolic identity $\cosh{(2 \phi)} = 2 \sinh^2{\phi} + 1$:
\begin{equation*}
\begin{split}
1 - \beta & \approx 1 - \left[ 1 + \dfrac{1}{\cosh{(2 \phi)} - 1} \right]^{-1} \\[5pt]
& = 1 - \left[ \dfrac{\cosh{(2 \phi)}}{\cosh{(2 \phi)} - 1} \right]^{-1} \\[7pt]
&= \dfrac{1}{\cosh{(2 \phi)}} ,
\end{split}
\end{equation*}
and if we introduce the hyperbolic secant ($\text{sech} \, \phi = 1 / \cosh{\phi}$):
\begin{equation}\label{eq:ur}
1 - \beta \approx \text{sech} \, (2 \phi) \qquad \textrm{\footnotesize{ (for $\beta \rightarrow 1$)}}.
\end{equation}
Not bad! A better-known approximation (virtually identical to Equation \ref{eq:ur} in the ultrarelativistic limit) comes from Equation \ref{eq:20}:
\begin{equation*}
\begin{split}
\gamma^{-2} &= 1 - \bigl( 1 - \left( 1 - \beta \right) \bigr)^2 \\
& = 1 - \left( 1 - 2 \left( 1 - \beta \right) + \left( 1 - \beta \right)^2 \right),
\end{split}
\end{equation*}
where we set $(1 - \beta)^2 \approx 0$, leaving:
\begin{equation}\label{eq:ug}
\begin{split}
\gamma^{-2} & \approx 2 \left( 1 - \beta \right) \\[5pt]
1 - \beta & \approx \dfrac{1}{2 \gamma^2} \left [= \dfrac{1}{2} \, \text{sech}^2 \, \phi \right] \qquad \textrm{\footnotesize{ (for $\beta \rightarrow 1$)}}.
\end{split}
\end{equation}
Equation \ref{eq:ug} is convenient if you know $\gamma$. Equation \ref{eq:ub} ($=$ Equation \ref{eq:ur}) is good if you know $E_0$ and $p c$. If you know the rapidity, take your pick.

That was a productive digression, but we don't need any of it for the pseudorapidity. The pseudorapidity is what the longitudinal rapidity $\varphi$ becomes---er, what the particle physicist's ``rapidity" $y$ becomes---when we set $\beta \approx 1$:
\begin{equation*}
\begin{aligned}
y \equiv \varphi &= \tanh^{-1} \beta_z \\
&= \tanh^{-1} ( \beta \cos{\theta} ) \\
& \approx  \tanh^{-1} ( \cos{\theta} ) \equiv \eta ,
\end{aligned}
\end{equation*}
where $\theta$ is the ``polar" angle between $\vvbeta$ (velocity of emitted particle) and the particle beam (positive $z$-direction). This $\eta$ is the pseudorapidity. It's usually expressed as a natural logarithm (Equations \ref{eq:hf}):
\begin{equation*}
\begin{split}
\eta &= \dfrac{1}{2} \mkern1mu \ln{ \left( \dfrac{1 + \cos{\theta}}{1 - \cos{\theta}} \right) } \\[3pt]
&= \ln{ \left( \cot {\dfrac{\theta}{2}} \right) },
\end{split}
\end{equation*}
where we've used the ``half-angle" trig identities $\cos^2{(\theta / 2)} = (1 + \cos{\theta})/2$ and $\sin^2{(\theta / 2)} = (1 - \cos{\theta})/2$.

Because $\eta$ is such a good approximation for $y$ in the ultrarelativistic limit, particle physicists sometimes loosely refer to the pseudorapidity as just \emph{rapidity}. All this inconsistent usage can be confusing, so let's recap. In our lingo, rapidity is $\phi = \Vert \bm{\upphi} \Vert = \tanh^{-1}{\beta}$, and the longitudinal rapidity $\varphi$ is the inverse hyperbolic tangent of the velocity component corresponding to the axis of a given Lorentz boost ($\tanh^{-1}{\beta_x}$ if $x$ is the boost axis). In particle physics, longitudinal rapidity gets the symbol $y$ and is often called the ``particle rapidity" or even just the ``rapidity" for short, although the pseudorapidity $\eta$ is also sometimes called the rapidity.


\subsection[Advanced Rectilinear Kinematics: Constant Proper Acceleration]{Advanced Rectilinear Kinematics:\\ Constant Proper Acceleration}\label{ssec:ak}

With an important caveat, the Newtonian kinematic equations for constant acceleration are still applicable in special relativity, provided that only \emph{coordinate} quantities appear in them. This is merely a matter of defining velocity and acceleration as the first and second time-derivatives of position. In our notation, for example, and using the subscript i for \emph{initial} values ($ct = 0$):\footnote{It's common to use the naught subscript for initial values, but we're already using that for \emph{proper} quantities.}
\begin{equation*}
\begin{aligned}
\int_0^{ct} \dfrac{\dd \vvbeta}{\dd (ct)} \, c \, \dd t &= \int_0^{ct} \vvzeta \, c \, \dd t \\[5pt]
\vvbeta -  \vvbeta_{\textrm{i}} &= \vvzeta \mkern2mu ct,
\end{aligned}
\end{equation*}
and then also:
\begin{equation*}
\begin{aligned}
\int_0^{ct} \dfrac{\dd \vv r}{\dd (ct)} \, c \, \dd t &= \int_0^{ct} \vvbeta \, c \, \dd t \\[5pt]
\vv r - \vv r_{\textrm{i}} &= \int_0^{ct} \left( \vvbeta_{\textrm{i}} + \vvzeta \, ct \right) c \, \dd t \\[3pt]
\vv r &= \vv r_{\textrm{i}} + \vvbeta_{\textrm{i}} \mkern2mu ct + \dfrac{1}{2} \mkern2mu \vvzeta \, ( ct )^2 \\[4pt]
& \left[ = \vv r_{\textrm{i}} + \vv v_{\textrm{i}} \mkern2mu t + \dfrac{1}{2} \mkern2mu \vv a t^2 \right] ,
\end{aligned}
\end{equation*}
which is a parabola (\textbf{parabolic motion} through spacetime). The important caveat is that constant coordinate acceleration cannot be maintained forever, since our universe has a speed limit.

But these equations aren't very \emph{interesting} in special relativity, for the simple reason that coordinate acceleration can only be constant in one inertial frame. Much more interesting is the case of constant \emph{proper} acceleration $\vvzeta_{\mkern1mu 0}$, which can be maintained indefinitely (fuel considerations aside) and whose magnitude is invariant. In general, Equation \ref{eq:pa} gives:
\begin{equation*}
\int_0^{ct} \left( \gamma^3 \vvzeta_{\parallel \vvbeta} + \gamma^2 \vvzeta_{\perp \vvbeta} \right) c \, \dd t = \int_0^{ct} \vvzeta_{\mkern1mu 0} \, c \, \dd t ,
\end{equation*}
or in terms of the traveler's proper time:
\begin{equation*}
\int_0^{ct_0} \left( \gamma^3 \vvzeta_{\parallel \vvbeta} + \gamma^2 \vvzeta_{\perp \vvbeta} \right) c \, \dd t_0 = \int_0^{ct_0} \vvzeta_{\mkern1mu 0} \, c \, \dd t_0 .
\end{equation*}
The time-dependence of the angle between $\vvzeta$ and $\vvbeta$ complicates things, but we'll concern ourselves only with rectilinear motion ($\vvzeta \parallel \vvbeta$), where that's not a problem. First we'll derive kinematic functions of coordinate time, then we'll derive kinematic functions in terms of proper time, and finally we'll discuss the significance of this rectilinear constant-$\vvzeta_{\mkern1mu 0}$ scenario.


\subsubsection{Functions of Coordinate Time (Team Celerity)}

With $\vvzeta \parallel \vvbeta$, we have:
\begin{equation*}
\int_0^{ct} \gamma^3 \vvzeta \, c \, \dd t = \int_0^{ct} \vvzeta_{\mkern1mu 0} \, c \, \dd t .
\end{equation*}
A gander at Equations \ref{eq:34} and \ref{eq:fa} shows that in this rectilinear case, the quantity $\gamma^3 \vvzeta$ is the $ct$-derivative of the ``normalized" celerity ($\dot{\vvomega} = \gamma \dot{\vvbeta} + \dot{\gamma} \vvbeta$), so for constant proper acceleration:
\begin{equation*}
\begin{split}
\qquad \qquad \int_0^{ct} \dot{\vvomega} \; c \, \dd t &= \int_0^{ct} \vvzeta_{\mkern1mu 0} \, c \, \dd t \\[5pt]
\vvomega - \vvomega_{ \textrm{i}} &= \vvzeta_{\mkern1mu 0} \mkern1mu ct ,
\end{split}
\end{equation*}
similar in form to $\vvbeta =  \vvbeta_{\textrm{i}} + \vvzeta \mkern1mu ct$ for constant coordinate acceleration. Now, we'll need to manipulate vector \emph{components} (we'll point out why when we get there), so let's say that the motion here is in the positive or negative $x$-direction. That way the $y$- and $z$-components of the velocity, celerity, rapidity, and acceleration vectors are zero, and the traveler's $y$ and $z$ coordinates remain constant ($y_{\textrm{i}}$ and $z_{\textrm{i}}$). Then:
\begin{equation}\label{eq:ce}
\omega_x (ct) = \omega_{x \mkern2mu \textrm{i}} + \zeta_{0 \mkern2mu x} \mkern2mu ct \qquad \textrm{\footnotesize{ (for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern1mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$)}}.
\end{equation}
(The function notation serves in part to clarify that initial values like $\omega_{x \mkern2mu \textrm{i}}$ in this section correspond to when $ct = 0$. In the next section, initial values will have the same notation but will correspond to when $ct_0 = 0$.)

Equation \ref{eq:ce} is our anchor, and we need only plug it into Equations \ref{eq:rx} to find expressions for $\phi_x$, $\gamma$, and $\beta_x$ as functions of $ct$. First $\phi_x$:
\begin{equation*}
\phi_x (ct) = \sinh^{-1} \left( \omega_{x \mkern2mu \textrm{i}} + \zeta_{0 \mkern2mu x} \mkern2mu ct \right) .
\end{equation*}
Next, $\gamma$ and $\beta_x$ in terms of $\phi_x$:
\begin{equation*}
\gamma (ct) = \cosh \left( \sinh^{-1} \left( \omega_{x \mkern2mu \textrm{i}} + \zeta_{0 \mkern2mu x} \mkern2mu ct \right)  \right)
\end{equation*}
\begin{equation*}
\beta_x (ct) = \tanh \left( \sinh^{-1} \left( \omega_{x \mkern2mu \textrm{i}} + \zeta_{0 \mkern2mu x} \mkern2mu ct \right) \right).
\end{equation*}
Alternatively, we can use the identity $\cosh{(\sinh^{-1} q) = \sqrt{1 + q^2}}$ (or Equation \ref{eq:25}) to write the Lorentz factor like this:
\begin{equation*}
\gamma (ct) = \sqrt{1 + \omega^2_x} \, ,
\end{equation*}
and we can use $\beta_x = \omega_x / \gamma$ or the identity $\tanh{(\sinh^{-1} q)} = q / \sqrt{1 + q^2}$ to write $\beta_x$ like this:
\begin{equation*}
\beta_x (ct) = \dfrac{\omega_x}{\sqrt{1 + \omega^2_x}} \, .
\end{equation*}
As for position, $\vv r (ct) = \langle x (ct) , y_{\mkern1mu \mathrm{i}}, z_{\mkern1mu \mathrm{i}} \rangle$, and we integrate the above equation for $\beta_x = \dot{x}$ to find $x$:
\begin{equation*}
\int_0^{ct} \dfrac{\dd x}{\dd (ct)} \, c \, \dd t = \int_0^{ct} \dfrac{\omega_x}{\sqrt{1 + \omega^2_x}} \, c \, \dd t .
\end{equation*}
Use substitution, with $u = 1 + \omega^2_x$ and $\dd u = 2 \mkern1mu \omega_x \mkern2mu \zeta_{0 \mkern2mu x} \, c \, \dd t$ (by Equation \ref{eq:ce}):
\begin{equation*}
\begin{split}
x (ct) - x_{\mathrm{i}} &= \dfrac{1}{2 \zeta_{0 \mkern2mu x}} \int_{ct = 0}^{ct = ct} u^{-1/2} \, \dd u \\[5pt]
&= \dfrac{1}{ \zeta_{0 \mkern2mu x}} \, \sqrt{1 + \omega^2_x } \, \Big \vert^{ct = ct}_{ct = 0}
\end{split}
\end{equation*}
(division by a vector is illegal---this is why we switched to component notation). But $\gamma = \sqrt{1 + \omega^2_x}$, so that's:
\begin{equation}\label{eq:x}
x (ct) = x_{\mathrm{i}} + \dfrac{ \gamma - \gamma_{\mkern1mu \mathrm{i}} }{ \zeta_{0 \mkern2mu x}} \qquad \textrm{\footnotesize{ (for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern1mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$)}}.
\end{equation}

We can also express the accelerating traveler's \emph{proper time} as a function of the inertial observer's coordinate time. By time dilation and $\gamma = \sqrt{1 + \omega^2_x}$:
\begin{equation*}
\begin{aligned}
\int^{ct}_0 \dfrac{\dd (c t_0)}{\dd (ct)}  \, c \, \dd t &= \int^{ct}_0 \dfrac{1}{\gamma} \, c \, \dd t \\[5pt]
\int^{ct_0}_{ct_{0 \mkern1mu \mathrm{i}}} c \, \dd t_0 &= \int^{ct}_0 \dfrac{c \, \dd t}{\sqrt{1 + \omega^2_x}}
\end{aligned}
\end{equation*}
($ct_{0 \mkern2mu \mathrm{i}} = ct_0 (ct = 0)$ need not be zero!). Use $\dd \omega_x = \zeta_{0 \mkern2mu x} \, c \, \dd t$ (Equation \ref{eq:ce}):
\begin{equation*}
ct_0 (ct) - ct_{0 \mkern2mu \mathrm{i}} = \dfrac{1}{\zeta_{0 \mkern2mu x}} \int^{\omega_x}_{\omega_{x \mkern1mu \textrm{i} }} \dfrac{\dd \omega_x}{\sqrt{1 + \omega^2_x}} \, ,
\end{equation*}
and since $1/\sqrt{1 + q^2}$ is the derivative of $\sinh^{-1} q$:
\begin{equation*}
ct_0 (ct) = ct_{0 \mkern2mu \mathrm{i}} + \dfrac{\sinh^{-1} \omega_x - \sinh^{-1} \omega_{x \mkern2mu \textrm{i} }}{ \zeta_{0 \mkern2mu x}} \, ,
\end{equation*}
or:
\begin{equation}\label{eq:pt}
ct_0 (ct) = ct_{0 \mkern2mu \mathrm{i}} + \dfrac{ \phi_x - \phi_{x \mkern2mu \mathrm{i} }}{ \zeta_{0 \mkern2mu x}}  \qquad \textrm{\footnotesize{ (for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern1mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$)}}.
\end{equation}


To sum up, for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern2mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$, our ``master" function of coordinate time is Equation \ref{eq:ce}:\footnote{If $\zeta_{0 \mkern1mu x}$ isn't constant, then Equation \ref{eq:ce} becomes $\omega_x (ct) = \omega_{x \mkern1mu \textrm{i} } + \int_0^{ct} \zeta_{0 \mkern1mu x} \, c \, \dd t$, but all of the other equations we derived in this section remain the selfsame functions of $\omega_x (ct)$ (though they become different functions of $ct$).}
\begin{equation*}
\omega_x (ct) = \omega_{x \mkern2mu \textrm{i} } + \zeta_{0 \mkern2mu x} \mkern2mu ct ,
\end{equation*}
and the kinematic functions of $ct$ we built on it are:
\begin{equation*}
\begin{aligned}
\phi_x (ct) &= \sinh^{-1} \omega_x , \\[5pt]
\gamma (ct) &= \cosh \phi_x = \sqrt{1 + \omega^2_x} \, , \\[3pt]
\beta_x (ct) &= \tanh \phi_x = \dfrac{\omega_x}{\sqrt{1 + \omega^2_x }}
\end{aligned}
\end{equation*}
(from relations we already knew), and the new Equations \ref{eq:x} and \ref{eq:pt} (specific to the given conditions):
\begin{equation*}
\begin{aligned}
x (ct) &= x_{\mathrm{i}} + \dfrac{ \gamma - \gamma_{\mkern1mu \mathrm{i}} }{ \zeta_{0 \mkern2mu x}} \\[5pt]
ct_0 (ct) &= ct_{0 \mkern2mu \mathrm{i}} + \dfrac{ \phi_x - \phi_{x \mkern2mu \mathrm{i} }}{ \zeta_{0 \mkern2mu x}} .
\end{aligned}
\end{equation*}
``Ideal" initial conditions for Equations \ref{eq:x} and \ref{eq:pt} are of course $x_{\mathrm{i}} = \gamma_{\mkern1mu \mathrm{i}} / \zeta_{0 \mkern2mu x}$ and $ct_{0 \mkern2mu \mathrm{i}} = \phi_{x \mkern2mu \mathrm{i} } / \zeta_{0 \mkern2mu x}$, leaving $x = \gamma / \zeta_{0 \mkern2mu x}$ and $ct_0 = \phi_x / \zeta_{0 \mkern2mu x}$. For the whole set of equations, the ``ideal" initial condition is $\omega_{\mkern1mu \textrm{i}} = 0$, so that every $\omega_x = \omega_x (ct)$ can be replaced with $\zeta_{0 \mkern2mu x} \mkern2mu ct$.\footnote{Note that if all three conditions are met, then $ct_{0 \mkern2mu \mathrm{i}} = 0$ but $x_{\mathrm{i}} = 1 / \zeta_{0 \mkern1mu x}$ [$\neq 0$].} Also, if our $x$-components are all non-negative, then we can replace them with their corresponding vector magnitudes (drop the $x$ subscripts). The exception is $x(ct)$ itself---we can't replace it with $r(ct)$ because $\vv r$ has its tail at the origin, and we haven't required that the (constant) $y$- and $z$-coordinates be zero. Putting that all together, ``ideally":
\begin{equation*}
\begin{aligned}
\omega (ct) &= \zeta_0 \mkern2mu ct \\[3pt]
\phi (ct) &= \sinh^{-1} \big( \zeta_0 \mkern2mu ct \big) \\[5pt]
\gamma (ct) &= \sqrt{1 + \big( \zeta_0 \mkern2mu ct \big)^2} \\[5pt]
\beta (ct) &= \dfrac{\zeta_0 \mkern2mu ct}{\sqrt{1 + \big( \zeta_0 \mkern2mu ct \big)^2}} \\[5pt]
x (ct) &= \dfrac{ \gamma }{ \zeta_0} = \dfrac{\sqrt{1 + \big( \zeta_0 \mkern2mu ct \big)^2}}{ \zeta_0} \\[5pt]
ct_0 (ct) &= \dfrac{ \phi }{ \zeta_0} = \dfrac{\sinh^{-1} \big( \zeta_0 \mkern2mu ct \big)}{ \zeta_0}.
\end{aligned}
\end{equation*}


\subsubsection{Functions of Proper Time (Team Rapidity)}

For constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern2mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$ (the same rectilinear case we dealt with in the previous section), we can find kinematic functions of the accelerating traveler's \emph{proper} time by starting with Equation \ref{eq:pa3} (for $\mathring{\vvphi} \parallel \vvphi$):
\begin{equation*}
\mathring{\phi}_x = \zeta_{0 \mkern2mu x}
\end{equation*}
and integrating:
\begin{equation*}
\int_0^{ct_0} \mathring{\phi}_x \, c \, \dd t_0 = \int_0^{ct_0}  \zeta_{0 \mkern2mu x} \, c \, \dd t _0 ,
\end{equation*}
giving
\begin{equation}\label{eq:ra}
\phi_x (ct_0) = \phi_{x \mkern2mu \textrm{i} } + \zeta_{0 \mkern2mu x} \mkern2mu ct_0 \qquad \textrm{\footnotesize{ (for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern1mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$)}}.
\end{equation}

Before going further, a word of caution. The initial value $\phi_{x \mkern2mu \textrm{i} }$ in Equation \ref{eq:ra} is \emph{not} the $\phi_{x \mkern2mu \textrm{i} }$ that appeared in Equation \ref{eq:pt}. One is $\phi_x (ct_0 = 0)$, the other is $\phi_x (ct = 0)$, and they're only equivalent if we ``start the clock" on coordinate time and proper time simultaneously. Same goes for any such pair of identical-\emph{looking} initial values from this section and the last---though we use the same notation for them, they're different quantities in the general case. The function notation clarifies what's what.

Now apply Equations \ref{eq:rx} to Equation \ref{eq:ra}:
\begin{equation*}
\begin{aligned}
\gamma (ct_0) &= \cosh{\left( \phi_{x \mkern2mu \textrm{i} } + \zeta_{0 \mkern2mu x} \mkern2mu ct_0 \right)} \\[3pt]
\beta_x (ct_0) &= \tanh{\left( \phi_{x \mkern2mu \textrm{i} } + \zeta_{0 \mkern2mu x} \mkern2mu ct_0 \right)} \\[3pt]
\omega_x (ct_0) &= \sinh{\left( \phi_{x \mkern2mu \textrm{i} } + \zeta_{0 \mkern2mu x} \mkern2mu ct_0 \right)} .
\end{aligned}
\end{equation*}
Because $\mathring{x} = \omega_x$, we integrate the above equation for $\omega_x$ to find $x(ct_0)$:
\begin{equation*}
\begin{split}
\int^{ct_0}_0 \dfrac{\dd x}{\dd (c t_0)} \, c \, \dd t_0 &= \int^{ct_0}_0 \sinh{\phi_x} \, c \, \dd t_0 \\[6pt]
x(ct_0) - x_{\mathrm{i}} &= \dfrac{1}{\zeta_{0 \mkern2mu x}} \int^{\phi_x}_{\phi_{x \mkern1mu \textrm{i} }} \sinh{\phi_x} \, \dd \phi_x
\end{split}
\end{equation*}
(by Equation \ref{eq:ra}). Since $\sinh{q}$ is the derivative of $\cosh{q}$, that's:
\begin{equation}\label{eq:x2}
x(ct_0) = x_{\mathrm{i}} + \dfrac{\gamma - \gamma_{\mkern1mu \mathrm{i}}}{ \zeta_{0 \mkern2mu x}} \qquad \textrm{\footnotesize{ (for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern1mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$)}}
\end{equation}
($\gamma = \cosh{\phi_x}$). Last, we can express our inertial observer's coordinate time as a function of our accelerating traveler's proper time. By time dilation and Equation \ref{eq:ra}:
\begin{equation*}
\begin{split}
\int^{ct_0}_0 \dfrac{\dd (ct)}{\dd (c t_0)} \, c \, \dd t_0 &= \int^{ct_0}_0 \gamma \, c \, \dd t_0 \\[6pt]
\int^{ct}_{ct_{\mathrm{i}}} c \, \dd t &= \dfrac{1}{ \zeta_{0 \mkern2mu x}} \int^{\phi_x}_{\phi_{x \mkern1mu \textrm{i} }} \cosh{\phi_x} \, \dd \phi_x
\end{split}
\end{equation*}
($ct_{\mathrm{i}} = ct (ct_0 = 0)$ need not be zero). Since $\cosh q$ is the derivative of $\sinh q$:
\begin{equation}\label{eq:ct}
ct(ct_0) = ct_{\mathrm{i}} + \dfrac{\omega_x - \omega_{x \mkern2mu \textrm{i} }}{ \zeta_{0 \mkern2mu x}} \qquad \textrm{\footnotesize{ (for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern1mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$)}}
\end{equation}
($\omega_x = \sinh{\phi_x}$).

To recap, for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern2mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$, our ``master" function of proper time is Equation \ref{eq:ra}:\footnote{If $\zeta_{0 \mkern1mu x}$ isn't constant, then Equation \ref{eq:ra} becomes $\phi_x (ct_0) = \phi_{x \mkern1mu \textrm{i} } + \int_0^{ct_0} \zeta_{0 \mkern1mu x} \, c \, \dd t_0$, but all of the other equations we derived in this section remain the selfsame functions of $\phi_x (ct_0)$ (though they become different functions of $ct_0$).}
\begin{equation*}
\phi_x (ct_0) = \phi_{x \mkern2mu \textrm{i} } + \zeta_{0 \mkern2mu x} \mkern2mu ct_0 ,
\end{equation*}
and the kinematic functions of $ct_0$ we built on it are:
\begin{equation*}
\begin{aligned}
\gamma (ct_0) &= \cosh{\phi_x }  , \\[3pt]
\beta_x (ct_0) &= \tanh{\phi_x }  , \\[3pt]
\omega_x (ct_0) &= \sinh{\phi_x } 
\end{aligned}
\end{equation*}
(from relations we already knew), and the new Equations \ref{eq:x2} and \ref{eq:ct} (specific to the given conditions):
\begin{equation*}
\begin{aligned}
x(ct_0) &= x_{\mathrm{i}} + \dfrac{\gamma - \gamma_{\mkern1mu \mathrm{i}}}{ \zeta_{0 \mkern2mu x}} \\[5pt]
ct(ct_0) &= ct_{\mathrm{i}} + \dfrac{\omega_x - \omega_{x \mkern2mu \textrm{i} }}{ \zeta_{0 \mkern2mu x}} .
\end{aligned}
\end{equation*}
They're a tad sleeker than their coordinate-time counterparts, as there's no $\sinh^{-1}$ function playing middleman here. ``Ideal" initial conditions for Equations \ref{eq:x2} and \ref{eq:ct} are $x_{\mathrm{i}} = \gamma_{\mkern1mu \mathrm{i}} / \zeta_{0 \mkern2mu x}$ and $ct_{\mathrm{i}} = \omega_{x \mkern2mu \textrm{i} } / \zeta_{0 \mkern2mu x}$, leaving $x = \gamma / \zeta_{0 \mkern2mu x}$ and $ct = \omega_x / \zeta_{0 \mkern2mu x}$. If also the initial rapidity is zero, then $\phi_x (ct_0) = \zeta_{0 \mkern2mu x} \mkern2mu ct_0$, $ct_{\mathrm{i}} = 0$, and $x_{\mathrm{i}} = 1 / \zeta_{0 \mkern2mu x}$, and the initial values in this section (at $ct_0 = 0$) \emph{are} equivalent to the corresponding initial values in the previous section (at $ct = 0$). Finally, if all $x$-components are non-negative, then we can replace them with the appropriate vector magnitudes (except for $x (ct_0)$ itself, since the $y$- and $z$-coordinates---though constant---might not be zero, and $\vv r$ has its tail at the origin). So the ``ideal" versions of our proper-time functions are:
\begin{equation*}
\begin{aligned}
\phi (ct_0) &= \zeta_0 \mkern2mu ct_0 \\[3pt]
\gamma (ct_0) &= \cosh \big( \zeta_0 \mkern2mu ct_0 \big) \\[3pt]
\beta (ct_0) &= \tanh \big( \zeta_0 \mkern2mu ct_0 \big) \\[3pt]
\omega (ct_0) &= \sinh \big( \zeta_0 \mkern2mu ct_0 \big) \\[3pt]
x(ct_0) &= \dfrac{\gamma}{ \zeta_0 } = \dfrac{\cosh \big( \zeta_0 \mkern2mu ct_0 \big)}{ \zeta_0 } \\[5pt]
ct(ct_0) &= \dfrac{\omega}{ \zeta_0} = \dfrac{\sinh \big( \zeta_0 \mkern2mu ct_0 \big)}{ \zeta_0 }.
\end{aligned}
\end{equation*}



\subsubsection{Hyperbolic Motion}

The rectilinear motion with constant proper acceleration we've been studying goes by another name: \textbf{hyperbolic motion} (through spacetime). To see why, let's use our lovely functions of proper time (though we could obtain similar results with Equations \ref{eq:ce} and \ref{eq:x}).

Start with Equations \ref{eq:x2} and \ref{eq:ct}, and rearrange them:
\begin{equation*}
x(ct_0) - \left( x_{\mathrm{i}} - \dfrac{\gamma_{\mkern1mu \mathrm{i}}}{ \zeta_{0 \mkern2mu x}} \right) = \dfrac{\gamma }{ \zeta_{0 \mkern2mu x}} \qquad \qquad  ct(ct_0) - \left( ct_{\mathrm{i}} - \dfrac{\omega_{x \mkern2mu \textrm{i} }}{ \zeta_{0 \mkern2mu x}} \right) = \dfrac{\omega_x }{ \zeta_{0 \mkern2mu x}} .
\end{equation*}
The right sides of these expressions stand out: by Equation \ref{eq:25}, we know that the square of the first minus the square of the second is just $1 / \zeta^2_{0 \mkern2mu x}$, so we can eliminate two variables by squaring and subtracting our equations. Here:
\begin{equation}\label{eq:hm}
\begin{aligned}
\left[ x(ct_0) - \left( x_{\mathrm{i}} - \dfrac{\gamma_{\mkern1mu \mathrm{i}}}{\zeta_{0 \mkern2mu x}} \right) \right] ^2 &- \left[ ct(ct_0) - \left( ct_{\mathrm{i}} - \dfrac{\omega_{x \mkern2mu \textrm{i} }}{ \zeta_{0 \mkern2mu x}} \right) \right] ^2 = \dfrac{1}{ \zeta^2_{0 \mkern2mu x}} \\[8pt]
& \textrm{\footnotesize{ (for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern1mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$)}}.
\end{aligned}
\end{equation}
Since the only variables left are the spacetime coordinates $x$ and $ct$ (parameterized by $ct_0$), Equation \ref{eq:hm} is a hyperbola in the $ct$/$x$-plane. One of its two branches traces the accelerating traveler's world line: if $\zeta_{0 \mkern2mu x} > 0$, then it's the branch that opens in the positive $x$-direction; if $\zeta_{0 \mkern2mu x} < 0$, then it's the branch that opens in the negative $x$-direction.\footnote{To recover the ``parabolic motion" we know from Newtonian mechanics, we can apply the Taylor series $\cosh{q} = 1 + q^2/2! + q^4/4! + q^6/6! + \dots$ to Equation \ref{eq:x2} ($\gamma = \cosh{\phi_x}$). For sufficiently small $| \phi_x | = | \phi_{x \mkern1mu \textrm{i} } + \zeta_{0 \mkern1mu x} \mkern2mu ct_0 |$ and $| \phi_{x \mkern1mu \textrm{i} } |$:
\begin{equation*}
\begin{split}
x(ct_0) &\approx x_{\mathrm{i}} + \dfrac{ \left[ 1 + \frac{1}{2} \big( \phi_{x \mkern1mu \textrm{i} } + \zeta_{0 \mkern1mu x} \mkern2mu ct_0 \big)^2 \right] \vphantom{\big|_{\mathstrut_{\mathstrut_{A}}}}  - \Bigl[ 1 + \frac{1}{2} \phi^2_{x \mkern1mu \textrm{i} } \Bigr] }{ \zeta_{0 \mkern1mu x}}\\
&=  x_{\mathrm{i}} \,  + \, \phi_{x \mkern1mu \textrm{i} } \mkern2mu ct_0 \, + \, \dfrac{1}{2} \mkern1mu \zeta_{0 \mkern1mu x} (ct_0)^2 .
\end{split}
\end{equation*}
With $\zeta_{0 \mkern1mu x} \approx \zeta_{x}$ and $ct_0 \approx ct$ (set $ct_{\mathrm{i}} = 0$), as well as $\phi_{x \mkern1mu \textrm{i} } \approx \beta_{x \mkern1mu \mathrm{i} }$ (by Equations \ref{eq:rx} and the Taylor series $\tanh^{-1}{q} = q + q^3/3 + q^5/5 + \dots$), the parabola reduces to the familiar
\begin{equation*}
x(ct_0) \approx x_{\mathrm{i}} \, + \, \beta_{x \mkern1mu \mathrm{i} } \mkern2mu ct \, + \, \dfrac{1}{2} \mkern1mu \zeta_{x} (ct)^2 \left[ = x_{\mathrm{i}} \, + \, v_{x \mkern1mu \mathrm{i} } \mkern2mu t \, + \, \dfrac{1}{2} \mkern1mu a_{x} t^2 \right] .
\end{equation*}
}

Equation \ref{eq:hm} is an \emph{equilateral} (or \emph{rectangular}) hyperbola, meaning that its asymptotes have slopes $c \mkern.5mu \Delta t / \Delta x = \pm \, 1$ and intersect at right angles at the hyperbola's center. Specifically, the asymptotes are the lines
\begin{equation*}
ct = \left( ct_{\mathrm{i}} - \dfrac{\omega_{x \mkern2mu \textrm{i} }}{ \zeta_{0 \mkern2mu x}} \right) \pm \left[  x - \left( x_{\mathrm{i}} - \dfrac{\gamma_{\mkern1mu \mathrm{i}}}{ \zeta_{0 \mkern2mu x}} \right) \right],
\end{equation*}
and the hyperbola's center is the event in spacetime whose coordinates in this frame are $ct = ct_{\mathrm{i}} - \omega_{x \mkern2mu \textrm{i} } / \zeta_{0 \mkern2mu x}$ and $x = x_{\mathrm{i}} - \gamma_{\mkern1mu \mathrm{i}} / \zeta_{0 \mkern2mu x}$ (and whatever the accelerating traveler's $y$ and $z$ coordinates are---they're constant, remember). 

There are some intriguing consequences to all this. First, notice that Equation \ref{eq:hm} tells us the spacetime interval $\Delta s$ between a given event on the accelerating traveler's world line and the event that is the hyperbola's center (henceforth the ``center event"):
\begin{equation*}
(\Delta s)^2 = (c \mkern.5mu \Delta t)^2 - (\Delta x)^2 = - \dfrac{1}{ \zeta^2_{0 \mkern2mu x}},
\end{equation*}
where $c \mkern.5mu \Delta t = ct - (ct_{\mathrm{i}} - \omega_{x \mkern2mu \textrm{i} } / \zeta_{0 \mkern2mu x})$ and $\Delta x = x - (x_{\mathrm{i}} - \gamma_{\mkern1mu \mathrm{i}} / \zeta_{0 \mkern2mu x})$ are the differences in the time and space coordinates of the events (again, as measured in any of the infinitely many inertial frames for which the traveler's $y$ and $z$ coordinates don't change). This interval is spacelike, since its square is negative, and it's also constant. The invariant $\Delta s / \mathrm{i} = |1 / \zeta_{0 \mkern2mu x}|$ is therefore the proper distance between the center event and \emph{every event on the traveler's world line}! What's more, $ct(ct_0) = ct_{\mathrm{i}} - \omega_{x \mkern2mu \textrm{i} } / \zeta_{0 \mkern2mu x}$ when the traveler is momentarily at rest in any of these frames (Equation \ref{eq:ct} with $\omega_x = 0$), giving $c \mkern.5mu \Delta t = 0$ and $(\Delta s)^2 = - (\Delta x)^2$ at that instant. This means that the center event is simultaneous with the traveler's ``turnaround event" (graphically this is obvious: the turnaround event is one of the hyperbola's vertices, and the vertices and the center lie on the major axis $ct = ct_{\mathrm{i}} - \omega_{x \mkern2mu \textrm{i} } / \zeta_{0 \mkern2mu x}$). But there's an instantaneous rest frame corresponding to \emph{every} event on the traveler's world line; they \emph{all} get their ``turn" as the turnaround event. So what we're really saying is that for each of the traveler's successive instantaneous rest frames, the center event is simultaneous with the traveler's coming to momentary rest. As far as the traveler is concerned, the center event is always happening ``right now" at a distance $|1 / \zeta_{0 \mkern2mu x}|$ in the direction opposite to the acceleration.\footnote{When we speak of \emph{right now} and \emph{at a distance} here, we're assuming that the accelerating traveler always adopts the $(ct, x, y, z)$ coordinates of the current instantaneous rest frame. But it's important to understand that although these are the only kinds of coordinates we've worked with in this paper, \emph{they are just coordinates}, and there are other options. For a traveler with constant proper acceleration, a better choice is arguably \textbf{Rindler coordinates}, which are like the spacetime analogue of polar coordinates, and in which the center event is \emph{not} ``eternal" and a fixed distance away (actually, the center event \emph{has no representation} in Rindler coordinates, much as the pole has no representation in polar coordinates). This ``eternal" and ``fixed-distance" business is just an artifact of choosing particular coordinate systems. There's nothing truly physical about it!}

If the center event's perpetual ``now-and-there-ness" for the accelerating traveler is difficult to take in, it may help to confirm it with a Lorentz transformation of $c \mkern.5mu \Delta t $ and $\Delta x$ from the preceding paragraph (i.e., the differences in the $ct$ and $x$ coordinates of the center event and an arbitrary event on the traveler's world line, as reckoned in one of our qualifying frames). First, by Equations \ref{eq:ct} and \ref{eq:x2}:
\begin{equation*}
c \mkern.5mu \Delta t = ct - \left( ct_{\mathrm{i}} - \dfrac{\omega_{x \mkern2mu \textrm{i} }}{ \zeta_{0 \mkern2mu x}} \right) = \dfrac{\omega_x}{ \zeta_{0 \mkern2mu x}}
\end{equation*}
and
\begin{equation*}
\Delta x = x - \left( x_{\mathrm{i}} - \dfrac{\gamma_{\mkern1mu \textrm{i} }}{ \zeta_{0 \mkern2mu x}} \right) = \dfrac{\gamma}{ \zeta_{0 \mkern2mu x}} .
\end{equation*}
Now use Equations \ref{eq:lt2} to boost to the traveler's (primed) instantaneous rest frame, noting that the boost parameters are $\gamma_{\mathrm{rel}} = \gamma$ and $\omega_{\mathrm{rel}} = \pm \, \omega_x$ (negative if $\omega_x < 0$, in which case this is an ``inverse" transformation and the $\omega_{\textrm{rel}}$ terms below take the $+$ sign):
\begin{equation*}
c \mkern.5mu \Delta t^\prime = \gamma_{\textrm{rel}} \, c \mkern.5mu \Delta t \, \pm \, \omega_{\textrm{rel}} \, \Delta x = \gamma \, \dfrac{\omega_x}{ \zeta_{0 \mkern2mu x}} - \omega_x \, \dfrac{\gamma}{ \zeta_{0 \mkern2mu x}} = 0
\end{equation*}
and
\begin{equation*}
\Delta x^\prime = \gamma_{\textrm{rel}} \, \Delta x \, \pm \, \omega_{\textrm{rel}} \, c \mkern.5mu \Delta t = \gamma \, \dfrac{\gamma}{ \zeta_{0 \mkern2mu x}} - \omega_x \, \dfrac{\omega_x}{ \zeta_{0 \mkern2mu x}} = \dfrac{1}{ \zeta_{0 \mkern2mu x}}
\end{equation*}
(by Equation \ref{eq:25}). So we see again that in each instantaneous rest frame, the center event is simultaneous with the traveler's coming to momentary rest and occurs a distance $|1 / \zeta_{0 \mkern2mu x}|$ away from the traveler.

Next, consider the asymptotes. With their slopes of $\pm \, 1$, they correspond to \emph{lightlike} world lines. In fact, let's go ahead and imagine that they \emph{are} the world lines of two pulses of light that were emitted infinitely long ago---one from far off in the negative $x$-direction and aimed in the positive $x$-direction, the other vice versa, and both with constant $y$ and $z$ coordinates identical to the accelerating traveler's. The pulses then intersect at the center event $(ct_{\mathrm{i}} - \omega_{x \mkern2mu \textrm{i} } / \zeta_{0 \mkern2mu x}, \, x_{\mathrm{i}} - \gamma_{\mkern1mu \mathrm{i}} / \zeta_{0 \mkern2mu x}, \, y_{\mathrm{i}}, \, z_{\mathrm{i}})$. Asymptotes being lines that the hyperbola approaches but never reaches, our qualifying inertial observers would say that one pulse approaches but never reaches the traveler, while the other was arbitrarily \emph{close} to the traveler in the distant past but always a smidge ahead. And as far as the accelerating traveler is concerned, the ``eternal" center event a fixed distance $|1 / \zeta_{0 \mkern2mu x}|$ away constitutes the pulses' entire existence!\footnote{Again, we're assuming that the traveler is using the time and space coordinates of the current instantaneous rest frame. There are other options.} They always were and always will be intersecting. We arrive at the inescapable conclusion that \emph{a traveler with constant proper acceleration will ``outrun" a pulse of light that's approaching from the direction opposite to the acceleration}, provided that the light is at least a proper distance $|1 / \zeta_{0 \mkern2mu x}|$ away.

Since information can travel no faster than $\beta = 1$, it also follows that information about any event \emph{beyond} the ``approaching" asymptote (the one that comes arbitrarily close to the relevant hyperbola branch in the \emph{future}) will never reach the ever-accelerating traveler. That asymptote then represents an unphysical but nonetheless meaningful ``boundary" in spacetime---called the \textbf{future Rindler horizon}---that ``splits" the $ct$/$x$-plane into two regions: a ``knowable" region populated by events whose existence the traveler may learn of, and an ``unknowable" region populated by events whose existence the traveler must remain forever ignorant of.\footnote{The future Rindler horizon (named after Wolfgang Rindler) is quite like the \textbf{event horizon} of a \textbf{black hole}, though the former's existence depends on the traveler's ``choice" to continue accelerating, whereas the latter is a gravitational phenomenon.} The other asymptote (the one that was arbitrarily close to the accelerating traveler in the \emph{past}) is the \textbf{past Rindler horizon}, and it splits the $ct$/$x$-plane into two regions as well: a ``pingable" region that can receive light signals from the traveler, and an ``unpingable" region that cannot.\footnote{Don't mistake this for a statement that \emph{observers} in the unpingable region can never receive signals from the traveler. On the contrary, an inertial observer in the unpingable region will eventually ``cross" the past horizon and enter the pingable region. Our accelerating traveler, on the other hand, will never cross the future horizon and enter the unknowable region.} Altogether the asymptotes quarter the $ct$/$x$-plane into four \textbf{Rindler wedges}: one contains the traveler's world line and is both knowable and pingable; a second is knowable but unpingable (it's behind the past horizon but not the future horizon); a third is pingable but unknowable (behind the future horizon but not the past horizon); and the last is both unknowable and unpingable (behind both Rindler horizons).


%\subsubsection{Born Rigidity}
%
%
%
%\subsubsection{Rindler Coordinates}

%So far in this paper, we've \emph{only} used ($ct$ + Cartesian) coordinate to describe events and four-vector components. And after this little section, we'll continue that practice. But there are other possibilities! For example, we can use ($ct$ + spherical) or ($ct$ + cylindrical) coordinates, and we can figure out how those spatial coordinates transform under a Lorentz boost by combining our knowledge of how Cartesian coordinates transform with our knowledge of how Cartesian coordinates are related to spherical or cylindrical coordinates.
%
%Now take it one step further: if we can ``keep" the $ct$ coordinate but ``mix" the Cartesian coordinates as we've just described, why can't we instead \emph{include} the $ct$ coordinate in the mixing? The answer is, we can! And for our traveler with constant proper acceleration, this is actually the natural choice. Let's assume ``ideal" conditions, so that a qualifying inertial observer writes the traveler's world line in ($ct$ + Cartesian) coordinates like this:
%\begin{equation*}
%x(ct_0) = \dfrac{\cosh \big( \zeta_0 \mkern2mu ct_0 \big)}{ \zeta_0 } \qquad \qquad ct(ct_0) = \dfrac{\sinh \big( \zeta_0 \mkern2mu ct_0 \big)}{ \zeta_0 }.
%\end{equation*}
%What we'd like to do is mix the ``old" $ct$ and $x$ coordinates into a pair of ``new" ones in such a way that one of the new coordinates \emph{remains constant} along the traveler's world line (the traveler will regard this as a kind of ``position" coordinate) while the other \emph{changes proportionally with the invariant $ct_0$} (a kind of ``time" coordinate). The idea is that in the new coordinate system, the traveler will be ``at rest," and ``time" will be what the traveler's wristwatch logs---that's why it's the ``natural choice." There's more than one way to accomplish this. Here is maybe the simplest, called \textbf{Rindler coordinates}:
%\begin{equation*}
%X = \sqrt{x^2 - (ct)^2} \qquad \qquad cT = \dfrac{1}{\zeta_0} \, \tanh^{-1} \dfrac{ct}{x} ,
%\end{equation*}
%with the ``inverse" transformation:
%\begin{equation*}
%x = X \cosh \big( \zeta_0 \mkern2mu cT \big) \qquad \qquad ct = X \sinh \big( \zeta_0 \mkern2mu cT \big),
%\end{equation*}
%($Y = y$ and $Z = z$). Note that the Rindler ``time" coordinate $cT$ is just the traveler's proper time $ct_0$, and that along the traveler's world line the ``position" coordinate $X$ is constant (equals $1 / \zeta_0$).

%The Rindler wedge (the knowable and pingable part of the $ct$/$x$-plane) is of particular interest. Let's now stipulate that our ``ideal" initial conditions hold ($x_{\mathrm{i}} = \gamma_{\mkern1mu \mathrm{i}} / \zeta_{0 \mkern2mu x}$, $ct_{\mathrm{i}} = \omega_{x \mkern2mu \textrm{i} } / \zeta_{0 \mkern2mu x}$, and $\phi_{x \mkern2mu \textrm{i} } = 0$, with the traveler's acceleration in the positive $x$-direction), and also that $y_{\mathrm{i}} = z_{\mathrm{i}} = 0$. Then the center event has coordinates $ct = ct_{\mathrm{i}} - \omega_{x \mkern2mu \textrm{i} } / \zeta_{0 \mkern2mu x} = 0$ and $x = x_{\mathrm{i}} - \gamma_{\mkern1mu \mathrm{i}} / \zeta_{0 \mkern2mu x} = 0$, and the accelerating traveler's four-position in component form is: 
%\begin{equation*}
%\vv R (ct_0) = \left \langle \dfrac{\sinh \big( \zeta_0 \mkern2mu ct_0 \big)}{ \zeta_0 }, \, \dfrac{\cosh \big( \zeta_0 \mkern2mu ct_0 \big)}{ \zeta_0 } , \, 0, \, 0 \right \rangle .
%\end{equation*}
%Actually, if all this is true for one inertial frame, then it's true for \emph{every} frame that's in standard configuration with it, since all such frames share a spacetime origin that is the center event. [...]



\clearpage

\section{Electrodynamics with Four-Vectors}\label{sec:rem}

Near the beginning of this document, we took a quantum shortcut: instead of using classical electromagnetism to show that the energy of light transforms between frames in the same manner as its frequency, we used the Planck--Einstein relation. This wasn't quite ahistorical, but it wasn't entirely satisfactory, either, and we pledged that we'd come back and redo it the ``right" way, without invoking the photon model. The time has come to settle that unfinished business. We'll start by formulating the basics of classical electrodynamics from scratch using four-vectors---no explicit tensor analysis, no index notation, no differential forms.\footnote{Section \ref{sec:dy}, by contrast, is a foray into dyadics, which is kind of like ``tensors lite." Also, I want to stress that while we'll get pretty far with our vector and dyadic approaches---farther than most realize is feasible, I'd wager---one really \emph{must} learn index notation to go deeper with the material or to move on to more advanced topics like general relativity. Index notation has a learning curve, but it's extremely powerful, and it's even beautiful once you get the hang of it. You should learn it! See Section \ref{ssec:index}.} Then we'll derive the transformation law for the energy of a light wave the way that Einstein did it (more or less). But first we'll need to go over some spacetime vector calculus. Familiarity with Euclidean vector calculus is assumed, though we'll keep our steps transparent and point out the identities we use.


\subsection{The Math (Spacetime Vector Calculus)}


\subsubsection{The Four-Del}\label{sssec:fd}

We'll need a four-vector equivalent of the ``del" (``nabla") symbol:
\begin{equation*}
\del = \left \langle \dfrac{\partial}{\partial x}, \, \dfrac{\partial}{\partial y}, \, \dfrac{\partial}{\partial z} \right \rangle .
\end{equation*}
Let's notate it $\partialup$ (``vectorized" $\partial$ symbol). A natural guess is:
\begin{equation*}
\partialup \stackrel{?}{=} \left \langle \dfrac{\partial}{\partial (ct)} , \, \dfrac{\partial}{\partial x}, \, \dfrac{\partial}{\partial y}, \, \dfrac{\partial}{\partial z} \right \rangle = \left \langle \dfrac{\partial}{\partial (ct)} , \, \del \right \rangle ,
\end{equation*}
but do these components transform Lorentzianly under a boost? If not, then $\partialup$ isn't a four-vector, and using it to operate on a spacetime scalar field (whose value at each event is invariant) won't have the intended effect of producing a four-vector field. To find out, use the multivariable chain rule and Equations \ref{eq:lt} (standard configuration, so $ct^\prime$ and $x^\prime$ are functions of $ct$ and $x$, and vice versa). In detail for the $ct^\prime$-partial, and then similarly for the $x^\prime$-partial:
\begin{equation*}
\begin{split}
\dfrac{\partial}{\partial (ct^\prime)} &= \dfrac{\partial (ct)}{\partial (ct^\prime)} \, \dfrac{\partial}{\partial (ct)} \, + \, \dfrac{\partial x}{\partial (ct^\prime)} \, \dfrac{\partial}{\partial x} \\[3pt] &= \dfrac{\partial}{\partial (ct^\prime)} \big( \gamma \mkern1mu ct^\prime + \gamma \beta x^\prime \big) \dfrac{\partial}{\partial (ct)} \, + \, \dfrac{\partial}{\partial (ct^\prime)} \big( \gamma x^\prime + \gamma \beta \mkern1mu ct^\prime \big) \dfrac{\partial}{\partial x} \\[3pt] &= \gamma \left( \dfrac{\partial}{\partial (ct)} + \beta \, \dfrac{\partial}{\partial x} \right) \\[3pt]
\dfrac{\partial}{\partial x^\prime} &= \dfrac{\partial x}{\partial x^\prime} \, \dfrac{\partial}{\partial x} \, + \, \dfrac{\partial (ct)}{\partial x^\prime} \, \dfrac{\partial}{\partial (ct)}  = \gamma \left( \dfrac{\partial}{\partial x} + \beta \, \dfrac{\partial}{\partial (ct)}  \right) .
\end{split}
\end{equation*}
That's not the Lorentz transformation; it's the \emph{inverse} Lorentz transformation! The \emph{Lorentz} transformation would then get us the unprimed components from the primed ones. This is precisely the opposite of how a four-vector's components transform.\footnote{\label{fn:cv}An object in Minkowski spacetime whose components transform in this ``backward" way is called a \textbf{four-\emph{co}vector}. Covectors are also called one-forms, dual vectors, or covariant vectors (if the latter, then vectors are called ``contravariant vectors"); the word \emph{covariant} here has a different meaning than it does in the term \emph{manifestly covariant}.} If we simply change the sign of $\del$, then the components \emph{do} transform like those of a four-vector:
\begin{equation*}
\begin{split}
\dfrac{\partial}{\partial (ct^\prime)} &= \gamma \left[ \dfrac{\partial}{\partial (ct)} - \beta \left(- \dfrac{\partial}{\partial x} \right) \right] \\[3pt]
- \dfrac{\partial}{\partial x^\prime} &= \gamma \left[ \left( - \dfrac{\partial}{\partial x} \right) - \beta \, \dfrac{\partial}{\partial (ct)}  \right] \\[3pt]
- \dfrac{\partial}{\partial y^\prime} &= - \dfrac{\partial}{\partial y} \\[3pt]
- \dfrac{\partial}{\partial z^\prime} &= - \dfrac{\partial}{\partial z} .
\end{split}
\end{equation*}
There we go. Our \textbf{four-del} symbol is:
\begin{equation}\label{eq:fn}
\boxed{
\begin{aligned}
\partialup &\equiv \left \langle \dfrac{\partial}{\partial (ct)} , \, - \del \right \rangle \\[3pt]
&= \left \langle \dfrac{\partial}{\partial (ct)} , \, - \dfrac{\partial}{\partial x}, \, - \dfrac{\partial}{\partial y}, \, - \dfrac{\partial}{\partial z} \right \rangle \\[3pt]
&=  \left \langle \partial^{ct}, \, \partial^x , \, \partial^y , \, \partial^z \right \rangle
\end{aligned}
}
\end{equation}
(recall that we use superscript for four-vector components).\footnote{The reason we use superscript for four-vector components is that subscripts are used for the components of \emph{co}vector components in more advanced treatments, and it's important to distinguish them (see Footnote \ref{fn:cv}). We won't use covectors explicitly, but we abide by this notational convention. In \emph{Euclidean} space, the Cartesian components of covectors are identical to those of vectors, so it doesn't really matter whether we use superscripts or subscripts for our three-vector components (we'll continue using subscripts).} We must be careful with signs when we use $\partial^x$, $\partial^y$, and $\partial^z$. These spatial components of $\partialup$ are \emph{negative} partial-derivative operators, but $\partial^{ct}$ is a \emph{positive} partial.


\subsubsection{Four-Gradient, Four-Divergence, and the d'Alembertian}

Like $\del$ in three-dimensional Euclidean space, the symbol $\partialup$ in Minkowski spacetime has several uses. Here are a few of them:
\begin{itemize}
\item{
When it operates on a spacetime scalar field $\psi = \psi (\vv R)$, it's the \textbf{four-gradient} function, and it outputs a four-vector field:
\begin{equation*}
\partialup \psi = \left \langle \partial^{ct} \psi, \, -\del \psi \right \rangle.
\end{equation*}
}
\item{
When it's dotted with a four-vector field $\vv Q = \vv Q (\vv R) = \langle Q^{ct}, \vv q \rangle$, we get the \textbf{four-divergence} function, and it outputs a spacetime scalar field:
\begin{equation*}
\begin{split}
\partialup \cdot \vv Q &= \partial^{ct} Q^{ct} - \left( - \del \cdot \vv q \right) \\
&= \partial^{ct} Q^{ct} + \del \cdot \vv q ,
\end{split}
\end{equation*}
whose value at every event is Lorentz-invariant.
}
\item{
When $\partialup$ is dotted with \emph{itself}, the resulting function is called the \textbf{wave operator}, the \textbf{four-Laplacian}, or the \textbf{d'Alembertian}, labeled $\Box$:\footnote{Another convention---preferable, perhaps, but less common these days---uses $\Box$ as the four-del symbol and $\Box^2$ as the d'Alembertian, mirroring the use of $\del$ for the three-del and $\nabla ^2 = \del \cdot \del$ for the three-Laplacian. The symbol $\Box$ is sometimes called \emph{quabla}.}
\begin{equation*}
\begin{split}
\Box &\equiv \partialup \cdot \partialup = \partial^{ct} \partial^{ct} - \left[ \left( - \del \right) \cdot \left( - \del \right) \right] \\[3pt]
&= \partial^{ct} \partial^{ct} - \nabla ^2 \\[2pt]
&= \dfrac{\partial^2}{\partial (ct)^2} - \dfrac{\partial^2}{\partial x^2} - \dfrac{\partial^2}{\partial y^2} - \dfrac{\partial^2}{\partial z^2} .\\
\end{split}
\end{equation*}
}
\end{itemize}
The d'Alembertian is the four-divergence of the four-gradient. Like the Laplacian, it can operate on a scalar field \emph{or} a vector field, always outputting a field of the same kind as its input.\footnote{\label{fn:gd}Does this mean that a vector field can have a gradient? Doesn't that contradict what we said above about the four-gradient acting on scalar fields? It depends on how we define the word \emph{gradient}, but basically the answer is yes! The details involve tensors (briefly---the gradient of a tensor field of rank $n$ is a tensor field of rank $n + 1$, and the divergence of a tensor field of rank $n$ is a tensor field of rank $n - 1$), but for now it's enough to know that the Laplacian and d'Alembertian can operate on both scalar and vector fields.} Think of the d'Alembertian as the four-del's invariant ``squared magnitude." It acts like a \emph{scalar} when combined with other operators, in the sense that the order of the operations is reversible: $\Box (\partialup \psi) = \partialup (\Box \psi)$ and $\Box (\partialup \cdot \vv Q) = \partialup \cdot (\Box \vv Q)$.\footnote{It's a good exercise to verify these relations and the analogous Laplacian identities $\nabla ^2 (\del f) = \del (\nabla ^2 f)$ and $\nabla ^2 (\del \cdot \vv c) = \del \cdot (\nabla ^2 \vv c)$. It boils down to the fact that the order of mixed partials doesn't matter (e.g., $\partial^{ct} \del \cdot \vv c = \del \cdot \partial^{ct} \vv c$, and $\partial^{ct} \del f = \del \partial^{ct} f$).} By contrast, the four-gradient of the four-divergence is not equivalent to the four-divergence of the four-gradient (the d'Alembertian): $\partialup (\partialup \cdot \vv Q) \neq \Box \vv Q$.


\subsubsection{Differential Triple Products; Gauge Transformations}\label{sssec:tp}

In three-dimensional Euclidean space, we have the ``$bac - cab$" mnemonic to help us remember how the vector triple product expands into two terms:
\begin{equation*}
\vv a \times (\vv b \times \vv c) = \vv b (\vv a \cdot \vv c) - \vv c (\vv a \cdot \vv b).
\end{equation*}
If $\vv b$ and $\vv c$ are given, we can regard the vector triple product as a function that takes a vector as input and returns a new vector as output:
\begin{equation*}
\text{\underline{\hspace{.6em}}} \times (\vv b \times \vv c) = \vv b ( \mkern1.5mu \text{\underline{\hspace{.6em}}} \cdot \vv c ) - \vv c ( \mkern1.5mu \text{\underline{\hspace{.6em}}} \cdot \vv b ).
\end{equation*}
A special case is when $\vv b = \del$ and $\vv c$ is some three-vector field $\vv c = \vv c (\vv r)$:
\begin{equation}\label{eq:cc}
\begin{split}
\text{\underline{\hspace{.6em}}} \times (\del \times \vv c) &= \del_{\vv c} ( \mkern1.5mu \text{\underline{\hspace{.6em}}} \cdot \vv c ) - \vv c ( \mkern1.5mu \text{\underline{\hspace{.6em}}} \cdot \del_{\vv c} ) \\[2pt]
&= \del_{\vv c} ( \mkern1.5mu \text{\underline{\hspace{.6em}}} \cdot \vv c ) - ( \mkern1.5mu \text{\underline{\hspace{.6em}}} \cdot \del ) \vv c ,
\end{split}
\end{equation}
where the subscript clarifies that $\del$ operates on $\vv c$ (and $\vv c$ alone) regardless of its placement in a term.\footnote{\label{fn:fsn}Richard Feynman tried to popularize this handy notation, though it's still somewhat rare. See: \url{http://www.feynmanlectures.caltech.edu/II_27.html\#Ch27-S3}.} This function is the cross-product-to-be of an input-vector and the curl of the field $\vv c$. If we feed it an $\vv a$, we have:
\begin{equation*}
\vv a \times (\del \times \vv c) = \del_{\vv c} ( \vv a \cdot \vv c ) - ( \vv a \cdot \del ) \vv c ,
\end{equation*}
and we note in passing that $(\vv a \cdot \del)$ is a \textbf{directional derivative} operator. We'll call $\vv a \times (\del \times \vv c)$ the \textbf{differential three-vector triple product}. Carrying out the dot products:
\begin{equation*}
\begin{split}
\vv a \times (\del \times \vv c) &= \del_{\vv c} \left( a_x c_x + a_y c_y + a_z c_z \right) - \left( a_x \, \dfrac{\partial}{\partial x} + a_y \, \dfrac{\partial}{\partial y} + a_z \, \dfrac{\partial}{\partial z} \right) \vv c \\[3pt]
&= \left( a_x \del c_x + a_y \del c_y + a_z \del c_z \right) - \left( a_x \, \dfrac{\partial \vv c}{\partial x} + a_y \, \dfrac{\partial \vv c}{\partial y} + a_z \, \dfrac{\partial \vv c}{\partial z} \right) ,
\end{split}
\end{equation*}
which, after some careful work (do it!), gives the Cartesian component form:
\begin{equation*}
\begin{split}
& \Bigg \langle a_y \left( \dfrac{\partial c_y}{\partial x} - \dfrac{\partial c_x}{\partial y} \right) + a_z \left( \dfrac{\partial c_z}{\partial x} - \dfrac{\partial c_x}{\partial z} \right) , \\
&\qquad  a_z \left( \dfrac{\partial c_z}{\partial y} - \dfrac{\partial c_y}{\partial z} \right) + a_x \left( \dfrac{\partial c_x}{\partial y} - \dfrac{\partial c_y}{\partial x} \right) , \\
&\qquad \qquad  a_x \left( \dfrac{\partial c_x}{\partial z} - \dfrac{\partial c_z}{\partial x} \right) + a_y \left( \dfrac{\partial c_y}{\partial z} - \dfrac{\partial c_z}{\partial y} \right) \Bigg \rangle .
\end{split}
\end{equation*}
This is an interesting vector. All three components of $\del \times \vv c$ make an appearance, as do their additive inverses (the components of what $\del \times \vv c$ would be under a left-hand rule). As the cross product $\vv a \times (\del \times \vv c)$, it's necessarily orthogonal to the input-vector $\vv a$, but that's evident from the vector's component form, too---if we multiply each component with the corresponding component of $\vv a$ and sum the resulting products (i.e., take the Euclidean dot product), we get zero.

In \emph{four} dimensions, an axis that's orthogonal to a given pair of vectors isn't unique, so there's no such thing as a Minkowski cross product or curl, and the expressions above containing the $\times$ symbol have no direct four-vector counterparts.\footnote{Actually, some people \emph{do} speak of a ``four-curl" in relation to what we're about to do, but this is a misnomer unless one tweaks the definition of \emph{curl} (and we won't).} We do, however, have a Minkowski dot product and a four-del, and that's enough to make four-vector triple products fashioned after the Euclidean ``$bac - cab$" expansions. In our study of electromagnetism, we'll find it particularly useful to have a spacetime analogue of the ``cross product of a curl" function. Mimicking the right side of Equation \ref{eq:cc} with the four-del and a four-vector field $\vv Q = \vv Q (\vv R)$ (and the \emph{Minkowski} dot product), that's:
\begin{equation*}
\partialup_{\vv Q} ( \mkern1.5mu \text{\underline{\hspace{.6em}}} \cdot \vv Q) - ( \mkern1.5mu \text{\underline{\hspace{.6em}}} \cdot \partialup) \vv Q .
\end{equation*}
Input a four-vector $\vv W$, and we have:
\begin{equation*}
\partialup_{\vv Q} ( \vv W \cdot \vv Q) - ( \vv W \cdot \partialup) \vv Q ,
\end{equation*}
a new four-vector (the second term is a directional derivative). This is our \textbf{differential four-vector triple product}. For convenience, we'll use a shorter notation for it, modeled after its three-vector counterpart:
\begin{equation}\label{eq:dtp}
\vv W \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv Q) \equiv \partialup_{\vv Q} ( \vv W \cdot \vv Q) - ( \vv W \cdot \partialup) \vv Q .
\end{equation}
The scare quotes remind us that we have \emph{not} defined the cross product or curl as four-vector operations (don't take the $\times$ notation literally!).\footnote{Warning: neither this notation nor the very concept of a ``differential four-vector triple product" is standard. But with the differential four-vector triple product in our toolkit, we can cover the basics of relativistic electrodynamics without getting mired in tensor analysis and index notation.}
 
With $\vv W = \langle W^{ct}, \, \vv w \rangle$ and $\vv Q = \langle Q^{ct} , \, \vv q \rangle$, we can put the right side of Equation \ref{eq:dtp} into component form. First carry out the dot products:
\begin{equation*}
\begin{split}
\vv W \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv Q) &= \partialup_{\vv Q} \left[ W^{ct} Q^{ct} - \left( \vv w \cdot \vv q \right) \right] - \left[ W^{ct} \partial^{ct} - \big(  \vv w \cdot [ - \del ] \big) \right] \vv Q \\[8pt]
&= W^{ct} \partialup Q^{ct} - \partialup_{\vv q} ( \vv w \cdot \vv q ) - W^{ct} \partial^{ct} \vv Q - ( \vv w \cdot \del ) \vv Q
\end{split}
\end{equation*}
(the $-\del$ comes from Equation \ref{eq:fn}). Now in component form:
\begin{equation*}
\begin{split}
& \left \langle W^{ct} \partial^{ct} Q^{ct} - \vv w \cdot \partial^{ct} \vv q - W^{ct} \partial^{ct} Q^{ct} - \vv w \cdot \del Q^{ct} , \right. \\[3pt]
& \qquad \left. W^{ct} (- \del Q^{ct}) - [- \del_{\vv q} (\vv w \cdot \vv q)]  -  W^{ct} \partial^{ct} \vv q - (\vv w \cdot \del) \vv q \right \rangle \\[10pt]
&= \left \langle \vv w \cdot \left( - \del Q^{ct} - \partial^{ct} \vv q  \right) , \, W^{ct} \left( - \del Q^{ct} - \partial^{ct} \vv q \right) \, + \, \vv w \times (\del \times \vv q) \right \rangle ,
\end{split}
\end{equation*}
where we've again used the identity $\del_{\vv q} (\vv w \cdot \vv q) - (\vv w \cdot \del) \vv q = \vv w \times (\del \times \vv q)$ (differential \emph{three}-vector triple product). Not much to look at, but there's a logic to it, and if we define the three-vectors ${\vv m \equiv - \del Q^{ct} - \partial^{ct} \vv q}$ and ${\vv n \equiv \del \times \vv q}$ (time-dependent three-vector \emph{fields}, really, since $\vv Q$ is a four-vector field), then:
\begin{equation}\label{eq:dt}
\vv W \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv Q) = \left \langle \vv w \cdot \vv m , \, W^{ct} \vv m + \vv w \times \vv n \right \rangle .
\end{equation}
This form makes plain that the output-vector is orthogonal to the input-vector $\vv W$ (dot product is zero), as $(\vv w \times \vv n) \cdot \vv w = 0$. So even though we have no Minkowski cross product or curl, Equation \ref{eq:dt} is indeed closely analogous to the cross product of a curl in three-dimensional Euclidean space.

Now, we know that $\vv a \times [ \del \times (\vv c + \del f) ] = \vv a \times (\del \times \vv c)$ for any scalar field $f$ (because the curl of a gradient is always the zero vector). This is apparent from the ``$bac - cab$" expansion, as well:
\begin{equation*}
\begin{split}
\vv a \times [ \del \times (\vv c + \del f) ] &= \del_{( \vv c \, + \del f )} \, [ \vv a \cdot ( \vv c + \del f ) ] - (\vv a \cdot \del) ( \vv c + \del f ) \\
&= \del_{\vv c} ( \vv a \cdot \vv c ) + \del_{f} ( \vv a \cdot \del f ) - (\vv a \cdot \del) \vv c - (\vv a \cdot \del) \del f \\
&= \del_{\vv c} ( \vv a \cdot \vv c ) - (\vv a \cdot \del) \vv c \\
&= \vv a \times (\del \times \vv c) ,
\end{split}
\end{equation*}
where we've used $\del_{f} ( \vv a \cdot \del f ) = \del_{f} ( \vv a \cdot \del ) f = ( \vv a \cdot \del ) \del f$ (write it out in terms of components if you need convincing). By the same token, we find that our differential \emph{four-vector} triple product is invariant under a transformation that takes the vector field $\vv Q$ to $(\vv Q + \partialup \psi)$ for any spacetime scalar field $\psi$:
\begin{equation*}
\begin{split}
\vv W \mkern1mu `` \mkern-4mu \times [ \partialup \times \mkern-4mu " \mkern1mu ( \vv Q + \partialup \psi ) ] &= \partialup_{( \vv Q  \, + \, \partialup \psi )} \, [ \vv W \cdot ( \vv Q + \partialup \psi ) ] - (\vv W \cdot \partialup) ( \vv Q + \partialup \psi ) \\
&= \partialup_{\vv Q} ( \vv W \cdot \vv Q ) + ( \vv W \cdot \partialup) \partialup \psi - (\vv W \cdot \partialup) \vv Q - (\vv W \cdot \partialup) \partialup \psi \\
&= \vv W \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv Q) .
\end{split}
\end{equation*}
This is an important identity! It means that if we ever encounter a physical quantity equal to $\vv W \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv Q)$ (we will!), we can be sure that it's also equal to $\vv W \mkern1mu `` \mkern-4mu \times [ \partialup \times \mkern-4mu " \mkern1mu ( \vv Q + \partialup \psi ) ]$ for any $\psi$ we choose. This is an example of \textbf{gauge invariance}, and adding a $\partialup \psi$ to $\vv Q$ here is called making a \textbf{gauge transformation}.

Why might we want to make a gauge transformation? Well, consider a differential four-vector triple product whose ``input vector" is the four-del:
\begin{equation*}
\begin{split}
\partialup \mkern1mu `` \mkern-4mu \times ( \partialup \times \mkern-4mu " \mkern1mu \vv Q ) &= \partialup ( \partialup \cdot \vv Q) - ( \partialup \cdot \partialup) \vv Q \\
&= \partialup ( \partialup \cdot \vv Q) - \Box \vv Q ,
\end{split}
\end{equation*}
a four-vector field analogous to the curl of the curl:
\begin{equation*}
\begin{split}
\del \times ( \del \times \vv c ) &= \del (\del \cdot \vv c) - (\del \cdot \del) \vv c \\
&=  \del (\del \cdot \vv c) - \nabla ^2 \vv c .
\end{split}
\end{equation*}
In this context our new identity looks like:
\begin{equation*}
\begin{split}
\partialup \mkern1mu `` \mkern-4mu \times [ \partialup \times \mkern-4mu " \mkern1mu ( \vv Q + \partialup \psi ) ] &= \partialup [ \partialup \cdot ( \vv Q + \partialup \psi ) ] - \Box ( \vv Q + \partialup \psi ) \\
&= \partialup ( \partialup \cdot \vv Q ) + \partialup ( \Box \psi ) - \Box \vv Q - \Box ( \partialup \psi ) \\
&=  \partialup (\partialup \cdot \vv Q) - \Box \vv Q \\
&= \partialup \mkern1mu `` \mkern-4mu \times ( \partialup \times \mkern-4mu " \mkern1mu \vv Q ) .
\end{split}
\end{equation*}
If we now choose a $\psi$ that satisfies ${\Box \psi = - \partialup \cdot \vv Q}$, then adding $\partialup \psi$ to the $\vv Q$ field causes the whole ``$bac$" term to vanish:
\begin{equation*}
\begin{split}
\partialup \mkern1mu `` \mkern-4mu \times [ \partialup \times \mkern-4mu " \mkern1mu ( \vv Q + \partialup \psi ) ] &= \partialup [ \partialup \cdot ( \vv Q + \partialup \psi ) ] - \Box ( \vv Q + \partialup \psi ) \\
&= \partialup ( \partialup \cdot \vv Q + \Box \psi ) - \Box ( \vv Q + \partialup \psi ) \\
&=  - \Box ( \vv Q + \partialup \psi ) ,
\end{split}
\end{equation*}
leaving just a wave equation for the divergenceless(!) $\vv Q^\prime = \vv Q + \partialup \psi$, a far more tractable expression than the full ``$bac - cab$" apparatus. Performing this particular transformation is called imposing the \textbf{Lorenz gauge} (named for Ludvig Lorenz, not Hendrik Lorentz); other gauges are convenient in their own ways. Note that we still have limited \textbf{gauge freedom} within the Lorenz gauge---for any spacetime scalar field $\chi$ whose d'Alembertian is zero (${\Box \chi = 0}$), we'd get $\Box ( \vv Q^\prime + \partialup \chi ) = \Box \vv Q^\prime + \partialup \Box \chi = \Box \vv Q^\prime$.

That's all the math we'll need. Back to the physics.



\subsection{The Physics (Covariant Electrodynamics)}

\subsubsection{A Prelude on Electric Charge, Particles, and Continua}

There's a tension at the heart of classical electrodynamics: on the one hand, the particle model is extraordinarily convenient---too convenient to dispense with, in practice---and indeed we've long known that \textbf{electric charge} \emph{really is} a property of elementary particles that comes only in discrete amounts; on the other hand, the theory is actually ``about" fields and continua, and it \emph{cannot} incorporate charged particles without becoming internally inconsistent. How should we proceed?

We start by acknowledging that classical electrodynamics isn't our best theory of electromagnetism. \emph{Quantum} electrodynamics is. The classical theory is an approximation that works astonishingly well in its domain of validity, which, by the way, is quite wide. In fact, its domain of validity is so wide that it often gives \emph{correct results} for point charges, despite their being a convenient fiction that it ultimately can't handle in a self-consistent way.

So here is what we'll do. While formulating the theory, we'll usually pretend that charge is a continuous property of matter. We'll go a step further and regard charge \emph{density} as more fundamental than charge itself (or at least consider this perspective). We'll throw in some asides about the reality of charge-discreteness. We'll mention in passing how density can still be defined in the presence of discrete charges. Sometimes, \emph{we'll even use the particle model while developing the theory}, just because it's so convenient, though not without reminding ourselves that we're doing something a little ``wrong," and that strictly speaking it would be better to stick with continua. And we grant that, once the theory has been formulated in terms of continua, it's \emph{perfectly fine} to invoke particles when working problems or dealing with real-world situations. It would be foolish to reject a useful tool just because it's got theoretical issues. \emph{The inconsistency between reality and the classical theory means that the theory itself has issues!}

\subsubsection[(Normalized) Four-Current Density]{(``Normalized") Four-Current Density}\label{sssec:fcd}

Experiment shows that electric charge $q$ is Lorentz-invariant. This allows for the construction of new four-vectors by scaling existing ones by $q$, but that turns out not to be so useful. What we need is a \emph{related} invariant called the \textbf{proper charge density}, defined only for a special class of charge distributions.

First, we define the \textbf{charge density} $\rho$, which is a property of \emph{any} charge distribution. It's just the charge per unit volume $V$. In our \emph{continuous} model,\footnote{To account for charge discreteness, the charge density can be written with the Dirac delta function (we won't bother).} the charge density of an infinitesimal piece of the distribution is:
\begin{equation*}
\rho = \lim\limits_{\substack{\\ \Delta V \to 0}} \, \dfrac{\Delta q}{\Delta V} = \dfrac{\dd q}{\dd V} .
\end{equation*}
If the continuous distribution is also \emph{fluid-like}, by which I mean it has a well-defined \emph{velocity} everywhere, then every infinitesimal piece of it has an (instantaneous) rest frame. (Such a distribution consists of only a single ``stream" of charge at each point, as opposed to ``overlapping" streams with their own trajectories.) For distributions like this, we define the proper charge density $\rho_0$ as the charge density measured in the (infinitesimal) distribution's own (instantaneous) rest frame:
\begin{equation*}
\rho_0 = \lim\limits_{\substack{\\ \Delta V_0 \to 0}} \, \dfrac{\Delta q}{\Delta V_0} = \dfrac{\dd q}{\dd V_0} = \dfrac{1}{\gamma} \, \dfrac{\dd q}{\dd V} = \dfrac{\rho}{\gamma},
\end{equation*}
where $V_0$ is the distribution's \textbf{proper volume}---that is, its volume as measured in its rest frame, an invariant. (The Lorentz factor comes from length contraction along the boost axis: $V_0 = \gamma V$. See Section \ref{sssec:td}.) Since both charge and proper volume are invariant, the proper charge density is, too.

Now we play a little make-believe. Temporarily, we pretend that all the charge in the universe can be modeled as one continuous, fluid-like distribution. If this is so (it isn't), then we can associate a proper charge density with every event in spacetime and regard it as a \emph{scalar field} ${\rho_0 = \rho_0 (\vv R)}$. We might even think of this spacetime scalar field as \emph{fundamental}, and conceive of charge ``merely" as the quantity we get when we take the volume-integral of $\rho = \gamma \rho_0$ over some spatial region in some inertial frame. (Well, you don't have to, but I do think it's helpful in classical field theory to adopt a density-first mindset, or at least to contemplate it.)

This (imaginary) universal fluid-like distribution further possesses a \emph{four-velocity} field $\vv B = \vv B (\vv R)$, since every infinitesimal piece of it has a well-defined timelike world line. It's now natural to scale this four-velocity field by the proper charge density to obtain a new four-vector field $\vv J = \vv J (\vv R)$ that represents the ``flow" of charge through spacetime:
\begin{equation}\label{eq:fcc}
\boxed{\vv J = \rho_0 \vv B = \rho_0 \langle \gamma , \gamma \vvbeta \rangle = \langle \rho, \, \vv j \rangle \quad \text{\footnotesize{ (for a continuous fluid-like distribution)}} } \, .
\end{equation}
This is the (``normalized") \textbf{four-current density} field for a continuous, fluid-like charge distribution. The three-vector $\vv j$ that we seem to have defined as $\rho \vvbeta$ (but read on!) is called the ``normalized" \textbf{three-current density}.\footnote{We're working in \textbf{Heaviside--Lorentz units}, with a slight modification: our $\vv J$ and $\vv j$ correspond to $\vv J / c$ and $\vv j / c$ in the usual schema (we've ``normalized" them).}

That's all really beautiful, and when we can we'll proceed as if it accords with reality. But we know there's a problem: charge \emph{is} discrete! Moreover, it comes in two ``flavors"---positive and negative---and can't be modeled as fluid-like in aggregate, even in the macroscopic world. For example, a simple continuous model of an electrically neutral current-carrying wire involves \emph{two overlapping} charge-streams: a positive one with zero velocity (really the discrete atomic nuclei at rest), and a negative one with a constant non-zero velocity (really the discrete moving electrons). Here, $\rho$ vanishes but $\vv j$ doesn't (we'll have to define $\vv j$ more carefully), and that would give the wire a \emph{spacelike} $\vv J$ (if indeed we can ``keep" $\vv J = \langle \rho, \, \vv j \rangle$), impossible if $\vv J$ is defined as $\rho_0 \vv B$.

Still, we don't have to abandon the continuous fluid-like model altogether. The \textbf{principle of superposition} allows us to make a compromise: we can model charge as \emph{multiple overlapping continuous fluid-like distributions}. How many distributions? As many as it takes. Each ``sub-distribution" gets its own ${\vv J_{\mathrm{sub}} \equiv \rho_{0 \, \mathrm{sub}} \, \vv B_{\mathrm{sub}} = \langle \rho_{\mathrm{sub}}, \, \vv j_{\mathrm{sub}} \equiv \rho_{\mathrm{sub}} \vvbeta_{\mathrm{sub}} \rangle}$ field, which at a given event must be (either zero or) timelike---future- or past-pointing depending on whether the sub-distribution carries positive or negative charge. The sum of \emph{all} $\vv J_{\mathrm{sub}}$'s then gives the ``total" ${\vv J = \langle \rho = \sum \rho_{\mathrm{sub}}, \, \vv j \equiv \sum \vv j_{\mathrm{sub}} \rangle }$ field, which at a given event might be timelike, null, or spacelike. It still describes the spacetime ``flow" of charge, and the universal distribution is still modeled as continuous, but now the distribution is \emph{not} fluid-like in aggregate. So:
\begin{equation}\label{eq:fc}
\boxed{ \vv J \equiv \sum \vv J_{\mathrm{sub}} = \sum \rho_{0 \, \mathrm{sub}} \, \vv B_{\mathrm{sub}} = \langle \rho, \, \vv j \rangle } \, .
\end{equation}
Note that $\vv J = \langle \rho, \, \vv j \rangle$ always works.

After all that, it may surprise you to learn that we can usually just \emph{use} $\vv J = \rho_0 \vv B$ (and $\vv j = \rho \vvbeta$) as we work out electrodynamic theory, substituting one for the other when needed (though I'll try to remember not to). How? The superposition principle! Nearly every equation we encounter that holds for each $\vv J_{\mathrm{sub}}$ individually---maybe \emph{every} such equation---will have a ``general" counterpart that holds for $\vv J$. Classical electrodynamics is a linear theory.

\subsubsection{Continuity Equation; Local Conservation; Flux}\label{sssec:cont}

An important property of the $\vv J$ field is that its four-divergence vanishes:
\begin{equation}\label{eq:con}
\boxed{\partialup \cdot \vv J = 0} \, ,
\end{equation}
or $\partial^{ct} \rho = - \del \cdot \vv j$. This \textbf{continuity equation} says that charge is \textbf{locally conserved}. If you haven't seen this sort of thing before, you may find it easier to grok if we integrate over a finite (and time-independent) spatial volume $V$:
\begin{equation*}
- \int_V \partial^{ct} \rho \, \dd V = \int_V \left( \del \cdot \vv j \right) \dd V ,
\end{equation*}
and use the divergence theorem:
\begin{equation}\label{eq:con2}
- \dfrac{\dd}{\dd (ct)} \int_V \rho \, \dd V = \int_S \left( \vv j \cdot \vv{\hat{n}} \right) \dd S
\end{equation}
(on the right side, $S$ is the bounding surface of $V$, and $\vv{\hat{n}}$ is an outward-pointing unit vector normal to the surface element $\dd S$; on the left side, the volume's time-independence allowed us to move the time-derivative outside of the integral). Since $\rho$ is the charge density, its volume-integral is just the total charge in the region, and the left side of the equation is then the rate at which the region's total charge is decreasing. The right side has the form of a \textbf{flux}, where $\vv j$ gives the per-unit-area rate of charge-transport (the \emph{flow} or \emph{flux density} of charge), and where the whole surface integral represents the rate at which charge is flowing \emph{out} of the region's bounds.\footnote{\label{fn:flux}Some semantics: is this the ``flux of charge" (the transported scalar) or the ``flux of $\vv j$" (the integrated vector)? Both conventions are common, but we'll prefer the ``flux of (the transported scalar)" convention. Note, though, that the surface integral of \emph{any} vector field is often called a flux, even when there's nothing transported (in which case only ``flux of [the integrated vector]" makes sense). Further, some people use the word \emph{flux} for the vector itself (our \emph{flow} or \emph{flux density}) or for its dot product with the unit normal ($\vv j \cdot \vv{\hat{n}}$).}

So Equation \ref{eq:con2} makes the meaning of the continuity equation explicit: the rate at which the total charge in a region decreases equals the rate at which charge is flowing out of it. The \emph{only} way for the total charge of a volume to change is for charge to be transported through its surface. This is what's meant by local conservation. Moreover, \emph{any} locally conserved quantity can be expressed as a continuity equation. The manifestly covariant form of such an equation is always the vanishing four-divergence of the ``spacetime flow" of the locally conserved quantity. The ``spacetime flow" of electric charge is $\vv J$, and $\partialup \cdot \vv J =  0$ (Equation \ref{eq:con}) expresses local charge conservation.

As seasoned special-relativists, it doesn't escape our notice that a ``non-local" conservation law couldn't be a law of nature anyway. The relativity of simultaneity prevents the disappearance of charge ``here" and the appearance of equal charge ``there" from occurring simultaneously for all inertial frames.

We've now taken several important properties of electric charge as experimental givens: its Lorentz-invariance, its additivity (tacitly), and its (local) conservation.\footnote{Local conservation (the continuity equation) doesn't \emph{have} to be taken as a given. It can instead be derived from the electromagnetic field equations.} No other quantity we've studied ticks all three of these boxes. Total energy, for instance, is additive and conserved, but not invariant. And rest energy (mass) is invariant (and trivially ``conserved" by the conservation of total energy), but not additive. So charge is rather special. Another of our experimental givens is that electric charge can be either positive or negative. That's of course a crucial one in practice, but it won't figure much in our treatment.

The four-current density is the electromagnetic \emph{source} field. We know that it's divergence-free, but to complete the picture we need to know how ``information" about $\vv J$ spreads and what physical effects this information has.


\subsubsection{Four-Potential}\label{sssec:fp}

Without motivation, we define the \textbf{four-potential} as any four-vector field $\vv A = \vv A (\vv R)$ whose negative ``double curl" is the four-current density field:
\begin{equation}\label{eq:gfp}
\boxed{
\begin{split}
\vv J &= - \partialup \mkern1mu `` \mkern-4mu \times ( \partialup \times \mkern-4mu " \mkern1mu \vv A ) \\
&= \Box \vv A - \partialup ( \partialup \cdot \vv A)
\end{split}
} \, .
\end{equation}
We say ``any" because, as we discussed in Section \ref{sssec:tp}, gauge transformations taking $\vv A$ to $(\vv A + \partialup \psi)$ for any spacetime scalar field $\psi$ leave the ``double curl" of $\vv A$ unchanged. This non-uniqueness of the four-potential doesn't bother us; we'll see that it's not $\vv A$ itself but rather \emph{derivatives} of $\vv A$ that matter physically, and they're uniquely defined and measurable.\footnote{Better put: \emph{physically observable electromagnetic phenomena} must be gauge-invariant, and in classical electrodynamics gauge-invariance is always ``achieved" by differentiating $\vv A$. In \emph{quantum} electrodynamics, however, gauge-invariance is sometimes ``achieved" by other means, and $\vv A$ may appear \emph{undifferentiated} in the mathematical description of observable phenomena (e.g., the \textbf{Aharonov--Bohm effect}); the potential is fundamental!} Note that Equation \ref{eq:gfp} would be impossible without the continuity equation (Equation \ref{eq:con}) because the four-divergence of a ``double curl" is identically zero: ${\partialup \cdot (\Box \vv A) - \partialup \cdot \partialup (\partialup \cdot \vv A) = \Box (\partialup \cdot \vv A) - \Box (\partialup \cdot \vv A)}$. This hints at a connection between gauge-invariance and charge-conservation.

Something beautiful happens when we impose the Lorenz gauge:
\begin{equation}\label{eq:fp}
\boxed{ \Box \vv A = \vv J } \quad \textrm{\footnotesize{ (in the Lorenz gauge)}} ,
\end{equation}
\begin{equation}\label{eq:lg}
\boxed{ \partialup \cdot \vv A = 0 } \quad \textrm{\footnotesize{ (Lorenz gauge condition)}}
\end{equation}
(again, see Section \ref{sssec:tp}). Equation \ref{eq:fp} is \emph{the} electromagnetic field equation. It's an inhomogeneous wave equation, and it says that a non-zero value of the $\vv J$ field at an event in spacetime generates a commensurate disturbance in the (Lorenz-gauge) $\vv A$ field that propagates outward in space at $\beta = 1$:
\begin{equation*}
\partial^{ct} \partial^{ct} \vv A - \nabla ^2 \vv A = \vv J .
\end{equation*}
The disturbance propagates forever, though it can undergo interference with other disturbances. This is how information about $\vv J$ spreads. The sum of all such disturbances passing through a point in space at a given time determines the ``value" of $\vv A$ at that event (scare quotes because the value isn't uniquely defined---we have ``gauge freedom" even within the Lorenz gauge, remember). A disturbance passing through a region where $\vv J = \textrm{\mbox{\boldmath $\emptyset$}}$ (\textbf{free space}) is called an \textbf{electromagnetic wave}.


\subsubsection{The Lorentz Four-Force; Electric and Magnetic Fields}\label{sssec:lff}

We've introduced the four-potential $\vv A = \vv A (\vv R)$ as any vector field whose negative ``double curl" equals the four-current density $\vv J$, but we haven't yet established why we should care about it. We care about it because of the following empirical result:\footnote{In the next section, we'll actually ``derive" it with the Euler--Lagrange equation.}
\begin{equation}\label{eq:ff}
\boxed{\vv F_{\textrm{L}} = q \vv B \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A)} \, .
\end{equation}
This $\vv F_{\textrm{L}}$ is called the \textbf{Lorentz four-force}.\footnote{Since $\vv J$ has dimensions of $Q / L^3$ (Equation \ref{eq:fc}), $\vv A$ must have $Q / L$ (Equation \ref{eq:gfp}). And since $\vv F$ has $E / L$, we now see that $q$ has $Q = \sqrt{EL}$. (Not so in the SI, by the way.)} It's the physical effect that information about the four-current density's value elsewhere has on a test charge upon reaching it.\footnote{\label{fn:recoil}By ``test charge" we mean that we neglect the influence of the particle's charge on the four-potential, which, in turn, affects the particle! Accounting for it is beyond our scope and not entirely possible in classical electrodynamics anyway. (Remember: the theory is really about \emph{continua}, not \emph{particles}. In fact, instead of starting with $\vv F_{\textrm{L}}$, it's better to regard the field energy, momentum, and stress as fundamental, derive from them and Maxwell's equations the Lorentz four-force \emph{density}, and use a limiting procedure to obtain $\vv F_{\textrm{L}}$ by neglecting higher-order terms. We'll cover all those quantities, but we start with force because it's easier.) We do note that accelerating charges are what generate electromagnetic waves, and that the ``recoil" four-force that a radiating particle is subject to (``beyond" $\vv F_{\textrm{L}}$) is, \emph{to first approximation}, $\vv F = (q^2 / (6 \pi))[\mathring{\vv Z} + (\vv Z \cdot \vv Z) \vv B]$, where $q$ is the particle's charge, $\vv B$ its normalized four-velocity, $\vv Z$ its ``normalized" four-acceleration, and $\mathring{\vv Z}$ its ``normalized" four-jerk. In most practical applications, this additional \textbf{Abraham--Lorentz--Dirac four-force} is negligible.} The quantities on the right side are the particle's charge $q$, the particle's normalized four-velocity $\vv B$, and the ``value" of the four-potential $\vv A$ at the particle's spacetime position (we're differentiating the four-potential, so its non-uniqueness doesn't matter). As we learned in Section \ref{sssec:tp}, this differential four-vector triple product is gauge-invariant and can also be written:
\begin{equation}\label{eq:lff}
\vv F_{\textrm{L}} = q \, \bigl[ \partialup_{\vv A} (\vv B \cdot \vv A) - (\vv B \cdot \partialup) \vv A \bigr] .\end{equation}
By Equations \ref{eq:dt} and \ref{eq:31}, the component form of Equation \ref{eq:ff} is:
\begin{equation}\label{eq:lfc}
\boxed{ \vv F_{\textrm{L}} = q \, \big \langle \vv \gamma \vvbeta \cdot \vv e , \, \gamma \vv e + \gamma \vvbeta \times \vv b \big \rangle } \, ,
\end{equation}
where we've defined $\vv e$ and $\vv b$ in terms of the four-potential $\vv A = \langle A^{ct}, \, \vv a \rangle$:
\begin{equation}\label{eq:eb}
\boxed{ \vv e \equiv - \del A^{ct} - \partial^{ct} \vv a \qquad \qquad \vv b \equiv \del \times \vv a } \, .
\end{equation}
These time-dependent three-vector fields are called the \textbf{electric field} and the \textbf{magnetic field}, respectively. Unlike $\vv A$, they are unique and measurable.

We know from our study of the differential four-vector triple product that $\vv F_{\mathrm{L}}$ must be orthogonal to the input-vector $\vv B$. This means that the Lorentz four-force is codirectional with the particle's resulting ``normalized" four-acceleration ($\vv Z =  \vv F / E_0$, Equation \ref{eq:39}) and causes no change in the particle's rest energy $E_0$ (its mass, if you prefer). Then by Equation \ref{eq:fb}, the spatial three-vector component of $\vv F_{\textrm{L}}$ given in Equation \ref{eq:lfc} is the particle's $\gamma$ times the \textbf{Lorentz three-force} $\vv f_{\textrm{L}}$ exerted on the particle:
\begin{equation}\label{eq:ltf}
\begin{split}
\gamma \vv f_{\textrm{L}} &= q ( \gamma \vv e + \gamma \vvbeta \times \vv b ) \\
\vv f_{\textrm{L}} &= q ( \vv e + \vvbeta \times \vv b ),
\end{split}
\end{equation}
and the temporal component is the particle's $\gamma$ times the \textbf{Lorentz power} $\mathcal{P}_{\textrm{L}}$, which we define as having units of force (the particle's $\dot{E} = \dot{E}_{\mkern.5mu \textrm{k}} = \vv f \cdot \vvbeta$):
\begin{equation}\label{eq:lp}
\begin{split}
\gamma \mathcal{P}_{\textrm{L}} &= q ( \gamma \vvbeta \cdot \vv e ) \\
\mathcal{P}_{\textrm{L}} &= q ( \vvbeta \cdot \vv e ) \, \left[ \, = \vv f_{\textrm{L}} \cdot \vvbeta \right].
\end{split}
\end{equation}
The magnetic field $\vv b$ plays no role in Equation \ref{eq:lp} because the \textbf{magnetic force} $q \vvbeta \times \vv b$ in Equation \ref{eq:ltf} is perpendicular to $\vvbeta$ and therefore only changes the \emph{direction} of the particle's velocity (and momentum), not its magnitude. In other words, the magnetic field does no work.\footnote{In \emph{quantum} physics---needed to fully explain even some everyday phenomena, like permanent magnets---the magnetic field arguably ``does work" because of particle \textbf{spin}.} The \textbf{electric force} $q \vv e$, on the other hand, is parallel to $\vv e$ regardless of the direction of $\vvbeta$, and generally changes both the direction and magnitude of the particle's velocity.

As four-vectors, $\vv J$, $\vv A$, and $\vv F_{\mathrm{L}}$ have components that obey the Lorentz transformation. As three-vectors, $\vv e$ and $\vv b$ do not, and equations involving them may be covariant but not manifestly so. Their components are \emph{built} from the components of four-vectors, however ($\partialup$ and $\vv A$), and transform accordingly. Here are the Cartesian components of $\vv e$ and $\vv b$ in terms of those four-vector components (using Equations \ref{eq:fn} and \ref{eq:eb}):
\begin{equation}\label{eq:ebc}
\begin{aligned}
e_x = \partial^x A^{ct} - \partial^{ct} A^x \qquad \qquad b_x = \partial^z A^y - \partial^y A^z \\
e_y = \partial^y A^{ct} - \partial^{ct} A^y \qquad \qquad b_y = \partial^x A^z - \partial^z A^x \\
e_z = \partial^z A^{ct} - \partial^{ct} A^z \qquad \qquad b_z = \partial^y A^x - \partial^x A^y 
\end{aligned}
\end{equation}
(remember, $\partial^{ct}$ is the positive $ct$-partial, but the other components of $\partialup$ are \emph{negative} partials). Under a standard-configuration boost with relative rapidity $\phi$ between the frames, $e^\prime_x$ is:
\begin{equation*}
\begin{split}
e^\prime_x &= \partial^{\mkern.5mu \prime \mkern1mu x} A^{\prime \mkern1mu ct} - \partial^{\mkern.5mu \prime \mkern1mu ct} A^{\prime \mkern1mu x} \\[2pt]
&= (\cosh{\phi} \; \partial^x  - \sinh{\phi} \; \partial^{ct} )(A^{ct} \cosh{\phi}  - A^x \sinh{\phi} ) \\
& \quad - (\cosh{\phi} \; \partial^{ct} -  \sinh{\phi} \; \partial^x )(A^x \cosh{\phi} - A^{ct} \sinh{\phi} ) \\[2pt]
&= \partial^x A^{ct} - \partial^{ct} A^x = e_x
\end{split}
\end{equation*}
(by Equation \ref{eq:50}). Let's do one more component:
\begin{equation*}
\begin{split}
b^\prime_y &= \partial^{\mkern.5mu \prime \mkern1mu x} A^{\prime \mkern1mu z} - \partial^{\mkern.5mu \prime \mkern1mu z} A^{\prime \mkern1mu x} \\[2pt]
&= (\cosh{\phi} \; \partial^x  - \sinh{\phi} \; \partial^{ct} ) A^z - \partial^z (A^x \cosh{\phi} - A^{ct} \sinh{\phi} ) \\[2pt]
&= \cosh{\phi} \, (\partial^x A^z - \partial^z A^x) + \sinh{\phi} \, (\partial^z A^{ct} - \partial^{ct} A^z) \\[2pt]
&= b_y \cosh{\phi} + e_z \sinh{\phi} .
\end{split}
\end{equation*}
Similar calculations give:
\begin{equation}\label{eq:ebt}
\begin{aligned}
&e^\prime_x = e_x  \qquad \qquad& &b^\prime_x = b_x  \\
&e^\prime_y = e_y \cosh{\phi} - b_z \sinh{\phi}  \qquad& &b^\prime_y = b_y \cosh{\phi} + e_z \sinh{\phi} \\
&e^\prime_z = e_z \cosh{\phi} + b_y \sinh{\phi}  \qquad& &b^\prime_z = b_z \cosh{\phi} - e_y \sinh{\phi} .
\end{aligned}
\end{equation}
For the inverse transformation, swap primed and unprimed components and switch the sign of each $\sinh{\phi}$. From Equations \ref{eq:ebt} and \ref{eq:50}, it follows (after some algebra) that  ${e^{\prime \, 2} - b^{\prime \, 2} = e^2 - b^2}$ and that $\vv e^\prime \cdot \vv b^\prime = \vv e \cdot \vv b$.\footnote{\label{fn:ps}As the curl of an ordinary vector, $\vv b$ is a \emph{pseudovector}, which means that it gains an extra sign-flip under ``orientation-reversing" coordinate transformations. The Lorentz-invariant $\vv e \cdot \vv b$ inherits this property (it's a \emph{pseudoscalar}).} These are the fundamental invariants we can construct from $\vv e$ and $\vv b$. An immediate consequence is the invariance of the special cases $\vv e \perp \vv b$ and $e = b$; i.e., if the electric and magnetic fields are perpendicular in one frame ($\vv e \cdot \vv b = 0$), then they're perpendicular in all frames, and if their magnitudes are equal in one frame ($e^2 - b^2 = 0$), then their magnitudes are equal in all frames.

Another way of writing Equations \ref{eq:ebt} is in terms of component vectors parallel and perpendicular to the boost direction $\hatphi = \langle 1, 0, 0 \rangle$ (this will be similar to how we arrived at Equations \ref{eq:lt3}). Using ${\vv e_\perp = \langle 0, e_y, e_z \rangle}$ and $\hatphi \times \vv b = \langle 0, -b_z, b_y \rangle$, and also $\vv b_\perp = \langle 0, b_y, b_z \rangle$ and $\hatphi \times \vv e = \langle 0, -e_z, e_y \rangle$:
\begin{equation*}
\begin{aligned}
& \vv e^\prime_\parallel = \vv e_\parallel  \qquad \qquad& & \vv b^\prime_\parallel = \vv b_\parallel  \\
& \vv e^\prime_\perp =  \cosh{\phi} \; \vv e_\perp  +  \sinh{\phi} \; ( \hatphi \times \vv b )   \qquad& & \vv b^\prime_\perp = \cosh{\phi} \; \vv b_\perp  - \sinh{\phi} \; ( \hatphi \times \vv e ) ,
\end{aligned}
\end{equation*}
which holds for a boost in an \emph{arbitrary} direction (not just $\langle 1, 0, 0 \rangle$). With $\vv e_\parallel = (\hatphi \cdot \vv e) \hatphi$ (vector-projection formula) and $\vv e_\perp = \vv e - \vv e_\parallel$, we can condense the transformation of $\vv e$ to a single equation that doesn't mention component vectors at all:
\begin{equation*}
\begin{split}
\vv e^\prime &= \vv e^\prime_\perp  + \vv e^\prime_\parallel \\
&= \cosh{\phi} \; \vv e_\perp  +  \sinh{\phi} \; ( \hatphi \times \vv b ) + \vv e_\parallel \\
&= \cosh{\phi} \; \big[ \vv e - (\hatphi \cdot \vv e) \hatphi \big] + \sinh{\phi} \; (\hatphi \times \vv b) + (\hatphi \cdot \vv e) \hatphi .
\end{split}
\end{equation*}
Do the same for $\vv b$, and simplify both expressions. Using a ``half-angle" identity to substitute $2 \sinh^2{(\phi/2)}$ for $\cosh{\phi} - 1$ (personal preference), that's:
\begin{equation}\label{eq:ebr}
\boxed{
\begin{aligned}
\vv e^\prime &= \cosh{\phi} \; \vv e + \sinh{\phi} \; (\hatphi \times \vv b) - 2 \sinh^2 \frac{\phi}{2} \; (\hatphi \cdot \vv e) \hatphi \\[4pt]
\vv b^\prime &= \cosh{\phi} \; \vv b - \sinh{\phi} \; (\hatphi \times \vv e) - 2 \sinh^2 \frac{\phi}{2} \; (\hatphi \cdot \vv b) \hatphi 
\end{aligned}
} \, .
\end{equation}
Equivalently, though maybe less elegantly:\footnote{Equations \ref{eq:ebr} are rather analogous to the \href{https://en.wikipedia.org/wiki/Rodrigues\%27_rotation_formula}{Rodrigues rotation formula} (Wikipedia).}
\begin{equation*}
\vv e^\prime = \gamma \left( \vv e + \vvbeta \times \vv b \right) - \dfrac{\gamma^2 (\vvbeta \cdot \vv e) \vvbeta}{\gamma + 1} \qquad \quad \vv b^\prime = \gamma \left( \vv b - \vvbeta \times \vv e \right) - \dfrac{\gamma^2 (\vvbeta \cdot \vv b) \vvbeta}{\gamma + 1},
\end{equation*}
where $\gamma = \cosh{\phi}$ and $\beta = \tanh{\phi}$ are boost parameters ($\hatbeta = \vvbeta / \beta = \hatphi$).\footnote{Naturally, transformations of \emph{four-vector components} under a Lorentz boost in an arbitrary direction can likewise be expressed in terms of the boost-rapidity. From Equations \ref{eq:lt3} (for the components of the four-position $\vv R$), it's not difficult to show that for some four-vector $\vv Q = \langle Q^{ct}, \vv q \rangle$, that's:
\begin{equation*}
Q^{\prime  \mkern1.5mu ct} = \cosh \phi \; Q^{ct} - \sinh \phi \; ( \hatphi \cdot \vv q ) \qquad \quad
\vv q^{\prime} = \vv q + \big( 2 \sinh^2 \frac{\phi}{2} \; ( \hatphi \cdot \vv q ) - \sinh \phi \; Q^{ct} \big) \hatphi .
\end{equation*}}

Finally, it's useful to find a relationship between $\vv J = \langle \rho, \, \vv j \rangle$ and the $\vv e$ and $\vv b$ fields. Let's work in the Lorenz gauge, with $\vv J= \Box \vv A = \langle \Box A^{ct}, \Box \vv a \rangle$. By the Lorenz condition ${\partialup \cdot \vv A = 0}$ (Equation \ref{eq:lg}), or $\partial^{ct} A^{ct} = - \del \cdot \vv a$:
\begin{equation}\label{eq:ace}
\begin{split}
\rho &= \Box A^{ct} \\
&= \partial^{ct} (\partial^{ct} A^{ct}) - \nabla ^2 A^{ct} \\
&= \partial^{ct} (- \del \cdot \vv a) - \del \cdot \del A^{ct} \\
&= \del \cdot (- \partial^{ct} \vv a - \del A^{ct}) \\
\rho &= \del \cdot \vv e ,
\end{split}
\end{equation}
by Equations \ref{eq:eb} (we can swap $\partial^{ct}$ and $\del \cdot$ in the penultimate step because mixed partials commute). Then using ${\nabla ^2 \vv a = \del(\del \cdot \vv a) - \del \times (\del \times \vv a)}$ (the ``$bac - cab$" identity for the vector Laplacian), we also have:
\begin{equation}\label{eq:abe}
\begin{split}
\vv j &= \Box \vv a \\
&= \partial^{ct} (\partial^{ct} \vv a) - \nabla ^2 \vv a \\
&= \del \times (\del \times \vv a) + \del(\partial^{ct} A^{ct}) + \partial^{ct} (\partial^{ct} \vv a) \\
\vv j &=  \del \times \vv b - \partial^{ct} \vv e ,
\end{split}
\end{equation}
where we've again invoked the Lorenz condition $\partial^{ct} A^{ct} = - \del \cdot \vv a$ and reversed the order of mixed partials. Equations \ref{eq:ace} and \ref{eq:abe} are the three-vector equivalent of the more compact Equation \ref{eq:fp}. They're the ``inhomogeneous" pair of \textbf{Maxwell's equations}.\footnote{If we weren't working in the Lorenz gauge, we'd instead want to find a relationship between $\vv J = \Box \vv A - \partialup (\partialup \cdot \vv A)$ and the $\vv e$ and $\vv b$ fields (see Equation \ref{eq:gfp}), and of course we couldn't invoke the Lorenz condition. But we'd still get Equations \ref{eq:me}! For example: $\rho = \Box A^{ct} - \partial^{ct}(\partialup \cdot \vv A) = \partial^{ct} (\partial^{ct} A^{ct}) - \del \cdot \del A^{ct} - \partial^{ct} (\partial^{ct} A^{ct} + \del \cdot \vv a) = \del \cdot \vv e$.} Here is the full set of Maxwell's equations:
\begin{equation}\label{eq:me}
\boxed{
\begin{aligned}
\del \cdot \vv e &= \rho \\
\del \times \vv b - \partial^{ct} \vv e &= \vv j \\ 
\del \cdot \vv b &= 0 \\
- \del \times \vv e - \partial^{ct} \vv b &= \vv 0
\end{aligned}
} \, .
\end{equation}
To obtain the ``homogeneous" pair, we've used Equations \ref{eq:eb} and two vector calculus identities: the divergence of a curl is $0$, and the curl of a gradient is $\vv 0$. Wondering about the negative signs in the last equation? Written this way, each pair of equations constitutes a four-vector, the first equivalent to $\Box \vv A - \partialup ( \partialup \cdot \vv A) = \vv J$ (or $\Box \vv A = \vv J$ in the Lorenz gauge), and the second equal to the zero four-vector $\textrm{\mbox{\boldmath $\emptyset$}} = \langle 0, \vv 0 \rangle$ (show this!). The left sides are suggestive; might each four-vector pair somehow be expressed with $\partialup$ appearing as a \emph{first} derivative of something? We'll come back to this in Section \ref{ssec:mfd}.


\subsubsection{The Lorentz Four-Force via the Euler--Lagrange Equation}\label{sssec:ele}

(In this section, the reader is assumed to have at least a passing familiarity with analytical mechanics. We'll use the Euler--Lagrange equation but we won't derive it.)

Earlier we called the Lorentz four-force (Equation \ref{eq:ff}) an ``empirical result." Alternatively, we can start with a suitable Lorentz-invariant Lagrangian and use the four-vector version of the Euler--Lagrange equation to \emph{derive} the Lorentz four-force. This is typically done with covectors and index notation, which we haven't covered, but our present toolkit will suffice if we define a new vector operator whose component-partials are with respect to the \emph{four-velocity} components. With some notational poetic license:
\begin{equation*}
\mathring{\partialup} \equiv \left \langle \dfrac{\partial}{\partial (c \mathring{t})} , \,  - \dfrac{\partial}{\partial \mathring{x}} , \, - \dfrac{\partial}{\partial \mathring{y}} , \, - \dfrac{\partial}{\partial \mathring{z}} \right \rangle = \left \langle \dfrac{\partial}{\partial B^{ct}} , \,  - \dfrac{\partial}{\partial B^x} , \, - \dfrac{\partial}{\partial B^y} , \, - \dfrac{\partial}{\partial B^z} \right \rangle
\end{equation*}
(the minus signs make it a four-vector, as was the case with $\partialup$---otherwise the components would transform \emph{inverse}-Lorentzianly, recall). With $L$ as the Lagrangian, the four-vector version of the Euler--Lagrange equation is:
\begin{equation}\label{eq:ele}
\dfrac{\dd}{\dd (c t_0)} \, ( \mathring{\partialup} L ) = \partialup L .
\end{equation}

Our Lagrangian for a particle in an electromagnetic field should have two manifestly covariant terms: one for the particle as if it were inertial (absent the field and any other forces) and one for its coupling with the field. Now, we haven't yet dealt with the principle of stationary action directly, but here and there we've had occasion to discuss the elapsed proper time along a particle's trajectory, and we even mentioned in passing that the \emph{inertial} journey between a pair of timelike-separated events is the one that maximizes the proper time elapsed en route (see Footnote \ref{fn:pma}). We haven't \emph{demonstrated} this principle of maximal aging, however, so let's quickly do that. From our discussion in Section \ref{sssec:ds}, we know that the elapsed proper time (for any timelike journey) is $c \mkern.5mu \Delta t_0 = \int c \, \dd t_0$, evaluated along the world line. By $c\mathring{t} = \gamma$, that's equivalently $c \mkern.5mu \Delta t_0 = \int \gamma^{-1} c \, \dd t = \int (1 - \beta^2)^{1/2} c \, \dd t $, which gives the elapsed proper time in terms of the \emph{coordinate} time for some inertial frame. It's clear that $c \mkern.5mu \Delta t_0$ will be maximized if in this frame the traveler's velocity is zero throughout the journey, and this can only be so if the traveler's route is inertial.\footnote{Of course, since the elapsed proper time is invariant, a maximal $c \mkern.5mu \Delta t_0$ doesn't \emph{require} the traveler's velocity to be always zero---just constant.}

Because of the principle of maximal aging, the Lagrangian term for the inertial particle ($L_{\textrm{inertial}}$) should be a \emph{constant} along the path taken, so that its action integral $\int \mkern-4mu L_{\textrm{inertial}} \, c \, \dd t_0$ for that path is proportional to the particle's elapsed proper time.\footnote{\label{fn:covpt}If we were actually using the calculus of variations to \emph{find} the path of stationary action, we'd have to integrate over some other variable whose values at the integration limits are \emph{fixed}, regardless of the path taken between them (i.e., not the path-dependent proper time). After finding the path of stationary action, it's fine to equate the integration variable with the particle's proper time \emph{for that path}, and that's all we've done.} Something like $L_{\textrm{inertial}} = E_0 (\vv B \cdot \vv B)$ should do the trick (we don't want to reduce $\vv B \cdot \vv B$ to $B^2 = 1$ at this stage---if we do, we won't get a four-acceleration when we differentiate, and then how will we recover the Lorentz four-force?\footnote{\label{fn:bb1}More to the point, we \emph{can't} reduce it to $B^2 = 1$ at this stage. Really, $\vv B = \mathring{\vv R}$ here is the derivative of the \emph{any-path particle's} four-position with respect to the \emph{actual-path particle's} proper time (see Footnote \ref{fn:covpt}). So only for the true path does $\vv B \cdot \vv B = 1$.}). For the coupling term, perhaps the simplest sensible invariant we can make from the field and the particle is $q (\vv B \cdot \vv A)$. Let's try it. Our candidate Lagrangian is then:
\begin{equation*}
L = \dfrac{1}{2} \, E_0 (\vv B \cdot \vv B) + q (\vv B \cdot \vv A)
\end{equation*}
(we've tossed in the $1/2$ to cancel the $2$ that will inevitably arise when we differentiate). Here, $\vv B = \vv B (ct_0)$ is the particle's four-velocity, $E_0$ its rest energy, $q$ its charge, and $\vv A = \vv A (\vv R (ct_0))$ the four-potential at the particle's four-position $\vv R = \vv R (ct_0)$. If we've chosen well, then plugging our Lagrangian into the four-vector version of the Euler--Lagrange equation (Equation \ref{eq:ele}) and carrying out the derivatives should get us the Lorentz four-force.

First we'll handle the ``canonical four-momentum" $\vv P _\textrm{c} \equiv \mathring{\partialup} L$:
\begin{equation*}
\vv P _\textrm{c} =  \dfrac{1}{2} \, E_0 \, \mathring{\partialup} (\vv B \cdot \vv B) + q \, \mathring{\partialup} (\vv B \cdot \vv A) .
\end{equation*}
We can express $\mathring{\partialup} (\vv B \cdot \vv B)$ and $\mathring{\partialup} (\vv B \cdot \vv A)$ in component form to make sense of them:
\begin{equation*}
\begin{split}
&\mathring{\partialup} (\vv B \cdot \vv B) \\[2pt]
&= \left \langle \dfrac{\partial}{\partial B^{ct}} , \,  - \dfrac{\partial}{\partial B^x} , \, - \dfrac{\partial}{\partial B^y} , \, - \dfrac{\partial}{\partial B^z} \right \rangle \left( B^{ct \, 2} - B^{x \, 2} - B^{y \, 2} - B^{z \, 2} \right) \\[3pt]
&= \left \langle \dfrac{\partial}{\partial B^{ct}} \left( B^{ct \, 2} \right) , \, - \dfrac{\partial}{\partial B^x} \left( - B^{x \, 2} \right) , \, - \dfrac{\partial}{\partial B^y} \left( - B^{y \, 2} \right)  , \, - \dfrac{\partial}{\partial B^z} \left( - B^{z \, 2} \right)  \right \rangle \\[2pt]
&= 2 \vv B
\end{split}
\end{equation*}
(there's that $2$); likewise:
\begin{equation*}
\begin{split}
&\mathring{\partialup} (\vv B \cdot \vv A) \\[2pt]
&= \left \langle \dfrac{\partial}{\partial B^{ct}} , \,  - \dfrac{\partial}{\partial B^x} , \, - \dfrac{\partial}{\partial B^y} , \, - \dfrac{\partial}{\partial B^z} \right \rangle \left( B^{ct} A^{ct} - B^x A^x - B^y A^y - B^z A^z \right) \\[3pt]
&= \left \langle A^{ct} \, \dfrac{\partial B^{ct}}{\partial B^{ct}} , \,  (- A^x) \left( - \dfrac{\partial B^x}{\partial B^x} \right) , \, (-A^y) \left( - \dfrac{\partial B^y}{\partial B^y} \right) , \, (-A^z) \left( - \dfrac{\partial B^z}{\partial B^z} \right) \right \rangle \\[2pt]
&= \vv A 
\end{split}
\end{equation*}
($\vv A = \vv A(\vv R(ct_0))$ has no $\vv B$-dependence). So canonical four-momentum is:
\begin{equation*}
\vv P _\textrm{c} =  E_0 \vv B + q \vv A
\end{equation*}
(the sum of $q \vv A$ and the ``mechanical" four-momentum $\vv P = E_0 \vv B$), and the left side of our Euler--Lagrange equation (Equation \ref{eq:ele}) is its $ct_0$-derivative:
\begin{equation*}
\mathring{\vv P}_\textrm{c}  =  E_0 \mathring{\vv B} + q \mathring{\vv A} .
\end{equation*}
The right side of the Euler--Lagrange equation is easy:
\begin{equation*}
\begin{split}
\partialup L &= \dfrac{1}{2} \, E_0 \, \partialup (\vv B \cdot \vv B) + q \, \partialup (\vv B \cdot \vv A) \\[2pt]
&= q \, \partialup_{\vv A} (\vv B \cdot \vv A),
\end{split}
\end{equation*}
since $\vv B = \vv B(ct_0)$ doesn't depend on $\vv R$.\footnote{For a \emph{physical trajectory} one can certainly do $\vv B = \vv B (\vv R (ct_0))$, but the Lagrangian's ``coordinate" and ``velocity" parameters are always treated as \emph{independent} variables.} Putting it all together:
\begin{equation}\label{eq:emeom}
\begin{split}
\mathring{\vv P}_\textrm{c} &= \partialup L \\[2pt]
E_0 \mathring{\vv B} + q \mathring{\vv A} &= q \, \partialup_{\vv A} (\vv B \cdot \vv A) \\[3pt]
\vv F &= q \, \partialup_{\vv A} (\vv B \cdot \vv A) - q \mathring{\vv A} ,
\end{split}
\end{equation}
where we've used $\vv F = E_0 \mathring{\vv B}$.

One more step. Consider the following operator, a directional derivative that appears in the Lorentz four-force (Equation \ref{eq:lff}):
\begin{equation*}
\begin{split}
\vv B \cdot \partialup &= \left \langle c \mathring{t}, \mathring{\vv r} \right \rangle \cdot \left \langle \partial^{ct}, - \del \right \rangle \\[4pt]
&= \dfrac{\dd (ct)}{\dd (ct_0)} \, \dfrac{\partial}{\partial (ct)} + \dfrac{\dd x}{\dd (ct_0)} \, \dfrac{\partial}{\partial x} +  \dfrac{\dd y}{\dd (ct_0)} \, \dfrac{\partial}{\partial y} + \dfrac{\dd z}{\dd (ct_0)} \, \dfrac{\partial}{\partial z} .
\end{split}
\end{equation*}
By the multivariable chain rule, this operator is equivalently $\dd / \dd (ct_0)$, the total derivative with respect to proper time.\footnote{The relation $\dd / \dd (ct_0) = \vv B \cdot \partialup$ often arises in general relativity. It's analogous to the Newtonian ``convective derivative," ${\dd / \dd t = \partial / \partial t + \vv v \cdot \del}$, though that term is sometimes reserved for when the operand is a property of a \emph{continuum} (where $\vv v$ is the continuum's velocity field), as opposed to a ``background" field's value along a particle's world line.} It acts on functions of ${\vv R (ct_0) = \langle ct (ct_0), \, x (ct_0), \, y (ct_0), \, z (ct_0) \rangle}$. Our four-potential fits the bill, so $\mathring{\vv A} = (\vv B \cdot \partialup) \vv A $, and Equation \ref{eq:emeom} becomes:
\begin{equation*}
\begin{split}
\vv F &= q \, \bigl[ \partialup_{\vv A} (\vv B \cdot \vv A) - (\vv B \cdot \partialup) \vv A \bigr] = q \vv B \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A) ,
\end{split}
\end{equation*}
which is indeed the Lorentz four-force (Equations \ref{eq:ff} and \ref{eq:lff}).

The Hamiltonian corresponding to our Lagrangian is given by $ \vv B \cdot \vv P _\mathrm{c} - L $, and its $ct_0$-derivative should vanish.\footnote{It should vanish for the true path, anyway, for which $\vv B \cdot \vv B = 1$ (see Footnotes \ref{fn:covpt} and \ref{fn:bb1}).} That's:
\begin{equation*}
\begin{split}
H &= \vv B \cdot \vv P_\mathrm{c} - L \\
&= \vv B \cdot \left( E_0 \vv B + q \vv A \right) - \dfrac{1}{2} \, E_0 (\vv B \cdot \vv B) - q (\vv B \cdot \vv A) \\
&= \dfrac{1}{2} \, E_0 (\vv B \cdot \vv B) ,
\end{split}
\end{equation*}
which checks out, though it should really be expressed in terms of the canonical momentum (using $\vv P_\mathrm{c} = E_0 \vv B + q \vv A $):
\begin{equation*}
H = \dfrac{\left( \vv P_\mathrm{c} - q \vv A \right) ^2}{2 E_0} .
\end{equation*}

\subsection{The Unfinished Business: Energy and Light}

``All" of classical electromagnetism is contained in Equations \ref{eq:fp} and \ref{eq:ff}. Now let's apply it. Remember, our goal is to obtain Equation \ref{eq:8} without using the Planck--Einstein relation. To do this, we'll first coax some general information about electromagnetic energy (and momentum) from the Lorentz four-force. Then we'll briefly discuss light---i.e., solutions to Equation \ref{eq:fp} when $\vv J = \textrm{\mbox{\boldmath $\emptyset$}}$---and try to figure out how its energy transforms under a Lorentz boost.

\subsubsection{The Lorentz Four-Force Density; Poynting's Theorem}\label{sssec:lffd}

If we differentiate the Lorentz four-force (Equation \ref{eq:ff}) with respect to the proper volume ${V_0 = \gamma V}$ of a small piece of a continuous fluid-like charge distribution, we get a four-vector called the \textbf{Lorentz four-force density}:\footnote{A more sophisticated continuum-based approach would reverse this: start with the force \emph{density}, and obtain the Lorentz force for a particle from a limiting procedure.}
\begin{equation*}
\bm{\mathfrak{F}} = \dfrac{\dd \vv F_{\mathrm{L}}}{\dd V_0} = \dfrac{\dd}{\dd V_0} \bigl[ q \vv B \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A) \bigr] .
\end{equation*}
Only $q$ has volume-dependence here, so that's:
\begin{equation*}
\bm{\mathfrak{F}} = \rho_0 \vv B \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A) .
\end{equation*}
Now apply Equation \ref{eq:fcc} ($\rho_0 \vv B = \vv J$):
\begin{equation}\label{eq:lffd}
\boxed{ \bm{\mathfrak{F}} \equiv \vv J \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A) } \, ,
\end{equation}
or
\begin{equation*}
\bm{\mathfrak{F}} = \partialup_{\vv A} (\vv J \cdot \vv A) - ( \vv J \cdot \partialup) \vv A .
\end{equation*}

We've saved the ``$\equiv$" sign for Equation \ref{eq:lffd} because while $\dd \vv F_{\mathrm{L}} / \dd V_0$ is only meaningful for fluid-like distributions (for which a rest frame and consequently a $\rho_0$ and a $V_0$ can be defined), Equation \ref{eq:lffd} is general. This is related to the ``sub-distribution" subtlety discussed in Section \ref{sssec:fcd}. Each fluid-like sub-distribution gets its own ``partial" four-force density, ${\vv J_{\mathrm{sub}} \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A)}$. But then: ${ \sum ( \vv J_{\mathrm{sub}} \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A) ) = (\sum \vv J_{\mathrm{sub}}) \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A) = \vv J \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A)}$ for the ``total" four-force density, using Equation \ref{eq:fc} ($\vv J = \sum \vv J_{\mathrm{sub}}$).

We can associate a Lorentz four-force density with every event in spacetime, so it's really a vector \emph{field}, $\bm{\mathfrak{F}} = \bm{\mathfrak{F}} (\vv R)$. For each individual fluid-like sub-distribution, its value represents the per-unit-proper-volume rate at which the four-potential $\vv A$ transfers four-momentum to the charged matter. In free space (where $\vv J = \textrm{\mbox{\boldmath $\emptyset$}}$), the Lorentz four-force density vanishes, as it must if four-momentum is to be conserved. That doesn't mean there's no four-momentum in the $\vv A$ field in free space---just that whatever four-momentum is there doesn't magically disappear.

Recall that the temporal and spatial components of the Lorentz four-force are $\gamma \mathcal{P}_{\mathrm{L}}$ and $\gamma \vv f_{\textrm{L}}$, where $\gamma$ is a test particle's Lorentz factor, and $\mathcal{P}_{\mathrm{L}}$ and $\vv f_{\textrm{L}}$ are the power and three-force exerted on the particle as a result of its interaction with the $\vv e$ and $\vv b$ fields (see Equations \ref{eq:ltf} and \ref{eq:lp}). The component form of the Lorentz four-force \emph{density} for a fluid-like sub-distribution is then:
\begin{equation*}
\dfrac{\dd \vv F_{\mathrm{L}}}{\dd V_0} = \left \langle \gamma \, \dfrac{\dd \mathcal{P}_{\textrm{L}} }{\dd V_0}, \, \gamma \, \dfrac{\dd \vv f_{\textrm{L}}}{\dd V_0} \right \rangle,
\end{equation*}
where $\gamma$ is the Lorentz factor of the infinitesimal piece of the distribution at the event in question (and has no volume-dependence). By ``volume contraction" ($\dd V_0 = \gamma \mkern1mu \dd V$), we've got:
\begin{equation}\label{eq:lffdc}
\bm{\mathfrak{F}} = \left \langle \dfrac{\dd \mathcal{P}_{\mathrm{L}}}{\dd V}, \, \dfrac{\dd \vv f_{\mathrm{L}}}{\dd V} \right \rangle ,
\end{equation}
which holds generally (not just for fluid-like distributions). So in a given frame, the temporal component is the per-unit-$V$ $ct$-rate of \emph{energy} transfer from the fields to the distribution, and the spatial component is the per-unit-$V$ $ct$-rate of \emph{three-momentum} transfer.

With this understanding, let's now write the component form of the Lorentz four-force density (Equation \ref{eq:lffd}) in terms of the $\vv e$ and $\vv b$ fields (using Equations \ref{eq:dt}, \ref{eq:fc}, and \ref{eq:eb}):
\begin{equation*}
\bm{\mathfrak{F}} = \left \langle \, \vv j \cdot \vv e, \, \rho \mkern1mu \vv e + \vv j \times \vv b \right \rangle ,
\end{equation*}
and with substitutions from Equations \ref{eq:ace} and \ref{eq:abe}:
\begin{equation}\label{eq:lffdc2}
\bm{\mathfrak{F}} = \Big \langle \left( \del \times \vv b - \partial^{ct} \vv e \right) \cdot \vv e , \, \left( \del \cdot \vv e \right) \vv e + \left( \del \times \vv b - \partial^{ct} \vv e \right) \times \vv b \Big \rangle .
\end{equation}
Doesn't look promising! Yet, these components can be simplified in a way that yields succinct statements of the (local) conservation of energy and three-momentum of the $\vv e$ and $\vv b$ fields. (And in Section \ref{sec:dy} we'll learn how to read Equation \ref{eq:lffd} as a statement of \emph{four-momentum} conservation.) Let's go ahead and spruce up that temporal component. Start with the identity $\del \cdot (\vv e \times \vv b) = \vv b \cdot (\del \times \vv e) - \vv e \cdot (\del \times \vv b)$:
\begin{equation*}
\vv e \cdot \left( \del \times \vv b - \partial^{ct} \vv e \right) = \vv b \cdot \left( \del \times \vv e \right) - \del \cdot \left( \vv e \times \vv b \right) - \vv e \cdot \partial^{ct} \vv e .
\end{equation*}
Then use $\del \times \vv e = - \partial^{ct} \vv b$ (Equations \ref{eq:me}), and bring in Equation \ref{eq:lffdc}:
\begin{equation*}
\begin{split}
\dfrac{\dd \mathcal{P}_{\textrm{L}}}{\dd V} &= - \del \cdot \left( \vv e \times \vv b \right) - \vv e \cdot \partial^{ct} \vv e - \vv b \cdot \partial^{ct} \vv b  \\
&= - \del \cdot \left( \vv e \times \vv b \right) - \frac{1}{2} \, \partial^{ct} \left( \vv e \cdot \vv e + \vv b \cdot \vv b \right) \\[2pt]
&= - \del \cdot \vv s - \partial^{ct} u ,
\end{split}
\end{equation*}
where $\vv s \equiv \vv e \times \vv b$ and $u \equiv (e^2 + b^2)/2$. Reordering the terms, we have:
\begin{equation}\label{eq:py}
\boxed{ - \partial^{ct} u =  \del \cdot \vv s + \dfrac{\dd \mathcal{P}_{\textrm{L}}}{\dd V} } \, .
\end{equation}

Equation \ref{eq:py} is \textbf{Poynting's theorem}. That it's the energy-conservation statement we were looking for is more transparent if we integrate it over a finite volume $V$, much like we did for the continuity equation in Section \ref{sssec:cont}:
\begin{equation*}
- \int_V \partial^{ct} u \, \dd V = \int_V \left(  \del \cdot \vv s + \dfrac{\dd \mathcal{P}_{\textrm{L}}}{\dd V} \right) \dd V ,
\end{equation*}
and again use the divergence theorem:
\begin{equation*}
- \, \dfrac{\dd}{\dd (ct)} \int_V u \, \dd V = \int_S \left( \vv s \cdot \vv{\hat{n}} \right) \dd S + \int_V \dfrac{\dd \mathcal{P}_{\textrm{L}}}{\dd V} \, \dd V
\end{equation*}
($S$ is the bounding surface of $V$, and $\vv{\hat{n}}$ is an outward-pointing unit vector normal to the surface element $\dd S$). Remembering that $\mathcal{P}_{\textrm{L}}$ is the ``Lorentz power" ($q \vvbeta \cdot \vv e$), let's write the last term explicitly as the total rate of energy-transfer from the fields (just $\vv e$, actually) to charged matter in the volume:
\begin{equation*}
- \, \dfrac{\dd}{\dd (ct)} \int_V u \, \dd V = \int_S \left( \vv s \cdot \vv{\hat{n}} \right) \dd S + \dot{E}_{\mkern1mu \textrm{matter (via fields)}} .
\end{equation*}
Evidently, $u$ is the volume-density of something, and the surface integral is the flux of that something (that is, the rate at which that something flows out of the volume's bounds). The last term clues us in as to what that something is: the energy stored in the fields. Poynting's theorem says that the rate at which a volume's electromagnetic energy decreases equals the rate at which electromagnetic energy is flowing out of the region through its surface plus the rate at which the fields are doing work on charged matter within the region (via the Lorentz four-force). Naturally, we call $u = (e^2 + b^2)/2$ the \textbf{electromagnetic energy density}. The \textbf{Poynting vector} $\vv s = \vv e \times \vv b$ gives the flow of electromagnetic energy (its per-unit-area rate of transport).

We have what we need from the Lorentz four-force density to take care of our unfinished business. Later we'll return to Equation \ref{eq:lffdc2} and simplify its \emph{spatial} component into a succinct statement of three-momentum conservation.


\subsubsection{Light}\label{sssec:li}

Light is an electromagnetic wave---a disturbance in the (Lorenz-gauge!) four-potential $\vv A = \vv A (\vv R)$ that propagates through free space (where the four-current density field $\vv J = \vv J (\vv R)$ is zero).\footnote{Our ``potential-first" approach in this section is \emph{only} valid in the Lorenz gauge. The results it generates for the $\vv e$ and $\vv b$ fields, however, are gauge-invariant.} To discuss it mathematically, then, means working with solutions to the homogeneous wave equation
\begin{equation*}
\Box \vv A =  \left(\dfrac{\partial^2}{\partial (ct)^2} - \nabla ^2 \right) \vv A = \textrm{\mbox{\boldmath $\emptyset$}}
\end{equation*}
(Equation \ref{eq:fp} with $\vv J = \textrm{\mbox{\boldmath$\emptyset$}}$). We won't go over \emph{how to solve} this second-order linear partial differential equation. Rather, we'll take for granted the result from Fourier analysis that any of its solutions can be expressed as a superposition of monochromatic plane waves, and that its general monochromatic solution is:
\begin{equation}\label{eq:pw}
\vv A (\vv R) = \Re \left \lbrace \bar{\vv A} \mathrm{e}^{\mathrm{i} \mkern.5mu \vv K \cdot \vv R} \right \rbrace ,
\end{equation}
where the magnitude of the constant four-vector $\bar{\vv A} = \langle \bar{A}^{ct}, \, \bar{\vv a} \rangle$ is the wave's peak amplitude, and the likewise constant \textbf{four-wavevector} ${\vv K = \langle K^{ct}, \, \vv k \rangle}$ points in the spacetime-direction of the wave's propagation ($\vv k$ points in the \emph{spatial} direction of the wave's propagation).\footnote{A more common notation for the amplitude vector is $\vv A_0$, but we've been using the naught subscript for ``proper" quantities, so we'll use bar notation instead.} If you were expecting a sinusoidal function of something like $(\vv k \cdot \vv r - 2 \pi \nu t )$, note that by Euler's formula ${\Re \lbrace \mathrm{e}^{\mathrm{i} \mkern.5mu \theta} \rbrace = \Re \lbrace \cos \theta + \mathrm{i} \sin \theta \rbrace = \cos \theta}$, and also that $\vv K \cdot \vv R = K^{ct}ct - \vv k \cdot \vv r$, with $K^{ct}$ related to the frequency $\nu$ by $K^{ct} = 2 \pi \nu / c$ (cosine is an even function, so it doesn't matter which term gets subtracted from the other). The complex exponential function is convenient for linear operations; only when we perform non-linear operations (we won't) or need the field itself do we take the real part.

We'll assume that the amplitude vector $\bar{\vv A}$ is real, but it's worth mentioning that in general it may be complex. In the simplest case of complex $\bar{\vv A}$, we'd have $\bar{\vv A} = \mathcal{A} \mathrm{e}^{\mathrm{i} \mkern.5mu \delta}$ for some real four-vector $\mathcal{A}$, and $\vv A$ would then be the real part of ${\mathcal{A} \mathrm{e}^{\mathrm{i} \mkern.5mu \delta} \mathrm{e}^{\mathrm{i} \mkern.5mu \vv K \cdot \vv R} = \mathcal{A} \mathrm{e}^{\mathrm{i} \mkern.5mu ( \vv K \cdot \vv R \, + \, \delta )}}$, which differs from Equation \ref{eq:pw} for real $\bar{\vv A}$ only by the inclusion of a phase constant in the exponent. A more complicated scenario arises when complex $\bar{\vv A}$ \emph{cannot} be expressed as $\mathcal{A} \mathrm{e}^{\mathrm{i} \mkern.5mu \delta}$ for real $\mathcal{A}$, but no matter---the resulting wave is always equivalent to a superposition of solutions that are free of this complication.\footnote{\label{fn:pol}This applies to a wave that's elliptically or circularly polarized.} In the end, keeping $\bar{\vv A}$ real won't affect the results we're after, though strictly speaking we should add a phase constant to the exponential and sinusoidal arguments we encounter. The four-wavevector $\vv K$ is necessarily real (for our free-space waves).

Now, surely $\vv K$ is lightlike (null), yes? As a sanity check, let's set the d'Alembertian of Equation \ref{eq:pw} to zero and see what happens (we need not take the real part, since differentiation is a linear operation). Introducing $f(\vv R) \equiv \vv K \cdot \vv R$ to avoid clutter, and using a product rule:\footnote{This product rule is $\partialup \cdot (g \vv Q) = g (\partialup \cdot \vv Q) + (\partialup g) \cdot \vv Q$ for a four-vector $\vv Q$ and a spacetime scalar function $g$, easily verified by writing it out in component form. In our case, $\vv Q = \partialup h$ (four-gradient of a scalar $h$), and so $\partialup \cdot (g \, \partialup h) = g \Box h + \partialup g \cdot \partialup h$.}
\begin{equation*}
\begin{aligned}
\textrm{\mbox{\boldmath $\emptyset$}} &= \Box \left( \bar{\vv A} \mathrm{e}^{\mathrm{i} f} \right) = \bar{\vv A} \, \Box \mkern1mu \mathrm{e}^{\mathrm{i} f} \\
0 &= \partialup \cdot \partialup \mathrm{e}^{\mathrm{i} f} = \partialup \cdot \left( \mathrm{i} \mathrm{e}^{\mathrm{i} f} \, \partialup f \right) \\
0 &= \mathrm{e}^{\mathrm{i} f} \, \Box f + \partialup \mathrm{e}^{\mathrm{i} f} \cdot \partialup f  \\
0 &=  \Box f + \mathrm{i} \, \partialup f \cdot \partialup f ,
\end{aligned}
\end{equation*}
which requires that $\Box f = 0$ and $\partialup f \cdot \partialup f = 0$. But $\partialup f = \partialup (\vv K \cdot \vv R) = \vv K$:
\begin{equation*}
\begin{aligned}
\partialup (\vv K \cdot \vv R) &= \left \langle \dfrac{\partial}{\partial (ct)}, \, - \dfrac{\partial}{\partial x}, \, - \dfrac{\partial}{\partial y}, \, - \dfrac{\partial}{\partial z} \right \rangle \left( K^{ct} ct - K^x x - K^y y - K^z z \right) \\[3pt]
&= \left \langle K^{ct} \dfrac{\partial (ct)}{\partial (ct)}, \, K^x \dfrac{\partial x}{\partial x}, \, K^y \dfrac{\partial y}{\partial y} , \, K^z \dfrac{\partial z}{\partial z} \right \rangle = \vv K ,
\end{aligned}
\end{equation*}
and our constraints become $\partialup \cdot \vv K = 0$ (trivial---$\vv K$ is constant) and $\vv K \cdot \vv K = 0$. So $\vv K$ \emph{is} lightlike, and let's further insist that it's future-pointing (otherwise we allow for light waves traveling backward in time!). The Lorenz condition $\partialup \cdot \vv A = 0$ (Equation \ref{eq:lg}) imposes an additional constraint on $\vv K$:
\begin{equation*}
\begin{aligned}
0 &= \partialup \cdot \left( \bar{\vv A} \mathrm{e}^{\mathrm{i} f} \right) \\
&= \bar{\vv A} \cdot \partialup \mathrm{e}^{\mathrm{i} f} \\
&= \mathrm{i} \mathrm{e}^{\mathrm{i} f} \bar{\vv A} \cdot \partialup f ,
\end{aligned}
\end{equation*}
giving $\bar{\vv A} \cdot \vv K = 0$. Combined with the fact that $\vv K$ is future-pointing lightlike ($K^{ct} = k$), this orthogonality means that $\bar{A}^{ct} K^{ct} = \bar{A}^{ct} k = \bar{\vv a} \cdot \vv k$.

With these constraints and Equations \ref{eq:eb}, we can differentiate Equation \ref{eq:pw} to obtain the electric and magnetic fields of the monochromatic plane wave. We'll keep using $f(\vv R) = \vv K \cdot \vv R$ for short, bearing in mind that $\partialup f = \vv K$, whose components are $\partial^{ct} f = K^{ct}$ and $- \del f = \vv k$. First the magnetic field:\footnote{The omitted $\Re \lbrace \rbrace$ notation is implied. We'll take the real part after differentiating.}
\begin{equation*}
\begin{aligned}
\del \times \vv a &= \del \times \left( \bar{\vv a} \mkern1mu \mathrm{e}^{\mathrm{i} f} \right) \\
&= \del \mathrm{e}^{\mathrm{i} f} \times \bar{\vv a} \\
&= \mathrm{i} \mathrm{e}^{\mathrm{i} f} \del f \times \bar{\vv a} \\
&= - \mathrm{i} \mathrm{e}^{\mathrm{i} f} \, \vv k \times \bar{\vv a},
\end{aligned}
\end{equation*}
where we've used the product rule for the curl of a scalar-times-a-vector. Defining $\bar{\vv b} \equiv \vv k \times \bar{\vv a}$ and noting $\Re \lbrace - \mathrm{i} \mathrm{e}^{\mathrm{i} f} \rbrace = \Re \lbrace -\mathrm{i} ( \cos f + \mathrm{i} \sin f ) \rbrace = \sin f$, the real part is:\footnote{A few paragraphs up, we mentioned that it doesn't matter whether we use positive or negative $f = \vv K \cdot \vv R$ because cosine is an even function. Well, here we have a sine function, which is \emph{odd}. Does it matter now? No: if we'd started with $\mathrm{e}^{-\mathrm{i} f}$, we'd have ended up with $- \sin (-f) = \sin f$.}
\begin{equation}\label{eq:bpw}
\vv b (\vv R) = \bar{\vv b} \, \sin \left( \vv K \cdot \vv R \right) .
\end{equation}
Now the electric field, using $K^{ct} = k = (\vv{\hat{k}} \cdot \vv k)$ and $\bar{A}^{ct} = \bar{\vv a} \cdot \vv k / k = (\vv{\hat{k}} \cdot \bar{\vv a} )$:
\begin{equation*}
\begin{aligned}
- \del A^{ct} - \partial^{ct} \vv a &= - \del \left( \bar{A}^{ct} \mathrm{e}^{\mathrm{i} f} \right) - \partial^{ct} \left( \bar{\vv a} \mkern1mu \mathrm{e}^{\mathrm{i} f} \right) \\
&= - \mathrm{i} \mathrm{e}^{\mathrm{i} f} \left( \bar{A}^{ct} \del f + \bar{\vv a} \, \partial^{ct} f \right) \\
&= - \mathrm{i} \mathrm{e}^{\mathrm{i} f} \left( K^{ct} \bar{\vv a} - \bar{A}^{ct} \vv k \right) \\[1pt]
&= - \mathrm{i} \mathrm{e}^{\mathrm{i} f} \left( \bar{\vv a} (\vv{\hat{k}} \cdot \vv k )  - \vv k (\vv{\hat{k}} \cdot \bar{\vv a} ) \right) \\[1pt]
&= - \mathrm{i} \mathrm{e}^{\mathrm{i} f} \, \vv{\hat{k}} \times (\bar{\vv a} \times \vv k) = - \mathrm{i} \mathrm{e}^{\mathrm{i} f} \, \bar{\vv b} \times \vv{\hat{k}}
\end{aligned}
\end{equation*}
(``$bac - cab$" rule). Defining $\bar{\vv e} \equiv \bar{\vv b} \times \vv{\hat{k}}$, the real part is:
\begin{equation}\label{eq:epw}
\vv e (\vv R) = \bar{\vv e} \, \sin \left( \vv K \cdot \vv R \right) .
\end{equation}
The takeaway here is that for a light wave, $\vv e$, $\vv b$, and $\vv k$ (in cyclic permutations of that order) constitute a right-handed set of mutually perpendicular three-vectors, with $\vv e$ and $\vv b$ having equal magnitudes (recall our discovery in Section \ref{sssec:lff} that $\vv e \perp \vv b$ and $e = b$ are Lorentz-invariant conditions).\footnote{This is actually too broad a statement, since a \emph{general} superposition of monochromatic plane-wave solutions doesn't retain these properties. A superposition of solutions that all share a $\vv{\hat{k}}$ does, however.} It follows that the electromagnetic energy density $u = (e^2 + b^2) / 2$ at any point reduces to $u = e^2 = b^2$, and that the Poynting vector $\vv s = \vv e \times \vv b$ reduces to ${\vv s = u \vv{\hat{k}} = e^2 \vv{\hat{k}} = b^2 \vv{\hat{k}}}$.\footnote{\label{fn:epc}In Section \ref{ssec:emm}, we'll learn that the Poynting vector is the electromagnetic momentum density (\emph{in addition} to representing the flux density or flow of electromagnetic energy). This means that the relation $s = u$ gives $E = pc$ for light (we'll have derived Equation \ref{eq:3}).} By Equations \ref{eq:bpw} and \ref{eq:epw}, the wave's \emph{average} energy density is $\langle u \rangle = \bar{e}^2 / 2 = \bar{b}^2 / 2$ (the average of a squared sinusoidal function over the course of a cycle is always $1/2$), and the wave's \emph{average} Poynting vector is $\langle \vv s \rangle = \langle u \rangle \vv{\hat{k}} = \vv{\hat{k}} \mkern1mu \bar{e}^2 / 2 = \vv{\hat{k}} \mkern1mu \bar{b}^2 / 2$.


\subsubsection{Transforming Light's Energy}

Having established that a light wave's average energy density is $\langle u \rangle = \bar{e}^2 / 2$, we can learn how light's energy transforms under a Lorentz boost by transforming $\langle u \rangle V = \bar{e}^2 V / 2$, where $V$ is the volume containing the electromagnetic energy in the first frame. Of course, dividing by two has no effect on a transformation rule, and $\bar{\vv e}$ is just the maximum value of $\vv e$ during a cycle. So $\langle u \rangle$ and $u$ transform the same way, and our mission stated more simply is to transform $uV = e^2 V$.

For $u = e^2 = \vv e \cdot \vv e$, let's use Equations \ref{eq:ebr}:
\begin{equation*}
\begin{aligned}
u^\prime &= \left( \cosh{\phi} \; \vv e + \sinh{\phi} \; (\hatphi \times \vv b) - 2 \sinh^2 \frac{\phi}{2} \; (\hatphi \cdot \vv e) \hatphi \right)^2 \\
&= e^2 \cosh^2{\phi} + (\hatphi \times \vv b)^2 \sinh^2{\phi} + 4(\hatphi \cdot \vv e)^2 \sinh^4{\frac{\phi}{2}} \\
& \qquad + 2 \vv e \cdot (\hatphi \times \vv b) \cosh{\phi} \, \sinh{\phi} - 4 (\hatphi \cdot \vv e)^2 \cosh{\phi} \,\sinh^2{\frac{\phi}{2}}
\end{aligned}
\end{equation*}
($\hatphi \cdot (\hatphi \times \vv b) = 0$). What a mess! But our hyperbolic identities save the day. First consider just the two terms with $\phi / 2$ arguments, temporarily suppressing their common $(\hatphi \cdot \vv e)^2$ factor. Using $2\sinh^2(\phi/2) = \cosh \phi - 1$ and $\cosh^2 \phi - \sinh^2 \phi = 1$:
\begin{equation*}
\begin{aligned}
& 4 \sinh^4{\frac{\phi}{2}} - 4 \cosh{\phi} \,\sinh^2{\frac{\phi}{2}} \\[2pt]
& \qquad  = 4 \left[ \left( \dfrac{\cosh{\phi} - 1}{2} \right)^2 - \cosh{\phi} \left( \dfrac{\cosh{\phi} - 1}{2} \right) \right] \\[4pt]
& \qquad = \left( \cosh^2{\phi} - 2 \cosh{\phi} + 1 \right) - 2 \left( \cosh^2{\phi} - \cosh{\phi} \right) \\[3pt]
& \qquad = - \sinh^2{\phi} .
\end{aligned}
\end{equation*}
Now we can group $- (\hatphi \cdot \vv e)^2 \sinh^2{\phi}$ with the remaining $\sinh^2{\phi}$ term:
\begin{equation*}
u^\prime = e^2 \cosh^2{\phi} + \left[ (\hatphi \times \vv b)^2 - (\hatphi \cdot \vv e)^2 \right] \sinh^2{\phi} - 2 e^2 ( \hatphi \cdot \vv{\hat s}) \cosh{\phi} \, \sinh{\phi} ,
\end{equation*}
where we've also used $\vv e \cdot (\hatphi \times \vv b) = \hatphi \cdot (\vv b \times \vv e) = - e^2 (\hatphi \cdot \vv{\hat s})$ (Poynting vector is $\vv s = \vv e \times \vv b$, and $e = b$ for light). Next, work on what's in the square brackets, invoking the identity $(\vv q \times \vv w)^2 = q^2 w^2 - (\vv q \cdot \vv w)^2$:
\begin{equation*}
\begin{aligned}
(\hatphi \times \vv b)^2 - (\hatphi \cdot \vv e)^2 &= b^2 - (\hatphi \cdot \vv b)^2 - (\hatphi \cdot \vv e)^2 \\
&= e^2 \left( 1 - (\hatphi \cdot \vv{\hat b})^2 - (\hatphi \cdot \vv{\hat e})^2 \right) \\
&= e^2 ( \hatphi \cdot \vv{\hat s} )^2
\end{aligned}
\end{equation*}
(because $\vv e$, $\vv b$, and $\vv s$ are mutually perpendicular for light).\footnote{This is merely the Euclidean distance formula in disguise: $(\hatphi \cdot \vv{\hat e})$, $(\hatphi \cdot \vv{\hat b})$, and $( \hatphi \cdot \vv{\hat s} )$ are the direction cosines for $\hatphi$ with respect to the ``Cartesian axes" $\vv e$, $\vv b$, and $\vv s$, so their squares must sum to $1$.} Suddenly our transformation for $u$ looks quite nice:
\begin{equation*}
u^\prime = e^2 \left( \cosh^2{\phi} - 2 ( \hatphi \cdot \vv{\hat s}) \cosh{\phi} \, \sinh{\phi} + ( \hatphi \cdot \vv{\hat s} )^2 \sinh^2{\phi} \right) ,
\end{equation*}
or subbing in $u$ for $e^2$ and factoring:
\begin{equation}\label{eq:edt}
u^\prime = u \left( \cosh{\phi} - \sinh{\phi} \, \cos{\theta} \right)^2 ,
\end{equation}
$\theta$ being the angle between the boost direction $\hatphi$ and the light wave's direction of propagation $\vv{\hat s}$.

Comparing Equation \ref{eq:edt} to our ``target" energy transformation (Equation \ref{eq:8}), and knowing that we'll have to multiply $u^\prime$ by the transformed volume $V^\prime$ to get the transformed energy, we see that the volume transformation \emph{must} have the form $V^\prime = V / ( \cosh{\phi} - \sinh{\phi} \, \cos{\theta} )$. But what volume are we talking about here? The energy in question doesn't ``belong" to anything that has a rest frame; rather, it's being transported at the speed of light for all inertial observers ($\vv s = u \vv{\hat{k}}$). There's no proper volume in this context, and our ``volume contraction" formula will do us no good.\footnote{This is similar to how length contraction played no role in our derivation of the relativistic Doppler effect for wavelength back in Section \ref{sssec:rdf} (Equation \ref{eq:rdw}). In fact, it will turn out that wavelength and the volume we're seeking transform in the same way.}

Here's one way to think about the volume we're interested in (more or less how Einstein did it). An omnipotent inertial observer ``pauses" time and places a make-believe sphere around a patch of space through which a monochromatic plane wave is propagating (or was, anyway, before time was paused). The sphere must be unphysical because when the observer ``unpauses" time, it travels \emph{with} the wave at the speed of light, always enclosing the same ``part" of the wave and thus the same amount of electromagnetic energy---equal to the wave's (average) energy density times the sphere's volume. What we want to do is write an equation for the sphere's surface in terms of position and time in the omnipotent observer's ``unprimed" frame, use the Lorentz transformation to sub in the ``primed" coordinates (without expecting the primed observer to regard the shape as a sphere), and calculate the shape's unprimed and primed volumes accordingly.

In Cartesian coordinates, the equation for the surface of a sphere is:
\begin{equation*}
(x - x_0)^2 + (y - y_0)^2 + (z - z_0)^2 = R^2,
\end{equation*}
where $R$ is the sphere's radius and the point $(x_0, y_0, z_0)$ is the sphere's center. The volume is $V = 4 \pi R^3 / 3$. Let's say that our omnipotent inertial observer ``pauses" time at ${ct = 0}$ and centers the sphere around the unprimed spatial origin. The initial unprimed equation for the surface is then:
\begin{equation*}
x^2 + y^2 + z^2 = R^2.
\end{equation*}
When time is ``unpaused," the sphere and its center move at the speed of light in the direction of the light's three-wavevector $\vv k$. At some later time $ct = ct$, then, the $x$-coordinate of the sphere's center is its initial $x$-coordinate ($0$) plus the $x$-component of the light's ``displacement" since $ct = 0$, which is the $x$-component of the light's (normalized) velocity vector $\vv{\hat{k}}$ times $ct$.\footnote{The three-wavevector $\vv k$ isn't dimensionless, but the unit vector $\vv{\hat{k}} = \vv k / k$ is, so $\vv{\hat{k}}$ is indeed the light's normalized velocity. Same goes for $\vv{\hat s} = \vv s / s$.} If $\theta$ is the angle between the positive $x$-direction and $\vv{\hat{k}}$, then the $x$-component of $\vv{\hat{k}}$ is $\Vert \vv{\hat{k}} \Vert \cos{\theta} = \cos{\theta}$. The $x$-coordinate of the sphere's center as a function of time is therefore $x_0 = \cos{\theta} \, ct$. Likewise, $y_0 = \cos{\alpha} \, \mkern.5mu ct$ and $z_0 = \cos{\kappa} \, \mkern.5mu ct$ for the corresponding angles $\alpha$ and $\kappa$, and the sphere's equation is:
\begin{equation*}
(x - \cos{\theta} \, ct)^2 + (y - \cos{\alpha} \, \mkern.5mu ct)^2 + (z - \cos{\kappa} \, \mkern.5mu ct)^2 = R^2 .
\end{equation*}

Under a standard-configuration boost, the primed frame moves in the positive $x$-direction, and $\theta$ is consequently the angle between the \emph{boost direction} and $\vv{\hat{k}}$. By Equations \ref{eq:lt}, $R^2$ in terms of the primed coordinates is:
\begin{equation*}
\begin{aligned}
&R^2 = \left[ \gamma (x^\prime + \beta ct^\prime )  - \cos{\theta} \, \gamma (ct^\prime + \beta x^\prime) \right]^2 \\[2pt]
& \qquad + \left[ y^\prime - \cos{\alpha} \, \mkern.5mu \gamma (ct^\prime + \beta x^\prime) \right]^2 + \left[ z^\prime - \cos{\kappa} \, \mkern.5mu \gamma (ct^\prime + \beta x^\prime) \right]^2.
\end{aligned}
\end{equation*}
The volume remains constant over time, so let's choose $ct^\prime = 0$:
\begin{equation}\label{eq:ell}
\left[ \gamma ( 1 - \beta \cos{\theta} ) \right]^2 x^{\prime \, 2} + \left( y^\prime -  \gamma \beta x^\prime \cos{\alpha} \right)^2 + \left( z^\prime - \gamma \beta x^\prime \cos{\kappa} \right)^2 = R^2 .
\end{equation}
As we anticipated, this primed equation isn't a sphere; it's an \emph{ellipsoid}. We need the ellipsoid's volume, which is proportional to the product of its three principal semi-axes. In his original paper on special relativity, Einstein shrugs this off as a ``simple calculation." It's not so simple! The problem is those $x^\prime$ terms in $(y^\prime - \gamma \beta x^\prime \cos \alpha)^2$ and $(z^\prime - \gamma \beta x^\prime \cos \kappa)^2$.\footnote{\label{fn:sph}These subtracted expressions are \emph{not} constants. At least one (highly recommended!) text makes this error, getting the right answer by happenstance while mischaracterizing the ellipsoid as a spheroid (an ellipsoid with at least two of its three principal semi-axes sharing a length): Ta-Pei Cheng, \emph{Einstein's Physics: Atoms, Quanta, and Relativity Derived, Explained, and Appraised} (Oxford: Oxford, 2013), 164.} If they weren't there, we could divide both sides by $R^2$ to put the ellipsoid in standard form, and the principal semi-axes would reveal themselves. But they're there, and we have to deal with them.

There's a hard way and a shortcut, and they both involve linear algebra. Let's take the shortcut first.\footnote{Hat tip: \url{https://www.mathpages.com/home/kmath354/kmath354.htm}.} The trick is that for the purpose of finding the ellipsoid's volume, \emph{we can pretend the troublesome terms aren't there at all}! That is, the volume of our ellipsoid is equal to the volume of \emph{another} ellipsoid that looks just like Equation \ref{eq:ell} without those terms. The reason is that a transformation of the form $X^\prime = x^\prime$, $Y^\prime = y^\prime + A x^\prime$, $Z^\prime = z^\prime + B x^\prime$ for constants $A$ and $B$ is a three-dimensional \emph{shearing}, and three-dimensional shearings \emph{preserve volume}. As a quick example of how this works, consider a cube in three-dimensional Euclidean space spanned by the unit vectors $\vv m = \langle 1, 0, 0 \rangle$, $\vv n = \langle 0, 1, 0 \rangle$, and $\vv o = \langle 0, 0, 1 \rangle$. The volume of the cube (or indeed of any parallelepiped spanned by three vectors) is the absolute value of the scalar triple product: $| \vv m \cdot (\vv n \times \vv o ) | = | \vv m \cdot \vv m | = 1$. Now, the transformation matrix for the shearing we've described is:\footnote{Readers well-versed in linear algebra will know that the determinant of this matrix is the transformation's volume-scaling factor. It's $1$, so volume is preserved.}
\begin{equation*}
\mathcal{S} =
\begin{bmatrix}
1 & 0 & 0 \\
A & 1 & 0 \\
B & 0 & 1
\end{bmatrix}.
\end{equation*}
Applying the transformation to our vectors gives $\mathcal{S} [\vv n] = [\vv n]$ and $\mathcal{S} [\vv o] = [\vv o]$ but $\mathcal{S} [\vv m] = [ \vv q ]$ where $\vv q = \langle 1, A, B \rangle$. Then the absolute value of the scalar triple product of our \emph{new} vectors is ${| \vv q \cdot (\vv n \times \vv o) | = | \langle 1, A, B \rangle \cdot \langle 1, 0, 0 \rangle | = 1}$, and the shearing has preserved the volume of the spanned solid. It's not difficult to obtain this result for a more complicated trio of original vectors (they needn't be mutually orthogonal or have the same length)---we'll always find that the magnitude of the scalar triple product (and hence the volume of the spanned parallelepiped) is the same before and after the transformation. We can appeal to integral calculus to extend the argument to our ellipsoid: if we approximate its structure by assembling many tiny parallelepipeds (cubes, say), then their total volume (preserved under a shearing) equals the volume of the ellipsoid in the limit that their number approaches infinity. Shearing the ellipsoid preserves its volume.

All that is to say, our ellipsoid has the same volume as this friendlier one:
\begin{equation*}
\left[ \gamma ( 1 - \beta \cos{\theta} ) \right]^2 x^{\prime \, 2} + y^{\prime \, 2} + z^{\prime \, 2} = R^2 .
\end{equation*}
The volume is $V^\prime = 4 \pi abc / 3$, where $a$, $b$, and $c$ are the lengths of the principal semi-axes, satisfying:
\begin{equation*}
\dfrac{x^{\prime \, 2}}{a^2} + \dfrac{y^{\prime \, 2}}{b^2} + \dfrac{z^{\prime \, 2}}{c^2} = 1 .
\end{equation*}
We have $b = c = R$ and $a = R / [\gamma ( 1 - \beta \cos{\theta} )]$, and so:
\begin{equation}\label{eq:vol}
\begin{split}
V^\prime &= \dfrac{4 \pi}{3} \, abc = \dfrac{4 \pi }{3} \, R^3 \, \dfrac{1}{\gamma \left( 1 - \beta \cos{\theta} \right)} \\
V^\prime &= \dfrac{V}{\gamma \left( 1 - \beta \cos{\theta} \right)} ,
\end{split}
\end{equation}
which is precisely the transformation we were expecting. Perhaps unsurprisingly, this volume transforms the same way as wavelength (Equation \ref{eq:rdw}). Combining Equations \ref{eq:edt} and \ref{eq:vol}, we see that the energy $E$ of a light wave, equal to its (average) energy density $u$ times a ``containing" volume $V$, transforms under a Lorentz boost like this:
\begin{equation}\label{eq:len}
\begin{split}
E^\prime &= u^\prime V^\prime \\
&= \left[ u \gamma^2 ( 1 - \beta \cos{\theta} )^2  \right] \left[ \dfrac{V}{\gamma ( 1 - \beta \cos{\theta} )} \right] \\[4pt]
&= uV \, \dfrac{\gamma^2 ( 1 - \beta \cos{\theta} )^2}{\gamma ( 1 - \beta \cos{\theta} )} \\[5pt]
E^\prime &= \gamma E \left( 1 - \beta \cos{\theta} \right) .
\end{split}
\end{equation}
Equations \ref{eq:len} and \ref{eq:8} are identical, and light's energy transforms the same way as its frequency (Equation \ref{eq:rdf}). At long last, we've made good on our promise to show this without resorting to the Planck--Einstein relation!

For the sake of completeness, here is the hard way to get the volume of the ellipsoid in Equation \ref{eq:ell} (consult a linear algebra text if you have trouble following along). Start by expanding it into quadratic form:
\begin{equation*}
Ax^{\prime \, 2} + y^{\prime \, 2} + z^{\prime \, 2} + D x^\prime y^\prime + E x^\prime z^\prime = R^2,
\end{equation*}
where $A = \gamma^2 (1 - \beta \cos \theta)^2 + (\gamma \beta)^2 ( \cos^2 \alpha + \cos^2 \kappa )$, $D = -2 \gamma \beta \cos \alpha$, and $E = -2 \gamma \beta \cos \kappa$. Note that $A = \gamma^2 (1 - \beta \cos \theta)^2 + (D^2 + E^2)/4$. In matrix form:
\begin{equation}\label{eq:ellm}
\begin{bmatrix}
x^\prime & y^\prime & z^\prime
\end{bmatrix}
\begin{bmatrix}
A & D/2 & E/2 \\
D/2 & 1 & 0 \\
E/2 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
x^\prime \\
y^\prime \\
z^\prime
\end{bmatrix}
= \begin{bmatrix} R^2 \end{bmatrix} .
\end{equation}
To reduce this ellipsoid to standard form, we'll have to make the appropriate \emph{orthogonal} coordinate transformation (one that maps orthonormal bases to orthonormal bases). In the new coordinates, then, we'll have:
\begin{equation*}
\begin{bmatrix}
X^\prime & Y^\prime & Z^\prime
\end{bmatrix}
\begin{bmatrix}
\lambda_1 & 0 & 0 \\
0 & \lambda_2 & 0 \\
0 & 0 & \lambda_3
\end{bmatrix}
\begin{bmatrix}
X^\prime \\
Y^\prime \\
Z^\prime
\end{bmatrix}
= \begin{bmatrix} R^2 \end{bmatrix} ,
\end{equation*}
where $\lambda_1$, $\lambda_2$, and $\lambda_3$ are the eigenvalues of the 3-by-3 matrix in Equation \ref{eq:ellm} (i.e., we diagonalize the original 3-by-3 to avoid cross-terms in the new coordinates). It follows that the lengths of the ellipsoid's principal semi-axes are $R / \sqrt{\lambda_1}$, $R / \sqrt{\lambda_2}$, and $R / \sqrt{\lambda_3}$, and we need only find the eigenvalues to determine the volume. We find them by solving for $\lambda$ here:
\begin{equation*}
\det
\left(
\begin{bmatrix}
A - \lambda & D/2 & E/2 \\
D/2 & 1 - \lambda & 0 \\
E/2 & 0 & 1 - \lambda
\end{bmatrix}
\right)
= 0 .
\end{equation*}
By Laplace expansion and some algebra, that's:
\begin{equation*}
\begin{aligned}
0 &= \left( A - \lambda \right) \left( 1 - \lambda \right)^2 - \dfrac{D}{2} \left( \dfrac{D}{2} \left(1 - \lambda \right) \right) + \dfrac{E}{2} \left( - \left(1 - \lambda \right) \dfrac{E}{2} \right) \\[5pt]
&= \left(1 - \lambda \right) \left( \lambda^2 - \left( 1 + A \right)\lambda + A - \dfrac{D^2 + E^2}{4} \right).
\end{aligned}
\end{equation*}
One eigenvalue is $1$. Getting the others isn't as bad as it looks. Sub in our $A$, $D$, and $E$ values, remembering that $A = \gamma^2 (1 - \beta \cos \theta)^2 + (D^2 + E^2)/4$:
\begin{equation*}
\lambda^2 - \left[ 1 + \gamma^2 (1 - \beta \cos \theta)^2 + (\gamma \beta)^2 ( \cos^2 \alpha + \cos^2 \kappa ) \right] \lambda + \gamma^2 (1 - \beta \cos \theta)^2 = 0.
\end{equation*}
In the square brackets, expand $(1 - \beta \cos \theta)^2$, replace ${\cos^2 \alpha + \cos^2 \kappa}$ with ${1 - \cos^2 \theta}$ (squared direction cosines sum to $1$), and use $\gamma^2 \beta^2 + 1 = \gamma^2$:
\begin{equation*}
\lambda^2 - 2 \gamma^2 ( 1 - \beta \cos \theta ) \lambda + \gamma^2 (1 - \beta \cos \theta)^2 = 0.
\end{equation*}
It's hard to spot, but because $(1 - \beta)(1 + \beta) = \gamma^{-2}$, that factors to:
\begin{equation*}
\left[ \lambda - \gamma^2 (1 - \beta \cos \theta) (1 + \beta) \right] \left[ \lambda - \gamma^2 (1 - \beta \cos \theta) (1 - \beta) \right] = 0,
\end{equation*}
and our other two eigenvalues are $\lambda = \gamma^2 (1 - \beta \cos \theta)(1 \pm \beta)$. The three principal semi-axes of the ellipsoid then have lengths $R$, $R /  \sqrt{\gamma^2 (1 - \beta \cos \theta)(1 + \beta)}$, and $R / \sqrt{\gamma^2 (1 - \beta \cos \theta)(1 - \beta)}$. Their product is $R^3 / [\gamma(1 - \beta \cos \theta)]$, and we can indeed reproduce Equation \ref{eq:vol}. Note that no two principal semi-axes of our ellipsoid have the same length. It is \emph{not} a spheroid (see Footnote \ref{fn:sph}).


\subsubsection{Relativistic Doppler Effect from the Four-Wavevector}

Before we move away from the subject of light, let's quickly re-derive the relativistic Doppler formula (Equation \ref{eq:rdf}). We'll use the four-wavevector ${\vv K = \langle K^{ct}, \, \vv k \rangle = \langle 2 \pi \nu / c, \, 2 \pi \nu \vv{\hat{k}} / c \rangle }$ from Section \ref{sssec:li} (if you're confused about the value of $\vv k$, recall our determination that $\vv K$ is future-pointing lightlike). Actually, let's define ${\vv H \equiv \vv K c / (2 \pi) = \langle H^{ct}, \, \vv h \rangle = \langle \nu, \, \nu \vv{\hat{h}} \rangle }$ (with $\vv{\hat{h}} = \vv{\hat{k}}$, of course) and use that instead.

This ``derivation" is little more than an acknowledgment that the components of $\vv H$, like those of any four-vector, transform Lorentzianly. We have ${\vv H = \langle H^{ct}, \, h_x, \, h_y, \, h_z \rangle = \langle \nu , \, h \cos \theta, \, h_y, \, h_z \rangle = \langle \nu , \, \nu \cos \theta, \, h_y, \, h_z \rangle}$, where $\theta$ is the angle that the light's direction of propagation makes with the positive unprimed $x$-axis. Then under a standard-configuration boost, where the primed frame moves in the positive $x$-direction (so that $\theta$ is also the angle that the light's direction of propagation makes with the primed frame's velocity), the time component transforms like $\nu ^{\prime} = \gamma (\nu - \beta h_x ) = \gamma \mkern1mu \nu (1 - \beta \cos \theta)$, and we're done! Much easier this way, eh? As an aside, it's now obvious that the unprimed frame in Equation \ref{eq:rdf} doesn't \emph{have} to be the light-source's rest frame. Finally, let's use the ``trick" from Section \ref{sssec:mc} to express the relationship between $\nu ^{\prime}$ and $\nu$ in a manifestly covariant form: $\nu ^{\prime} / \nu = (\vv B^{\textrm{p}} \cdot \vv H) / (\vv B^{\textrm{u}} \cdot \vv H)$, where $\vv B^{\textrm{p}}$ and $\vv B^{\textrm{u}}$ are respectively the normalized four-velocities of the primed and unprimed frames. (I avoid the symbols $\vv B ^{\prime}$ and $\vv B$ here because that notation might wrongly suggest a single underlying quantity; $\vv B^{\textrm{p}}$ and $\vv B^{\textrm{u}}$ are two \emph{different} four-vectors.)


\clearpage

\section[Covariant Electrodyadics (Tensors Lite)]{Covariant ``Electrodyadics" (Tensors Lite)}\label{sec:dy}

In Section \ref{sssec:lffd}, we simplified the temporal component of the Lorentz four-force density into a succinct statement of energy conservation. Now that we've taken care of our unfinished business concerning light and energy, we return to the Lorentz four-force density and simplify its three-vector \emph{spatial} component into a succinct statement of \emph{three-momentum} conservation. As we might expect from what we saw with Poynting's theorem (Equation \ref{eq:py} et seq.), this statement will equate the rate of decrease of electromagnetic three-momentum in a region to the outward ``flux" of electromagnetic three-momentum plus the rate at which the fields transfer three-momentum to particles within the region. Why the scare quotes around the word \emph{flux}? Because flux is traditionally the rate of flow of a \emph{scalar} quantity through a surface,\footnote{(according to our definition, anyway; see Footnote \ref{fn:flux})} and three-momentum is a \emph{vector} quantity. The usual divergence theorem equates a \emph{scalar}-valued flux to the volume-integral of the divergence of a \emph{vector} field, whereas the corresponding volume-integrand for a \emph{vector}-valued ``flux" would have to be the ``divergence" of a \emph{rank-2 tensor} field (see Footnote \ref{fn:gd}). As we haven't dealt explicitly with tensor notation or the ``divergence" of anything but a vector field, this presents a challenge!\footnote{That said, we deal implicitly with rank-2 tensors every time we take a directional derivative or Laplacian/d'Alembertian of a vector field.}

We take this opportunity to dip our toes into rank-2 tensors, without getting into their full machinery, and without getting bogged down in \emph{too} much new notation. Actually, we've already done so: at the end of Section \ref{ssec:in}, we represented one three-vector as a column matrix and another as a row matrix, we multiplied the matrices in that order (column times row), and we mentioned in passing that the resulting 3-by-3 was the matrix representation of the \emph{dyadic product} of the two vectors (we used the symbol $\otimes$ for this operation). What we didn't mention is that the dyadic product is a special case of the \textbf{tensor product}, and that the \textbf{dyad} it outputs is indeed a kind of rank-2 tensor.

We'll mainly limit our discussion of rank-2 tensors to \textbf{dyadics}, which are dyads and their sums. First we'll discuss \textbf{Euclidean dyadics} (or \textbf{three-dyadics}) and cover the conservation of electromagnetic three-momentum. Then we'll look at dyadics in Minkowski spacetime and use them to recast our (already manifestly covariant) electrodynamics from Section \ref{sec:rem} into even more powerful and elegant forms.

\subsection{Euclidean Dyadics and Three-Momentum}\label{ssec:emm}

\subsubsection{Geometric Objects vs. Their Matrix Representations}

It's important to understand that tensors, like vectors (but unlike matrices), are \emph{geometric} objects, characterized by components that transform a certain way under a coordinate transformation. In fact, vectors \emph{are} tensors---of rank-1. And scalars are tensors of rank-0. A tensor's rank tells you (among other things) how many indices you need to uniquely specify one of its components. Not all three-dimensional Euclidean dyadics are dyads (that is, the dyadic product of two vectors), but those that aren't are \emph{sums} of dyads, and all Euclidean dyadics have nine ``doubled up" Cartesian components ($xx$, $xy$, $xz$, $yx$, $yy$, $yz$, $zx$, $zy$, $zz$), perfect for a square-matrix representation.\footnote{To keep things simple we'll continue to use Cartesian coordinates only.} (A rank-3 tensor can be represented by a cubic array, and so on.)

The geometric nature of tensors is why we're careful to use language like ``the matrix representation of a dyadic," and why we'll continue to use different notation for matrix representations of geometric objects than we do for the geometric objects themselves. For example, we'll use $[\vv e]$ for the column-matrix representation of the electric field, and we'll use $[\vv e]^{\textrm{T}}$ for the transpose of that matrix (i.e., for the row-matrix representation of $\vv e$). Likewise, $[\vv c][\vv d]^{\textrm{T}}$ is the matrix representation of the dyad $\vv c \otimes \vv d$, and ${[\vv d][\vv c]^{\textrm{T}} = ([\vv c][\vv d]^{\textrm{T}})^{\textrm{T}}}$ is the matrix representation of $\vv d \otimes \vv c$ (the dyadic product is not commutative). As for the divergence $\del \cdot \vv c$ (of a vector), we have the matrix representation of the dot product $[\del]^{\textrm{T}}[\vv c]$.\footnote{Though we'll often use brackets to indicate matrix representations, we'll still sometimes use them for \emph{grouping} (like parentheses). Context should make clear which is meant.}

And what about the ``divergence" of a Euclidean dyadic? Or more generally, what about the Euclidean ``dot product" of a vector and a dyadic? The operation should produce a \emph{vector}. Let's let our matrix notation lead the way. If we have a vector $\vv m$ that we'd like to ``dot" with the dyad $\vv c \otimes \vv d$, there are actually \emph{two different} matrix products involving $[\vv c][\vv d]^{\textrm{T}}$ and a matrix representation of $\vv m$ that generate a vector. The first is $[\vv m]^{\mathrm{T}}[\vv c][\vv d]^{\textrm{T}}$, which gives the row-matrix representation of a vector, and the second is $[\vv c][\vv d]^{\textrm{T}}[\vv m]$, which gives the column-matrix representation of a \emph{different} vector. So let's define \emph{two} ``dot products" between a vector and a dyad: we define $\vv m \cdot (\vv c \otimes \vv d)$ as the vector $(\vv m \cdot \vv c) \vv d$ whose row-matrix representation is $[\vv m]^{\mathrm{T}}[\vv c][\vv d]^{\textrm{T}}$, and we define $(\vv c \otimes \vv d) \cdot \vv m$ as the vector $(\vv d \cdot \vv m) \vv c$ whose column-matrix representation is $[\vv c][\vv d]^{\textrm{T}}[\vv m]$. Since the matrix product is distributive over addition, and since a dyadic that isn't a dyad can be expressed as the sum of dyads, we can extend these dot-product definitions to dyadics generally (just replace $[\vv c][\vv d]^{\textrm{T}}$ with the square-matrix representation of the dyadic).\footnote{This will be a theme: define for dyads, and invoke linearity to extend to dyadics.} For the divergence, we can apply our first dot-product definition to $\del$, so that $\del \cdot (\vv c \otimes \vv d)$ is the vector whose row-matrix representation is $[\del]^{\textrm{T}}([\vv c][\vv d]^{\textrm{T}})$, where we've included the parentheses to make clear that $\del$ operates not just on $\vv c$ but on the dyadic $\vv c \otimes \vv d$. We'll make use of a product rule that goes $[\del]^{\textrm{T}}([\vv c][\vv d]^{\textrm{T}}) = [\del_{\vv c}]^{\textrm{T}}[\vv c][\vv d]^{\textrm{T}} + [\vv c]^{\textrm{T}}[\del_{\vv d}][\vv d]^{\textrm{T}}$ (if you need convincing, carry out the matrix multiplication). In our preferred vector notation, that's $\del \cdot (\vv c \otimes \vv d) = (\del \cdot \vv c) \vv d + (\vv c \cdot \del) \vv d$, so any time we spot the $(\del \cdot \vv c) \vv d + (\vv c \cdot \del) \vv d$ pattern in the wild we may express it as the divergence of $\vv c \otimes \vv d$.

One more preliminary. The Euclidean dyadic whose Cartesian matrix representation is the identity matrix $I_3$ is called the \textbf{unit dyadic}, which we'll notate $\inlinedy{\upiota}$ (that's a lowercase roman iota topped with a double-sided arrow). Its divergence $\del \cdot \inlinedy{\upiota}$ vanishes, since $[\del]^{\textrm{T}}I_3 = [\vv 0]^{\textrm{T}}$. Note that for a scalar function $f$, the divergence $\del \cdot (f \mkern-2mu \inlinedy{\upiota})$ is just $\del f$ (gradient of $f$) because ${[\del]^{\textrm{T}}(f I_3) = [\del f]^{\textrm{T}} I_3 + f [\del]^{\textrm{T}} I_3 = [\del f]^{\textrm{T}}}$.\footnote{The unit dyadic is related to the more broadly applicable \textbf{Kronecker delta}.}

\subsubsection{Electromagnetic Three-Momentum; Maxwell Stress Tensor}

With all that out of the way, let's proceed with our first task: to simplify the three-vector spatial component of the Lorentz four-force density into a succinct statement of three-momentum conservation. From Equations \ref{eq:lffdc} and \ref{eq:lffdc2}:
\begin{equation*}
\dfrac{\dd \vv f_{\mathrm{L}}}{\dd V} = \left( \del \cdot \vv e \right) \vv e - \vv b \times \left( \del \times \vv b \right) + \vv b \times \partial^{ct} \vv e .
\end{equation*}
The first term on the right we recognize as one of the two ingredients we'd need to make $\del \cdot (\vv e \otimes \vv e)$. The ingredient we don't have is $(\vv e \cdot \del) \vv e$, which we could get as the ``$cab$" part of a ``$bac - cab$" expansion of $\vv e \times (\del \times \vv e)$ if only we had an $\vv e \times (\del \times \vv e)$. On the flip side, we do have a $\vv b \times (\del \times \vv b)$, which we could ``$bac - cab$" to get the $(\vv b \cdot \del) \vv b$ ingredient of $\del \cdot (\vv b \otimes \vv b)$, but we're missing $(\del \cdot \vv b) \vv b$. Well, actually we \emph{do} have $(\del \cdot \vv b) \vv b$, because by Maxwell's equations that's just $\vv 0$ (Equations \ref{eq:me}). As for $\vv e \times (\del \times \vv e)$, consider the last term in the equation. By the product rule for the cross product (mind the signs!), $\vv b \times \partial^{ct} \vv e$ is equivalently $\vv e \times \partial^{ct} \vv b - \partial^{ct} \left( \vv e \times \vv b \right)$, which we can shorten to $\vv e \times \partial^{ct} \vv b - \partial^{ct} \vv s$ (Poynting vector). But since Maxwell's equations also give us $\partial^{ct} \vv b = - \del \times \vv e$, we have $\vv b \times \partial^{ct} \vv e = - \vv e \times (\del \times \vv e) - \partial^{ct} \vv s$, and so:
\begin{equation*}
\dfrac{\dd \vv f_{\mathrm{L}}}{\dd V} = \left( \del \cdot \vv e \right) \vv e - \vv e \times (\del \times \vv e) + (\del \cdot \vv b) \vv b - \vv b \times \left( \del \times \vv b \right) - \partial^{ct} \vv s .
\end{equation*}
Now use the ``$bac - cab$" rule on the differential vector triple products, noting that the ``$bac$" parts are $\frac{1}{2} \del (\vv e \cdot \vv e)$ and $\frac{1}{2} \del (\vv b \cdot \vv b)$, which sum to $\del u$ (gradient of the electromagnetic energy density):
\begin{equation*}
\begin{aligned}
\dfrac{\dd \vv f_{\mathrm{L}}}{\dd V} &= \left( \del \cdot \vv e \right) \vv e + \left( \vv e \cdot \del \right) \vv e + (\del \cdot \vv b) \vv b + \left( \vv b \cdot \del \right) \vv b - \del u - \partial^{ct} \vv s \\
&= \del \cdot (\vv e \otimes \vv e) + \del \cdot (\vv b \otimes \vv b) - \del \cdot (u \mkern-4.5mu \inlinedy{\upiota}) - \partial^{ct} \vv s \\[2pt]
&= - \del \cdot \inlinedy{\upsigma} - \, \partial^{ct} \vv s ,
\end{aligned}
\end{equation*}
where the dyadic $\inlinedy{\upsigma} \equiv u \mkern-4.5mu \inlinedy{\upiota} - \, \vv e \otimes \vv e - \vv b \otimes \vv b$ is the \textbf{Maxwell stress tensor}.\footnote{Often $\inlinedy{\upsigma}$ is instead defined as $\vv e \otimes \vv e + \vv b \otimes \vv b - u \mkern-4.5mu \inlinedy{\upiota}$. Our choice leads to fewer minus signs in ``important" equations.} Rearranging, our statement of momentum conservation is:
\begin{equation}\label{eq:tmc}
\boxed{ - \partial^{ct} \vv s = \del \cdot \inlinedy{\upsigma} \, + \, \dfrac{\dd \vv f_{\mathrm{L}}}{\dd V} } \, .
\end{equation}
A comparison with Poynting's theorem (Equation \ref{eq:py}) shows that $\vv s$ and $\inlinedy{\upsigma}$ are to electromagnetic three-momentum as $u$ and $\vv s$ are to electromagnetic energy. The Poynting vector does ``double duty": from Poynting's theorem we learned that it represents the flux density (or flow) of electromagnetic energy, but with Equation \ref{eq:tmc} we see that it's also the \textbf{electromagnetic momentum density}.\footnote{In light of what we learned in Section \ref{sssec:li} (see Footnote \ref{fn:epc}), we've now derived $E = pc$ for electromagnetic waves (full version of Equation \ref{eq:3}).} And the Maxwell stress tensor---a dyadic, as promised---represents the flux density (or flow) of electromagnetic three-momentum.

\subsubsection{How Dyadic Components Transform}

Let's take a closer look at the matrix representation of the Euclidean dyadic product $\vv c \otimes \vv d$:
\begin{equation*}
\begin{bmatrix}
c_x \\
c_y \\
c_z
\end{bmatrix}
\begin{bmatrix}
d_x & d_y & d_z
\end{bmatrix}
=
\begin{bmatrix}
c_x d_x & c_x d_y & c_x d_z \\
c_y d_x & c_y d_y & c_y d_z \\
c_z d_x & c_z d_z & c_z d_z 
\end{bmatrix} .
\end{equation*}
Under a counterclockwise rotation of the $x$- and $y$-axes through the angle $\theta$, the components of $\vv c$ transform like
\begin{equation*}
\begin{bmatrix}
c_x^\prime \\[1pt]
c_y^\prime \\[1pt]
c_z^\prime
\end{bmatrix}
=
\begin{bmatrix}
\cos \theta & \sin \theta & 0 \\
- \sin \theta & \cos \theta & 0 \\
0 & 0 & 1 
\end{bmatrix}
\begin{bmatrix}
c_x \\
c_y \\
c_z
\end{bmatrix} ,
\end{equation*}
and likewise for $\vv d$. If we label that 3-by-3 rotation matrix $R$, it follows that the nine ``doubled up" Cartesian components of the dyad $\vv c \otimes \vv d$ transform by two ``applications" of the rotation matrix like this:
\begin{equation*}
[\vv c^\prime \mkern1mu ] [\vv d^\prime \mkern1mu ]^{\mathrm{T}} = ( R [\vv c] ) ( R [\vv d] )^\mathrm{T} = R \, [\vv c] [\vv d]^{\mathrm{T}} R^{\mathrm{T}} ,
\end{equation*}
where $R^{\mathrm{T}}$ is the matrix transpose of $R$ (swap the signs of the sines). Indeed, the Cartesian components of \emph{every} Euclidean dyadic (dyad or sum of dyads) transform this way under the specified rotation, with the matrix representation of the dyadic flanked on the left by $R$ and on the right by $R^{\mathrm{T}}$. The Maxwell stress tensor from the previous section, for example:
\begin{equation*}
[ \mkern2mu {\inlinedy{\upsigma}} ^{\mkern1mu \prime} \mkern1mu ] = R \mkern1mu [\mkern1.5mu \inlinedy{\upsigma} \mkern1.5mu] R^{\mathrm{T}}
\end{equation*}
(we've extended our bracket notation to signify a \emph{dyadic's} square-matrix representation). Euclidean dyadics can be \emph{defined} as geometric objects whose Cartesian components transform like this under a rotation of axes.

\subsection{Core Electrodyadics}\label{ssec:mfd}

\subsubsection{Minkowski Dyadics}

In a similar way, a dyad in Minkowski spacetime (a \textbf{Minkowski dyad} or a \textbf{four-dyad}) is the dyadic product of \emph{four}-vectors---e.g., $\vv Q \otimes \vv W$---and its \emph{sixteen} ``doubled up" ($ct$ + Cartesian) components consequently transform like so under a \emph{Lorentz} transformation:
\begin{equation*}
[\vv Q^\prime \mkern1mu ] [\vv W^\prime \mkern1mu ]^{\mathrm{T}} = ( \Lambda [\vv Q] ) ( \Lambda [\vv W] )^\mathrm{T} = \Lambda [\vv Q] [\vv W]^{\mathrm{T}} \Lambda^{\mathrm{T}} ,
\end{equation*}
where $\Lambda$ is the Lorentz-transformation matrix and $\Lambda^{\mathrm{T}}$ is its transpose (they're both Equation \ref{eq:bm} if the transformation is a standard-configuration boost along the $x$-axis). \emph{Dyadics} in Minkowski spacetime (\textbf{Minkowski dyadics} or \textbf{four-dyadics}) are four-dyads and their sums. They can be \emph{defined} as geometric objects whose sixteen ``doubled up" ($ct$ + Cartesian) components transform like those of four-dyads under a Lorentz transformation:
\begin{equation}\label{eq:ltd}
\boxed{ [ \mkern1mu {\capdy{\mathsf{D}} {\vphantom{D}}^\prime} \mkern1mu ] = \Lambda [ \capdy{\mathsf{D}} ] \Lambda^{\mathrm{T}}}
\end{equation}
(we're keeping our double-sided arrow notation, though instead of the lowercase roman type that we use for Euclidean three-dyadics we'll prefer \emph{uppercase} roman for four-dyadics, mirroring our use of lowercase for three-vectors and uppercase for four-vectors).

Let's verify that the ``transforms-like-a-dyad" definition of \emph{dyadic} is compatible with the ``sum-of-dyads" definition. It should be obvious that this would work for Euclidean dyadics, too, but we'll do it with four-dyadics.

Given a four-dyadic $\inlinedy{\mathsf{D}}$ (whose components are known to transform correctly), we have for its matrix representation in some inertial frame:
\begin{equation*}
\begin{bmatrix}
\mathsfit D^{ct \mkern1.5mu ct} & \mathsfit D^{ct \mkern1.5mu x} & \mathsfit D^{ct \mkern1.5mu y} & \mathsfit D^{ct \mkern1.5mu z} \\
\mathsfit D^{x \mkern1.5mu ct} & \mathsfit D^{x x} & \mathsfit D^{x y} & \mathsfit D^{x z}  \\
\mathsfit D^{y \mkern1.5mu ct} & \mathsfit D^{y x} & \mathsfit D^{y y} & \mathsfit D^{y z}  \\
\mathsfit D^{z \mkern1.5mu ct} & \mathsfit D^{z x} & \mathsfit D^{z y} & \mathsfit D^{z z} 
\end{bmatrix}.
\end{equation*}
Define a four-vector $\vv D_1$ with components $\langle \mathsfit D^{ct \mkern1.5mu ct}, \mathsfit D^{x \mkern1.5mu ct}, \mathsfit D^{y \mkern1.5mu ct}, \mathsfit D^{z \mkern1.5mu ct} \rangle$ in this frame (first column of $[ \inlinedy{\mathsf{D}} ]$), and another $\vv E_{ct}$ with components $\langle 1, 0, 0, 0 \rangle$, and observe that the matrix representation of the dyad $\vv D_1 \otimes \vv E_{ct}$ shares its first column with $[ \inlinedy{\mathsf{D}} ]$ but otherwise has only zeros:
\begin{equation*}
\begin{bmatrix}
\mathsfit D^{ct \mkern1.5mu ct} \\
\mathsfit D^{x \mkern1.5mu ct} \\
 \mathsfit D^{y \mkern1.5mu ct} \\
\mathsfit D^{z \mkern1.5mu ct}
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 0 & 0
\end{bmatrix}
=
\begin{bmatrix}
\mathsfit D^{ct \mkern1.5mu ct} & 0 & 0 & 0 \\
\mathsfit D^{x \mkern1.5mu ct} & 0 & 0 & 0 \\
\mathsfit D^{y \mkern1.5mu ct} & 0 & 0 & 0 \\
\mathsfit D^{z \mkern1.5mu ct} & 0 & 0 & 0
\end{bmatrix}.
\end{equation*}
Define similar four-vectors corresponding to the other columns of $[ \inlinedy{\mathsf{D}} ]$, and obtain: $\inlinedy{\mathsf{D}} = \vv D_1 \otimes \vv E_{ct} + \vv D_2 \otimes \vv E_{x} + \vv D_3 \otimes \vv E_{y} + \vv D_4 \otimes \vv E_{z}$.\footnote{$\vv E_{ct}$, $\vv E_{x}$, $\vv E_{y}$, and $\vv E_{z}$ are the basis vectors we briefly discussed in Section \ref{sssec:mc}.} So yes, a dyadic can always be decomposed into a sum of dyads.

\subsubsection{Faraday Tensor; Antisymmetry; Transpose; Wedge Product}

In the context of electromagnetism, a particularly useful four-dyadic (field) is $\partialup \otimes \vv A - \vv A \otimes \partialup_{\vv A}$, where $\vv A$ is the four-potential (we'll refer to the dyad $\partialup \otimes \vv A$ as the \emph{gradient} of the four-vector $\vv A$). In matrix notation, that's ${[\partialup][\vv A]^{\mathrm{T}} - [\vv A][\partialup_{\vv A}]^{\mathrm{T}}}$, or:
\begin{equation*}
\begin{bmatrix}
\partial^{ct} \\
\partial^x \\
\partial^y \\
\partial^z
\end{bmatrix}
\begin{bmatrix}
A^{ct} & A^x & A^y & A^z
\end{bmatrix}
-
\begin{bmatrix}
A^{ct} \\
A^x \\
A^y \\
A^z
\end{bmatrix}
\begin{bmatrix}
\partial^{ct} & \partial^x & \partial^y & \partial^z
\end{bmatrix}_{\vv A},
\end{equation*}
which comes to
\begin{equation*}
\begin{bmatrix}
0 & ( \partial^{ct} A^x - \partial^x A^{ct} ) & ( \partial^{ct} A^y - \partial^y A^{ct} ) & ( \partial^{ct} A^z - \partial^z A^{ct} ) \\[1.5ex]
( \partial^x A^{ct} - \partial^{ct} A^x ) & 0 & ( \partial^x A^y - \partial^y A^x ) & ( \partial^x A^z - \partial^z A^x ) \\[1.5ex]
( \partial^y A^{ct} - \partial^{ct} A^y ) & ( \partial^y A^x - \partial^x A^y ) & 0 & ( \partial^y A^z - \partial^z A^y ) \\[1.5ex]
( \partial^z A^{ct} - \partial^{ct} A^z ) & ( \partial^z A^x - \partial^x A^z ) & ( \partial^z A^y - \partial^y A^z ) & 0
\end{bmatrix} .
\end{equation*}
We call this dyadic the \textbf{Faraday tensor}, or the \textbf{electromagnetic field tensor}.\footnote{\label{fn:dy}Specifically, dyadics are \emph{contravariant} rank-2 tensors, a detail that matters in more advanced treatments of Minkowski spacetime (doesn't matter in practice for Euclidean space if one is using Cartesian coordinates).} Labeling it $\inlinedy{\mathsf{F}} = \inlinedy{\mathsf{F}} (\vv R)$, and invoking Equations \ref{eq:ebc}, we have:
\begin{equation}\label{eq:emt}
[\capdy{\mathsf{F}}]
=
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\[.5ex]
e_x & 0 & -b_z & b_y \\[.5ex]
e_y & b_z & 0 & -b_x \\[.5ex]
e_z & -b_y & b_x & 0
\end{bmatrix} .
\end{equation}
The matrix is antisymmetric, and we call the dyadic itself antisymmetric. (Likewise, a dyadic whose matrix representation is \emph{symmetric} is said to be symmetric.) It's also convenient to call a dyadic's \emph{transpose} the dyadic whose matrix representation is the transpose of the first's. Then we can say ${\inlinedy{\mathsf{F}} = \partialup \otimes \vv A - \vv A \otimes \partialup_{\vv A} = \partialup \otimes \vv A - (\partialup \otimes \vv A)^\mathrm{T}}$, dropping the subscript notation. Finally, there's a shorthand for this procedure of producing an antisymmetric dyadic from a pair of vectors: $\vv Q \wedge \vv W \equiv \vv Q \otimes \vv W - \vv W \otimes \vv Q$ (called the \textbf{wedge product}).\footnote{The resulting antisymmetric dyadic is sometimes called a \textbf{bivector}.} So:
\begin{equation}\label{eq:emta}
\boxed{ \vphantom{A_{A_A}} \capdy{\mathsf{F}} \equiv \partialup \wedge \vv A } \, . 
\end{equation}

\subsubsection{Dotting Four-Dyadics with Four-Vectors; Metric Dyadic}

To see why the Faraday tensor is useful, we'll define the Minkowski dot product(s) between a four-dyadic $\inlinedy{\mathsf{D}}$ and a four-vector $\vv S$. We can let the matrix notation guide us again, but this time there are negative signs to account for---i.e., the matrix representation of $\vv Q \cdot \vv W$ (dot product of two four-vectors) isn't $[\vv Q]^{\textrm{T}} [\vv W]$ but is rather $[\vv Q]^{\textrm{T}} \eta [\vv W]$, with
\begin{equation}\label{eq:mmt}
\eta
=
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & -1 & 0 \\
0 & 0 & 0 & -1
\end{bmatrix} .
\end{equation}
Using the matrix $\eta$, we define $\vv S \cdot (\vv Q \otimes \vv W)$ as the four-vector $(\vv S \cdot \vv Q) \vv W$ whose row-matrix representation is $[\vv S]^\mathrm{T} \eta [\vv Q] [\vv W]^\mathrm{T}$, and we define $(\vv Q \otimes \vv W) \cdot \vv S$ as the four-vector $(\vv W \cdot \vv S) \vv Q$ whose column-matrix representation is $[\vv Q] [\vv W]^\mathrm{T} \eta [\vv S]$. By the same logic we used with Euclidean dyadics (the matrix product is distributive over addition, and a dyadic that isn't a dyad can be expressed as the sum of dyads), we extend these operations for four-dyads to four-\emph{dyadics} by replacing $[\vv Q][\vv W]^{\textrm{T}}$ with the square-matrix representation of the dyadic in question. Then $\vv S \cdot \inlinedy{\mathsf{D}}$ is the four-vector whose row-matrix representation is $[\vv S]^{\mathrm{T}} \eta [\inlinedy{\mathsf{D}}]$, and the ``reverse" operation $\inlinedy{\mathsf{D}} \cdot \, \vv S$ gives the four-vector whose column-matrix representation is $[\inlinedy{\mathsf{D}}] \eta [\vv S]$.

Note that $\inlinedy{\mathsf{D}} \cdot \, \vv S = \vv S \cdot \inlinedy{\mathsf{D}} {}^\mathrm{T}$ (always). If $\inlinedy{\mathsf{D}}$ happens to be antisymmetric, then also $\inlinedy{\mathsf{D}} \cdot \, \vv S = - \vv S \cdot \inlinedy{\mathsf{D}}$.

The four-\emph{divergence} of the dyadic $\inlinedy{\mathsf{D}}$ we notate $\partialup \cdot \inlinedy{\mathsf{D}}$, and so we define it as the four-vector whose row-matrix representation is $[\partialup]^{\mathrm{T}} \eta [\inlinedy{\mathsf{D}}]$. Well, we have $\inlinedy{\mathsf{D}} \cdot \mkern2mu \partialup_{\inlinedy{\mathsf{D}}} = \partialup \cdot \inlinedy{\mathsf{D}} {}^\mathrm{T}$, too, so really there are \emph{two} ways to take the divergence of a dyadic. If the dyadic is symmetric or antisymmetric (as will often be the case for us), then its divergences aren't independent. Regardless, by default we'll mean $\partialup \cdot \inlinedy{\mathsf{D}}$.

When we have the combination $[\partialup]^\mathrm{T} \eta$ in our matrix equations, it's understood that $\partialup$ operates on what's to the \emph{right} of $\eta$, after we matrix-multiply $\eta$ with one of its neighbors to give everything the right sign. As a quick example, the matrix product
\begin{equation*}
\mkern0mu [\partialup]^{\mathrm{T}} \eta [\capdy{\mathsf{D}}]
=
\begin{bmatrix}
\partial^{ct} & \partial^x & \partial^y & \partial^z
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & -1 & 0 \\
0 & 0 & 0 & -1
\end{bmatrix}
\begin{bmatrix}
\mathsfit D^{ct \mkern1.5mu ct} & \mathsfit D^{ct \mkern1.5mu x} & \mathsfit D^{ct \mkern1.5mu y} & \mathsfit D^{ct \mkern1.5mu z} \\
\mathsfit D^{x \mkern1.5mu ct} & \mathsfit D^{x x} & \mathsfit D^{x y} & \mathsfit D^{x z}  \\
\mathsfit D^{y \mkern1.5mu ct} & \mathsfit D^{y x} & \mathsfit D^{y y} & \mathsfit D^{y z}  \\
\mathsfit D^{z \mkern1.5mu ct} & \mathsfit D^{z x} & \mathsfit D^{z y} & \mathsfit D^{z z} 
\end{bmatrix}
\end{equation*}
is to be interpreted as:\footnote{Notice that the product $[\partialup]^\mathrm{T} \eta$ is a row matrix whose entries are our \emph{positive} partial derivatives ($\del = \langle - \partial^x , - \partial^y , - \partial^z \rangle$, recall, by Equation \ref{eq:fn}). In Section \ref{sssec:fd}, we found that these four positive operators together transform \emph{inverse}-Lorentzianly, and we mentioned in Footnote \ref{fn:cv} that an object whose components transform this way is a four-covector (or covariant four-vector). We won't make direct use of the covector concept, but we remark in passing that for \emph{any} four-vector $\vv Q$, the products $[\vv Q]^\mathrm{T} \eta$ and $\eta [\vv Q]$ give matrix representations of the covector whose components are $\langle Q_{ct}, \, Q_x, \, Q_y, \, Q_z \rangle = \langle Q^{ct}, \, -Q^x, \, -Q^y, \, -Q^z \rangle$.}
\begin{equation*}
\begin{bmatrix}
\partial^{ct} & - \partial^x & - \partial^y & -\partial^z
\end{bmatrix}
\begin{bmatrix}
\mathsfit D^{ct \mkern1.5mu ct} & \mathsfit D^{ct \mkern1.5mu x} & \mathsfit D^{ct \mkern1.5mu y} & \mathsfit D^{ct \mkern1.5mu z} \\
\mathsfit D^{x \mkern1.5mu ct} & \mathsfit D^{x x} & \mathsfit D^{x y} & \mathsfit D^{x z}  \\
\mathsfit D^{y \mkern1.5mu ct} & \mathsfit D^{y x} & \mathsfit D^{y y} & \mathsfit D^{y z}  \\
\mathsfit D^{z \mkern1.5mu ct} & \mathsfit D^{z x} & \mathsfit D^{z y} & \mathsfit D^{z z} 
\end{bmatrix},
\end{equation*}
or equivalently:
\begin{equation*}
\begin{bmatrix}
\partial^{ct} & \partial^x & \partial^y & \partial^z
\end{bmatrix}
\begin{bmatrix}
\mathsfit D^{ct \mkern1.5mu ct} & \mathsfit D^{ct \mkern1.5mu x} & \mathsfit D^{ct \mkern1.5mu y} & \mathsfit D^{ct \mkern1.5mu z} \\
- \mathsfit D^{x \mkern1.5mu ct} & - \mathsfit D^{x x} & - \mathsfit D^{x y} & - \mathsfit D^{x z}  \\
- \mathsfit D^{y \mkern1.5mu ct} & - \mathsfit D^{y x} & - \mathsfit D^{y y} & - \mathsfit D^{y z}  \\
- \mathsfit D^{z \mkern1.5mu ct} & - \mathsfit D^{z x} & - \mathsfit D^{z y} & - \mathsfit D^{z z} 
\end{bmatrix}.
\end{equation*}

Importantly, $\eta$ (Equation \ref{eq:mmt}) is itself the matrix representation of a four-dyadic, since it satisfies the four-dyadic transformation rule Equation \ref{eq:ltd} (try it!---under a Lorentz boost, the $1$ transforms to $1$, the $-1$'s transform to $-1$'s, and the $0$'s transform to $0$'s). This tensor is called the \textbf{Minkowski metric dyadic}, or the \textbf{(inverse) Minkowski metric tensor}, and we use it implicitly every time we take a Minkowski dot product.\footnote{\label{fn:cov}To be precise, what we use implicitly when we take dot products is the metric tensor itself (the \emph{covariant} metric), whereas the dyadic we're introducing now is the \emph{inverse} (or \emph{contravariant}) Minkowski metric (see Footnote \ref{fn:dy}). They're closely related and happen to have the same matrix representation, but they \emph{are} different objects. To convince yourself of this, Lorentz-transform the dot product of four-vectors in matrix notation: ${[ \vv Q ] ^\mathrm{T} \eta [ \vv W ] = [ \vv Q {}^\prime ] ^\mathrm{T} \eta ^\prime [ \vv W {}^\prime ] =  ( \Lambda [ \vv Q ] ) ^\mathrm{T} \eta ^\prime ( \Lambda [ \vv W ] ) = [ \vv Q ] ^\mathrm{T} ( \Lambda ^\mathrm{T} \eta^ \prime \Lambda ) [ \vv W ]} $, yielding $ \eta = \Lambda ^\mathrm{T} \eta^ \prime \Lambda $, or $\eta ^\prime = ( \Lambda ^{-1} ) ^\mathrm{T} \eta \Lambda ^{-1} $. Compare this with Equation \ref{eq:ltd}---that's \emph{not} how a four-dyadic's components transform! In fact, it's the transformation rule for \emph{covariant} rank-2 tensors, aka codyadics. Naturally, the covector and codyadic concepts are linked.} It's functionally analogous to the Euclidean unit dyadic we encountered in Section \ref{ssec:emm}. When we want to notate it explicitly \emph{as} a four-dyadic, we'll write $\inlinedy{\upeta}$ (a conventional exception to our preference for uppercase).

\subsubsection{Lorentz Force and Maxwell Equations with Dyadics}

Anyway, now we can make good use of the Faraday tensor. First establish a handy identity connecting ``vector triple products" with wedge products:
\begin{equation}\label{eq:vtpi}
\begin{split}
\vv S \mkern1mu `` \mkern-4mu \times (\vv Q \times \mkern-4mu " \mkern1mu \vv W) &= \vv Q ( \vv W \cdot \vv S) - \vv W ( \vv Q \cdot \vv S)  \\[2pt]
&= ( \vv Q \otimes \vv W ) \cdot \vv S - ( \vv W \otimes \vv Q ) \cdot \vv S \\[2pt]
\vv S \mkern1mu `` \mkern-4mu \times (\vv Q \times \mkern-4mu " \mkern1mu \vv W) &= ( \vv Q \wedge \vv W ) \cdot \vv S .
\end{split}
\end{equation}
Then apply it to our core electrodynamic equations. Here is the Lorentz four-force (Equation \ref{eq:ff}):
\begin{equation*}
\vv F_{\textrm{L}} = q \vv B \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A) = q (\partialup \wedge \vv A) \cdot \vv B ,
\end{equation*}
or more compactly in terms of the Faraday tensor (Equation \ref{eq:emta}):
\begin{equation}\label{eq:lfffd}
\boxed{ \vphantom{A_{A_A}} \vv F_{\mathrm{L}} = q \capdy{\mathsf{F}} \cdot \vv B } \, ,
\end{equation}
as we can verify with Equations \ref{eq:emt}, \ref{eq:mmt}, and \ref{eq:31}:
\begin{equation*}
q [\capdy{\mathsf{F}}] \eta [\vv B]
=
q
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\
e_x & 0 & -b_z & b_y \\
e_y & b_z & 0 & -b_x \\
e_z & -b_y & b_x & 0
\end{bmatrix}
\begin{bmatrix}
\gamma \\
- \gamma \beta_x \\
- \gamma \beta_y \\
- \gamma \beta_z
\end{bmatrix}
=
q
\begin{bmatrix}
\gamma \vvbeta \cdot \vv e \\
[ \gamma \vv e + \gamma \vvbeta \times \vv b ]
\end{bmatrix}
\end{equation*}
(cf. Equation \ref{eq:lfc}). The same procedure for the Lorentz four-force \emph{density} ${\bm{\mathfrak{F}} = \vv J \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A)}$ (Equation \ref{eq:lffd})---arguably the more fundamental quantity in the classical theory---gives:
\begin{equation}\label{eq:lffdfd}
\boxed{ \vphantom{A_{A_A}} \bm{\mathfrak{F}} = \capdy{\mathsf{F}} \cdot \vv J } \, .
\end{equation}

Next, the field equations. When we initially obtained Maxwell's equations (Equations \ref{eq:me}), we noted that they constitute two four-vectors, and that their forms suggest expressions involving $\partialup$ as the \emph{first} derivative of something. At the time, we didn't have the tools to proceed. Now we do, and we start with the inhomogeneous pair, $\del \cdot \, \vv e = \rho$ and ${ \del \times \vv b - \partial^{ct} \vv e = \vv j }$. From Equations \ref{eq:gfp} and \ref{eq:vtpi}:
\begin{equation*}
\vv J = - \partialup \mkern1mu `` \mkern-4mu \times ( \partialup \times \mkern-4mu " \mkern1mu \vv A ) = - ( \partialup \wedge \vv A ) \cdot \partialup_{\vv A} = - \partialup \cdot ( \partialup \wedge \vv A )^\mathrm{T} = \partialup \cdot (\partialup \wedge \vv A),
\end{equation*}
where the last step follows from the wedge product's antisymmetry, and:
\begin{equation}\label{eq:ime}
\boxed{ \vphantom{A_{A_A}} \partialup \cdot \capdy{\mathsf{F}} = \vv J } \, .
\end{equation}
The left side in matrix notation is:
\begin{equation*}
[\partialup]^{\mathrm{T}} \eta [\capdy{\mathsf{F}}]
=
\begin{bmatrix}
\partial^{ct} & - \partial^x & - \partial^y & - \partial^z
\end{bmatrix}
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\
e_x & 0 & -b_z & b_y \\
e_y & b_z & 0 & -b_x \\
e_z & -b_y & b_x & 0
\end{bmatrix} ,
\end{equation*}
giving $\partialup \cdot \inlinedy{\mathsf{F}} = \langle \del \cdot \vv e , \, - \partial^{ct} \vv e + \del \times \vv b \rangle$, as expected (we must remember that ${\del = \langle - \partial^x , - \partial^y , - \partial^z \rangle}$). As for the remaining Maxwell equations (the homogeneous $\del \cdot \vv b = 0$ and ${ - \del \times \vv e - \partial^{ct} \vv b = \vv 0 }$), we note that their left sides are just like the left sides of the first two, but with the roles of $\vv e$ and $\vv b$ reversed and a sign changed. Specifically, $\vv e$ goes to $\vv b$ and $\vv b$ goes to $- \vv e$. It should therefore be possible to form a new four-dyadic $\inlinedy{\mathsf{G}}$ from the Faraday tensor (Equation \ref{eq:emt}) by ``swapping" the $\vv e$ and $\vv b$ components and flipping the appropriate signs; the four-divergence of this new dyadic should equal the zero four-vector $\textrm{\mbox{\boldmath $\emptyset$}}$. Here:
\begin{equation}\label{eq:mt}
[ \capdy{\mathsf{G}} ]
=
\begin{bmatrix}
0 & -b_x & -b_y & -b_z \\[.5ex]
b_x & 0 & e_z & -e_y \\[.5ex]
b_y & -e_z & 0 & e_x \\[.5ex]
b_z & e_y & -e_x & 0
\end{bmatrix}
\end{equation}
(to verify that $\inlinedy{\mathsf{G}}$ is indeed a four-dyadic, check that its components transform according to Equation \ref{eq:ltd} under a Lorentz boost; for standard configuration, use Equations \ref{eq:bm} and \ref{eq:ebt}). Then:
\begin{equation}\label{eq:hme}
\boxed{ \vphantom{A_{A_A}} \partialup \cdot \capdy{\mathsf{G}} = \textrm{\mbox{\boldmath $\emptyset$}} } \, ,
\end{equation}
where the left side in matrix notation is:
\begin{equation*}
[\partialup]^{\mathrm{T}} \eta [\capdy{\mathsf{G}}]
=
\begin{bmatrix}
\partial^{ct} & - \partial^x & - \partial^y & - \partial^z
\end{bmatrix}
\begin{bmatrix}
0 & -b_x & -b_y & -b_z \\
b_x & 0 & e_z & -e_y \\
b_y & -e_z & 0 & e_x \\
b_z & e_y & -e_x & 0
\end{bmatrix} ,
\end{equation*}
giving the expected $\partialup \cdot \inlinedy{\mathsf{G}} = \langle \del \cdot \vv b , \, - \partial^{ct} \vv b - \del \times \vv e \rangle$. By Equations \ref{eq:ebc}, $[\inlinedy{\mathsf{G}}]$ is also:
\begin{equation*}
\begin{bmatrix}
0 & ( \partial^y A^z - \partial^z A^y ) & ( \partial^z A^x - \partial^x A^z ) & ( \partial^x A^y - \partial^y A^x ) \\[1.5ex]
( \partial^z A^y - \partial^y A^z ) & 0 & ( \partial^z A^{ct} - \partial^{ct} A^z ) & ( \partial^{ct} A^y - \partial^y A^{ct} ) \\[1.5ex]
( \partial^x A^z - \partial^z A^x ) & ( \partial^{ct} A^z - \partial^z A^{ct} ) & 0 & ( \partial^x A^{ct} - \partial^{ct} A^x ) \\[1.5ex]
( \partial^y A^x - \partial^x A^y ) & ( \partial^y A^{ct} - \partial^{ct} A^y ) & ( \partial^{ct} A^x - \partial^x A^{ct} ) & 0
\end{bmatrix} .
\end{equation*}
Using \emph{that} matrix to carry out $[\partialup]^{\mathrm{T}} \eta [\inlinedy{\mathsf{G}}]$, we can confirm that all components of $\partialup \cdot \inlinedy{\mathsf{G}}$ indeed vanish. For example, the time component is:
\begin{equation*}
(\partialup \cdot \capdy{\mathsf{G}})^{ct} = - \partial^x ( \partial^z A^y - \partial^y A^z ) - \partial^y ( \partial^x A^z - \partial^z A^x ) - \partial^z ( \partial^y A^x - \partial^x A^y ) ,
\end{equation*}
with each of the six terms canceling its additive inverse. The same thing happens with the spatial components. So we get Equation \ref{eq:hme} ``for free" by virtue of the Faraday tensor's having the form $\partialup \wedge \vv A$. We'll soon learn that there's a mathematical identity at work here.

Equations \ref{eq:ime} and \ref{eq:hme} are Maxwell's equations in a manifestly covariant and gauge-invariant form. Equations \ref{eq:gfp} and \ref{eq:fp} contain the same information as Equation \ref{eq:ime}, but the former is rather opaque and the latter is specific to the Lorenz gauge. By the way, we'll call our new four-dyadic $\inlinedy{\mathsf{G}}$ the \textbf{Maxwell tensor} (not to be confused with the Euclidean Maxwell \emph{stress} tensor $\inlinedy{\upsigma}$ that we met in Section \ref{ssec:emm}).


\subsection{Hodge Duality and the Maxwell Tensor}

\subsubsection{Hodge Duality for Four-Dyadics}\label{sssec:hd}

The Maxwell tensor $\inlinedy{\mathsf{G}}$ is the \textbf{Hodge dual} of the Faraday tensor $\inlinedy{\mathsf{F}}$. To explain what this means ``geometrically," we first introduce a limited sense in which three-vectors may be treated as ``quasi"-four-vectors: \emph{at a single moment for a given inertial frame}, we can ``append" a zero temporal component to a three-vector (that isn't already the spatial component of a true four-vector) and manipulate the four ``components" as if they constituted a spacelike four-vector orthogonal to the frame's (normalized) four-velocity $\vv B = \langle 1, \vv 0 \rangle$. We'll notate these quasi-four-vectors like $\tilde{\vv w} = \langle 0, \vv w \rangle$, with a tilde over the symbol for the corresponding three-vector. We stress that quasi-four-vectors aren't \emph{actually} four-vectors---their ``components" don't transform Lorentzianly between frames! But sometimes intra-frame equations involving them \emph{have the same form} in all inertial frames.

Since any three-vector $\vv w$ can be expressed as a cross product ${ \vv m \times \vv n }$ (or curl, if $\vv m$ is $\del$), we can always construct a unique quasi-four-\emph{dyadic} from a quasi-four-vector $\tilde{\vv w} = \langle 0, \vv w \rangle$ by taking the wedge product $\tilde{\vv m} \wedge \tilde{\vv n}$ for some $\tilde{\vv m} = \langle 0, \vv m \rangle$ and $\tilde{\vv n} = \langle 0, \vv n \rangle$ that satisfy ${ \vv m \times \vv n = \vv w }$. In matrix form, that's:
\begin{equation*}
\begin{bmatrix}
0 \\
m_x \\
m_y \\
m_z
\end{bmatrix}
\begin{bmatrix}
0 & n_x & n_y & n_z
\end{bmatrix}
-
\begin{bmatrix}
0 \\
n_x \\
n_y \\
n_z
\end{bmatrix}
\begin{bmatrix}
0 & m_x & m_y & m_z
\end{bmatrix}
=
\begin{bmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & w_z & -w_y \\
0 & -w_z & 0 & w_x \\
0 & w_y & -w_x & 0
\end{bmatrix} .
\end{equation*}
Let's label this quasi-four-dyadic $\{ \tilde{\vv w} \}$.

We'll now demonstrate that at a single moment for a given inertial frame with four-velocity $\vv B$, we can form an antisymmetric Minkowski dyadic $\inlinedy{\mathsf{A}}$ in terms of two quasi-four-vectors $\tilde{\vv q} = \langle 0, \vv q \rangle$ and $\tilde{\vv w} = \langle 0, \vv w \rangle$ like this:
\begin{equation*}
\capdy{\mathsf{A}} = \tilde{\vv q} \wedge \vv B - \{ \tilde{\vv w} \}.
\end{equation*}
We've already established the matrix representation of $\{ \tilde{\vv w} \}$. For the other quasi-four-dyadic $\tilde{\vv q} \wedge \vv B = \tilde{\vv q} \otimes \vv B - \vv B \otimes \tilde{\vv q}$, we have the matrix representation:
\begin{equation*}
\begin{bmatrix}
0 \\
q_x \\
q_y \\
q_z
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 0 & 0
\end{bmatrix}
-
\begin{bmatrix}
1 \\
0 \\
0 \\
0
\end{bmatrix}
\begin{bmatrix}
0 & q_x & q_y & q_z
\end{bmatrix}
=
\begin{bmatrix}
0 & -q_x & -q_y & -q_z \\
q_x & 0 & 0 & 0 \\
q_y & 0 & 0 & 0 \\
q_z & 0 & 0 & 0
\end{bmatrix}.
\end{equation*}
Thus:
\begin{equation*}
[\capdy{\mathsf{A}}] = [\tilde{\vv q} \wedge \vv B] - [\{ \tilde{\vv w} \}]
=
\begin{bmatrix}
0 & -q_x & -q_y & -q_z \\[.5ex]
q_x & 0 & -w_z & w_y \\[.5ex]
q_y & w_z & 0 & -w_x \\[.5ex]
q_z & -w_y & w_x & 0
\end{bmatrix},
\end{equation*}
and we see that the matrix representation of $\inlinedy{\mathsf{A}}$ has exactly the same form as that of the Faraday tensor $\inlinedy{\mathsf{F}}$ (Equation \ref{eq:emt}). It follows that if we transform $\vv q$ and $\vv w$ under a Lorentz boost in the same way that we transform the electric and magnetic fields (Equations \ref{eq:ebt} or \ref{eq:ebr}), we'll find that the form $\inlinedy{\mathsf{A}} = \tilde{\vv q} \wedge \vv B - \{ \tilde{\vv w} \}$ holds for \emph{all} inertial frames (where for each frame $\vv B$ is \emph{that frame's} four-velocity $\langle 1, \vv 0 \rangle$), and $\inlinedy{\mathsf{A}}$ must be a true antisymmetric Minkowski dyadic like $\inlinedy{\mathsf{F}}$ is. With the right choice of $\tilde{\vv q}$ and $\tilde{\vv w}$, any antisymmetric four-dyadic can be expressed in this manner.
%\footnote{For a deeper dive that proves it, see this unpublished paper from 2018 by Jonas Larsson and Karl Larsson: \url{https://urn.kb.se/resolve?urn=urn:nbn:se:umu:diva-152747}.}

So the combination of quasi-four-dyadics $\tilde{\vv q} \wedge \vv B - \{ \tilde{\vv w} \}$ makes a \emph{true} antisymmetric dyadic. For the Faraday tensor, it's $\inlinedy{\mathsf{F}} = \tilde{\vv e} \wedge \vv B - \{ \tilde{\vv b} \}$. And if we study the matrix representation of the Maxwell tensor (Equation \ref{eq:mt}), we find that $\inlinedy{\mathsf{G}} = \tilde{\vv b} \wedge \vv B - \{ - \tilde{\vv e} \}$. That's how $\inlinedy{\mathsf{F}}$ and $\inlinedy{\mathsf{G}}$ are related in terms of (quasi-)geometric objects: a given antisymmetric Minkowski dyadic ${\inlinedy{\mathsf{A}} = \tilde{\vv q} \wedge \vv B - \{ \tilde{\vv w} \}}$ can be mapped to another one, its \emph{Hodge dual} ${\tilde{\vv w} \wedge \vv B - \{ - \tilde{\vv q} \}}$, by swapping the quasi-four-vectors and slapping a negative sign on the one that ends up in second position. We use the $\star$ symbol to indicate the Hodge dual (some people use an asterisk):
\begin{equation}\label{eq:mthd}
\boxed{ \vphantom{A_{A_A}} \capdy{\mathsf{G}} \equiv \star \capdy{\mathsf{F}} } \, .
\end{equation}
The Hodge dual of the Maxwell tensor is then $\star \inlinedy{\mathsf{G}} = \star \star \inlinedy{\mathsf{F}} = - \inlinedy{\mathsf{F}}$. Equation \ref{eq:mthd} is a true geometric definition of the Maxwell tensor, superior aesthetically and conceptually to the matrix representation we found first in Equation \ref{eq:mt}.

There's an important property of $\inlinedy{\mathsf{G}}$ we've yet to mention. Because $\vv b$ is a pseudovector that picks up an extra sign-flip under orientation-reversing coordinate transformations (see Footnote \ref{fn:ps})---and because $\vv e$ isn't---the act of ``swapping" $\tilde{\vv e}$ and $\tilde{\vv b}$ to get $\star \inlinedy{\mathsf{F}}$ makes the resulting Maxwell tensor $\inlinedy{\mathsf{G}}$ itself a \emph{pseudodyadic} that does the same. (Pseudoscalars, pseudovectors, and pseudodyadics are all examples of \emph{pseudotensors}, which, to be clear, is an altogether separate concept from the ``quasi-" objects we just worked with.) The Hodge dual of an ordinary antisymmetric four-dyadic is always an antisymmetric Minkowski pseudodyadic, and vice versa.

\subsubsection{Hodge Duality (a Bit) More Generally}

Hodge duality isn't specific to antisymmetric Minkowski dyadics. Other types of geometric objects have Hodge duals, too, and that includes vectors and scalars. The mathematical toolkit we're using isn't equipped to handle the more advanced facets of this topic---for that, something like differential forms or index notation is necessary---but a few remarks are in order.

In an $n$-dimensional space, the Hodge dual maps an antisymmetric tensor of rank $k$ to an antisymmetric \emph{pseudo}tensor of rank $n - k$, and vice versa (all vectors and scalars are regarded as antisymmetric in this context).\footnote{To get a feel for what antisymmetry means for higher-rank tensors, see \href{https://en.wikipedia.org/wiki/Levi-Civita_symbol\#/media/File:Epsilontensor.svg}{this illustration} (Wikipedia) that shows the Cartesian cubic-array representation of an antisymmetric rank-3 tensor in three-dimensional Euclidean space (that particular tensor is in fact $\star 1$, the rank-3 pseudotensor that's the Hodge dual of the scalar $1$).} The Hodge dual of the Hodge dual (the ``double Hodge") is either the original tensor or its additive inverse, depending not only on the values of $n$ and $k$, but also on the trace $s$ of the metric tensor's matrix-representation in an orthonormal coordinate system.\footnote{Here is a reminder that although we're only working with orthonormal coordinates in this paper---Cartesian coordinates in Euclidean space, and ($ct$ + Cartesian) coordinates in Minkowski spacetime---other coordinate systems are always possible. In orthonormal coordinates, the metric's matrix-representation is necessarily diagonal, with the diagonal entries limited to $1$'s and $-1$'s. Later, in Section \ref{ssec:dd}, we'll define the trace of a \emph{dyadic}, and that definition will \emph{not} simply be the trace of the dyadic's matrix representation.} For instance, $s = -2$ for four-dimensional Minkowski spacetime (trace of $\eta$ [Equation \ref{eq:mmt}]), and $s = 3$ for three-dimensional Euclidean space (trace of $I_3$, which is the Cartesian matrix representation of the unit dyadic $\inlinedy{\upiota}$ [metric tensor!]). Whether the double Hodge returns the original tensor or its additive inverse is then determined by the formula $\star \star = (-1)^{k(n-k) + (n-s)/2}$, which looks more complex than it is.

So in four-dimensional Minkowski spacetime ($n = 4$, $s = -2$), the Hodge dual maps rank-2 to rank-2 (dyadics to dyadics, as we've seen), rank-4 to rank-0 (scalars), rank-3 to rank-1 (vectors), and vice versa for all of the above, with the double Hodge returning the additive inverse of the original tensor in all but the odd-rank cases. But in three-dimensional Euclidean space ($n = 3$, $s = 3$), the Hodge dual operates between rank-3 and rank-0 and between rank-2 and rank-1, with the double Hodge \emph{always} returning the original tensor.

We've already tacitly used the Hodge dual of a three-vector many times. For example, in Section \ref{sssec:hd}, our quasi-four-dyadic $\{ \tilde{\vv w} \}$ housed the components of $ \vv m \wedge \vv n = \vv m \otimes \vv n - \vv n \otimes \vv m $, which turned out to be nothing but the components of $\pm ( \vv m \times \vv n )$. And we first encountered the $\pm ( \vv m \times \vv n )$ pattern all the way back in Section \ref{sssec:tp}, when we noted that both the positive and negative components of $\del \times \vv c$ were present in the components of $\vv a \times ( \del \times \vv c )$. The cross product and the three-vector wedge product contain exactly the same ``information." They are a ``Hodge pair":
\begin{equation*}
\star \star ( \vv m \wedge \vv n ) = \star ( \vv m \times \vv n ) = \vv m \wedge \vv n . 
\end{equation*}
More to the point, \emph{every} three-vector has an antisymmetric three-dyadic for a Hodge dual, whose six off-diagonal components are the positive and negative components of the vector in the pattern established in Section \ref{sssec:hd}. This includes the electric field:
\begin{equation*}
[ \star \vv e ]
=
\begin{bmatrix}
0 & e_z & -e_y \\
-e_z & 0 & e_x \\
e_y & -e_x & 0
\end{bmatrix} ,
\end{equation*}
and the magnetic field $\vv b = \del \times \vv a$, with $\star \vv b = \del \wedge \vv a$. Now we can represent the Faraday tensor with a nice ``block matrix" notation:
\begin{equation}\label{eq:ftc}
[ \capdy{\mathsf{F}} ]
=
\begin{bmatrix}
0 & [ - \vv e ]^\mathrm{T} \\[1ex]
[ \vv e ] & [ - \star \mkern-3mu \vv b ]
\end{bmatrix} ,
\end{equation}
and the Maxwell tensor:
\begin{equation}\label{eq:gtc}
[ \capdy{\mathsf{G}} ]
=
\begin{bmatrix}
0 & [ - \vv b ]^\mathrm{T} \\[1ex]
[ \vv b ] & [ \star \vv e ]
\end{bmatrix}
\end{equation}
(cf. Equations \ref{eq:emt} and \ref{eq:mt}).

To reiterate, each member of a Hodge pair is antisymmetric and contains the same information as its counterpart, but one is a pseudotensor. In the Hodge pair $\inlinedy{\mathsf{F}}$ and $\inlinedy{\mathsf{G}}$, the pseudotensor is the four-dyadic $\inlinedy{\mathsf{G}}$. In the Hodge pair $\vv e$ and $\star \vv e$, the pseudotensor is the three-dyadic $\star \vv e$. In the Hodge pair $\vv b$ and $\star \vv b$, the pseudotensor is the three-vector $\vv b$. (And $\star \star = - 1$ for the four-dyadics, but $\star \star = 1$ for the three-vectors/dyadics.)


\subsubsection{Divergencelessness of the Maxwell Tensor as an Identity}\label{sssec:dedv}

We're now in a position to appreciate that getting Equation \ref{eq:hme} ``for free" by virtue of the Faraday tensor's having the form $\partialup \wedge \vv A$ is an instance of a mathematical identity: for any four-vector field $\vv Q$, the Hodge dual of $\partialup \wedge \vv Q$ is divergenceless (i.e., $\partialup \cdot \, \star ( \partialup \wedge \vv Q ) = \textrm{\mbox{\boldmath $\emptyset$}}$). This rule is directly analogous to the divergencelessness of a curl in three-dimensional Euclidean space. Indeed, $\del \cdot \vv b = \del \cdot ( \del \times \vv a ) = 0$ is the time component of $\partialup \cdot \inlinedy{\mathsf{G}} = \textrm{\mbox{\boldmath $\emptyset$}}$, and we can even write it with the Hodge dual of the wedge product: $\del \cdot \star ( \del \wedge \vv a ) = 0$.

In the language of differential forms, both of these rules can be expressed as back-to-back applications of the \textbf{exterior derivative}, which is a generalization of our ``differential wedge product" ($\partialup \wedge \text{\underline{\hspace{.6em}}}$ or $\del \wedge \text{\underline{\hspace{.6em}}}$). A key property of this operator is that applying it twice results in zero identically.\footnote{This is sometimes called the \textbf{Poincar\'e lemma}, but often that term is reserved for something else (something closely related but distinct), so we won't use it.} The details are of course beyond our scope, but this property arises from antisymmetry and the fact that the order of mixed partials is reversible, and it's connected to the \textbf{generalized Stokes theorem} (a unification of the fundamental theorem of calculus and the various region/boundary integral rules like the divergence theorem). Underlying this connection is the topological principle that ``the boundary of a boundary is zero," as John Wheeler famously put it. Anyway, another manifestation of the vanishing double exterior derivative is the curl-lessness of the gradient, which can \emph{also} be written with the Hodge dual and the wedge product: $\del \times ( \del f ) = \star ( \del \wedge ( \del f ) ) = \vv 0 $. \emph{Because the order of mixed partials is reversible}, we have $\del \wedge ( \del f ) = ( \del \wedge \del ) f$ (write out the components explicitly if you need convincing), and a ``self--wedge product" like $ \del \wedge \del $ is obviously always zero ($\del \otimes \del - \del \otimes \del$). So this is a beautifully simple way to \emph{derive} the identity ${ \del \times \del f = \vv 0 }$. Naturally, the mixed-partial argument works just as well for the four-del: ${ \partialup \wedge ( \partialup f ) = ( \partialup \wedge \partialup ) f }$ vanishes.

\subsection{Light with Four-Dyadics}

In Section \ref{sssec:li}, we discussed electromagnetic plane waves in a manifestly covariant fashion by starting with Equation \ref{eq:fp} ($\vv J = \Box \vv A$). But that equation is valid only in the Lorenz gauge. We might wonder, then, whether it's possible to describe light in a way that's both manifestly covariant \emph{and gauge-invariant}. It is! First impose the free-space condition ($\vv J = \textrm{\mbox{\boldmath $\emptyset$}}$) on Equation \ref{eq:gfp}:
\begin{equation*}
\Box \vv A = \partialup(\partialup \cdot \vv A).
\end{equation*}
Next, take the d'Alembertian of the Faraday tensor (Equation \ref{eq:emta}) and sub in $\partialup(\partialup \cdot \vv A)$ for $\Box \vv A$, letting $\chi = \partialup \cdot \vv A$ for short:\footnote{We're verging on rank-3 territory now, since the d'Alembertian is the divergence of the gradient. This ``implied" gradient of a dyadic is as close as we've come to tensors beyond the second rank; any closer and we'd really need to introduce more flexible notation.}
\begin{equation}\label{eq:fthwe}
\begin{split}
\Box \capdy{\mathsf{F}} &= \Box ( \partialup \wedge \vv A ) \\
&= \partialup \wedge ( \Box \vv A) \\
&= \partialup \wedge (\partialup \chi) \\
&= ( \partialup \wedge \partialup ) \chi \\
\Box \capdy{\mathsf{F}} &= \capdy{\mathsf{0}} .
\end{split}
\end{equation}
Here $\inlinedy{\mathsf{0}}$ is the \textbf{zero four-dyadic} whose components are all zero. The step $\partialup \wedge (\partialup \chi) = (\partialup \wedge \partialup) \chi$ was discussed in Section \ref{sssec:dedv}. Equation \ref{eq:fthwe} is a homogeneous wave equation for the Faraday tensor in free space. Its general monochromatic plane-wave solution is (the real part of):
\begin{equation}\label{eq:ftwe}
\capdy{\mathsf{F}} (\vv R) = \bar{\capdy{\mathsf{F}}} \mathrm{e}^{\mathrm{i} \mkern.5mu \vv K \cdot \vv R} ,
\end{equation}
where the ``barred" amplitude four-dyadic is constant and antisymmetric, and where $\vv K = \langle K^{ct}, \vv k \rangle$ is the constant four-wavevector as before.

The solution is constrained by $\partialup \cdot \inlinedy{\mathsf{F}} = \textrm{\mbox{\boldmath $\emptyset$}}$ (Equation \ref{eq:ime} with $\vv J = \textrm{\mbox{\boldmath $\emptyset$}}$):
\begin{equation*}
\begin{split}
\textrm{\mbox{\boldmath $\emptyset$}} &= \partialup \cdot \bigl( \bar{\capdy{\mathsf{F}}} \mathrm{e}^{\mathrm{i} \mkern.5mu \vv K \cdot \vv R} \bigr) \\[2pt]
&= \left( \partialup \mathrm{e}^{\mathrm{i} \mkern.5mu \vv K \cdot \vv R} \right) \cdot \bar{\capdy{\mathsf{F}}} \\[2pt]
\textrm{\mbox{\boldmath $\emptyset$}} &= \vv K \cdot \bar{\capdy{\mathsf{F}}} .
\end{split}
\end{equation*}
If we label the ``barred" four-dyadic's components $\bar{e}_x$, $\bar{b}_x$, etc. (corresponding to their ``unbarred" counterparts in the Faraday tensor), and if we remember that $K^{ct} = k$ (because the four-wavevector is future-pointing lightlike), then carrying out the associated matrix multiplication gives us the conditions $\vv k \cdot \bar{\vv e} = 0$ and ${\bar{\vv e} = \bar{\vv b} \times \vv{\hat k}}$, matching part of what we found in Section \ref{sssec:li}. To show the rest ($\bar{e} = \bar{b}$ and $\bar{\vv b} \perp \vv k$), repeat the process with the Maxwell tensor and apply $\partialup \cdot \inlinedy{\mathsf{G}} = \textrm{\mbox{\boldmath $\emptyset$}}$ (Equation \ref{eq:hme}), yielding:
\begin{equation*}
\textrm{\mbox{\boldmath $\emptyset$}} = \vv K \cdot \bar{\capdy{\mathsf{G}}} ,
\end{equation*}
wherefrom $\vv k \cdot \bar{\vv b} = 0$ and ${\bar{\vv b} = \vv{\hat k}} \times \bar{\vv e}$. So everything checks out, and Equation \ref{eq:ftwe} is a manifestly covariant \emph{gauge-invariant} description of monochromatic electromagnetic plane waves.\footnote{You might note that the real part of Equation \ref{eq:ftwe} gives a \emph{cosine} function for $\vv e$ and $\vv b$, whereas Equations \ref{eq:bpw} and \ref{eq:epw} gave \emph{sine} functions. Strictly speaking, we should allow for a phase constant in the exponential argument.}

For the Lorenz gauge specifically, we can just combine Equations \ref{eq:pw} and \ref{eq:emta} (real part implied):
\begin{equation*}
\begin{split}
\capdy{\mathsf{F}} (\vv R) &= \partialup \wedge (  \bar{\vv A} \mathrm{e}^{\mathrm{i} \mkern.5mu \vv K \cdot \vv R} ) \\[2pt]
&= ( \partialup \mathrm{e}^{\mathrm{i} \mkern.5mu \vv K \cdot \vv R} ) \wedge \bar{\vv A}
\end{split}
\end{equation*}
(by a product rule that some matrix-ing can verify; $\bar{\vv A} = \langle \bar{A}^{ct}, \bar{\vv a} \rangle$ is constant, recall). Differentiating:
\begin{equation}\label{eq:emtl}
\capdy{\mathsf{F}} (\vv R) = \mathrm{i} \mathrm{e}^{\mathrm{i} \mkern.5mu \vv K \cdot \vv R} \, \vv K \wedge \bar{\vv A} .
\end{equation}
Let's make sure that the plane-wave solutions for $\vv e$ and $\vv b$ that we found in Section \ref{sssec:li} are embedded here. We'll reintroduce our shorthand $f = \vv K \cdot \vv R$. In matrix notation (using Equation \ref{eq:ftc}), our Lorenz-gauge Equation \ref{eq:emtl} is:
\begin{equation*}
[ \capdy{\mathsf{F}} ]
=
\begin{bmatrix}
0 & [ - \vv e]^\mathrm{T} \\[1ex]
[\vv e] & [ - \star \mkern-3mu \vv b ]
\end{bmatrix}
=
\mathrm{i} \mathrm{e}^{\mathrm{i} f}
\begin{bmatrix}
0 & [K^{ct} \bar{\vv a} - \bar{A}^{ct} \vv k ]^\mathrm{T} \mkern1mu \\[1ex]
[\bar{A}^{ct} \vv k - K^{ct} \bar{\vv a}] & [ \vv k \wedge \bar{\vv a} ] 
\end{bmatrix} .
\end{equation*}
This gives $\vv b = \mathrm{i} \mathrm{e}^{\mathrm{i} f} \, \bar{\vv a} \times \vv k$ and $\vv e = \mathrm{i} \mathrm{e}^{\mathrm{i} f} ( \bar{A}^{ct} \vv k - K^{ct} \bar{\vv a} )$, which were indeed steps on the way to Equations \ref{eq:bpw} and \ref{eq:epw}, and everything checks out again.


\subsection{Double Dot Products; Trace; Field Invariants}\label{ssec:dd}

If we have a pair of four-dyads $\vv Q \otimes \vv W$ and $\vv S \otimes \vv U$, we can make a Lorentz-invariant scalar from them like this:
\begin{equation*}
(\vv Q \otimes \vv W) : (\vv S \otimes \vv U) \equiv (\vv Q \cdot \vv S) (\vv W \cdot \vv U).
\end{equation*}
This commutative operation is called the \textbf{double dot product}. To extend the idea to non-dyad dyadics, we put them into sum-of-dyads form and invoke linearity:
\begin{equation*}
\begin{aligned}
&(\vv Q \otimes \vv W + \vv H \otimes \vv E + \dots) : (\vv S \otimes \vv U + \vv M \otimes \vv N + \dots) \\[3pt]
&\qquad = (\vv Q \otimes \vv W) : (\vv S \otimes \vv U) + (\vv Q \otimes \vv W) : (\vv M \otimes \vv N) + (\vv Q \otimes \vv W) : \, \dots \\
& \qquad \quad + (\vv H \otimes \vv E) : (\vv S \otimes \vv U) + (\vv H \otimes \vv E) : (\vv M \otimes \vv N) + (\vv H \otimes \vv E) : \, \dots \\
& \qquad \quad + \dots
\end{aligned}
\end{equation*}
Of course, if we already have the sixteen ``doubled up" ($ct$ + Cartesian) components of each input-dyadic, it may be easier to calculate with matrices directly than to put the dyadics into sum-of-dyads form and fuss with all that distribution. But what \emph{is} the matrix representation of the double dot product? For dyads it's clearly $[ (\vv Q \otimes \vv W) : (\vv S \otimes \vv U) ] = [\vv Q]^{\mathrm{T}} \eta [\vv S] [\vv W]^{\mathrm{T}} \eta [\vv U]$, a 1-by-1 matrix, but this won't help us for non-dyad dyadics (where would we plug them in?). It would be handy to have an alternative matrix representation of the double dot product for dyads, one that doesn't ``separate" an input-dyad's constituent vectors from each other.

The key is to bring in the matrix \emph{trace}, which is the sum of the main-diagonal entries of a square matrix (a 1-by-1 qualifies!). We'll take advantage of the fact that the trace of a matrix product is invariant under cyclic permutations of the multiplied matrices:
\begin{equation*}
\begin{aligned}
(\vv Q \otimes \vv W) : (\vv S \otimes \vv U) &= (\vv Q \cdot \vv S) (\vv W \cdot \vv U) \\
&= (\vv S \cdot \vv Q) (\vv W \cdot \vv U) \\
&= \mathrm{Tr} \left( [\vv S]^{\mathrm{T}} \eta [\vv Q] [\vv W]^{\mathrm{T}} \eta [\vv U] \right) \\[2pt]
&= \mathrm{Tr} \left( \eta [\vv Q] [\vv W]^{\mathrm{T}} \eta [\vv U] [\vv S]^{\mathrm{T}} \right) \\[2pt]
&= \mathrm{Tr} \left( \eta [\vv Q \otimes \vv W] \eta [\vv S \otimes \vv U]^\mathrm{T} \right).
\end{aligned}
\end{equation*}
\emph{Now} we have a matrix representation of the double dot product (several, actually) that we can use for arbitrary four-dyadics $\inlinedy{\mathsf{D}}$ and $\inlinedy{\mathsf{K}}$:
\begin{equation*}
\capdy{\mathsf{D}} : \capdy{\mathsf{K}} = \mathrm{Tr} \left( \eta [\capdy{\mathsf{D}}] \eta [ \capdy{\mathsf{K}} ]^{\mathrm{T}} \right) = \mathrm{Tr} \left( [\capdy{\mathsf{K}}] \eta^{\mathrm{T}} [ \capdy{\mathsf{D}} ]^{\mathrm{T}} \eta^{\mathrm{T}} \right) = \mathrm{Tr} \left( \eta [\capdy{\mathsf{K}}] \eta [ \capdy{\mathsf{D}} ]^{\mathrm{T}} \right) = \capdy{\mathsf{K}} : \capdy{\mathsf{D}} ,
\end{equation*}
where we've transposed the matrix product to show that commutativity still holds (the transpose of a matrix product is the product of the transposes in reverse order, and the symmetric $\eta$ is its own transpose). This works because a matrix and its transpose share a trace. Trivially from the above, then:
\begin{equation*}
\capdy{\mathsf{D}} \mkern0mu ^\mathrm{T} : \capdy{\mathsf{K}} \mkern0mu ^\mathrm{T} = \mathrm{Tr} \left( \eta [\capdy{\mathsf{D}}]^{\mathrm{T}} \eta [ \capdy{\mathsf{K}} ] \right) = \capdy{\mathsf{D}} : \capdy{\mathsf{K}} .
\end{equation*}
A special case is when a dyadic is double-dotted with $\inlinedy{\upeta}$, the Minkowski metric dyadic (whose matrix representation in every frame is $\eta$):
\begin{equation*}
\begin{split}
\capdy{\mathsf{D}} : \inlinedy{\upeta} &= \mathrm{Tr} \left( \eta [\capdy{\mathsf{D}}] \eta [ \mkern1mu \inlinedy{\upeta} \mkern.5mu ]^{\mathrm{T}} \right) \\
&= \mathrm{Tr} \left( \eta [\capdy{\mathsf{D}}] \right) = \mathsfit D^{ct \mkern1.5mu ct} - \mathsfit D^{xx} - \mathsfit D^{yy} - \mathsfit D^{zz}
\end{split}
\end{equation*}
(verify that $\eta \eta = I_4$, the 4-by-4 identity matrix). This invariant is called the trace \emph{of the dyadic} $\inlinedy{\mathsf{D}}$, though its matrix representation is $\mathrm{Tr} (\eta [ \inlinedy{\mathsf{D}} ])$, \emph{not} $\mathrm{Tr} ( [ \inlinedy{\mathsf{D}} ] )$. For example, the trace of $\inlinedy{\upeta}$ is $\mathrm{Tr} (I_4) = 4$. Obviously all antisymmetric dyadics are traceless (their matrix representations have only zeros on the main diagonal). More generally, the double dot product of an antisymmetric dyadic $\inlinedy{\mathsf{A}}$ with \emph{any symmetric} dyadic $\inlinedy{\mathsf{S}}$ is zero, since $\inlinedy{\mathsf{A}} = - \inlinedy{\mathsf{A}} \mkern-2mu ^\mathrm{T}$ and $\inlinedy{\mathsf{S}} = \inlinedy{\mathsf{S}} \mkern-2mu^\mathrm{T}$:
\begin{equation*}
\capdy{\mathsf{A}} : \capdy{\mathsf{S}} = ( - \capdy{\mathsf{A}} \mkern0mu ^\mathrm{T} ) : ( + \capdy{\mathsf{S}} \mkern0mu ^\mathrm{T} ) = - ( \capdy{\mathsf{A}} \mkern0mu ^\mathrm{T} : \capdy{\mathsf{S}} \mkern0mu ^\mathrm{T} ) = - ( \capdy{\mathsf{A}} : \capdy{\mathsf{S}} )
\end{equation*}
(if it's equal to its additive inverse then it must be zero).

An immediate application of the double dot product is to discover new(?) Lorentz invariants by double-dotting the Faraday and Maxwell tensors (Equations \ref{eq:emt} and \ref{eq:mt}) with each other and with themselves. First:
\begin{equation}\label{eq:fddg}
\begin{split}
\capdy{\mathsf{F}} : \capdy{\mathsf{G}} &= \mathrm{Tr} \left( \eta [ \capdy{\mathsf{F}} ] \eta [ \capdy{\mathsf{G}} ]^{\mathrm{T}} \right) \\[5pt]
&=
\mathrm{Tr}
\left(
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\
-e_x & 0 & b_z & -b_y \\
-e_y & -b_z & 0 & b_x \\
-e_z & b_y & -b_x & 0
\end{bmatrix}
\begin{bmatrix}
0 & b_x & b_y & b_z \\
b_x & 0 & e_z & -e_y \\
b_y & -e_z & 0 & e_x \\
b_z & e_y & -e_x & 0
\end{bmatrix}
\right) \\[5pt]
\capdy{\mathsf{F}} : \capdy{\mathsf{G}} &= -4 (\vv e \cdot \vv b).
\end{split}
\end{equation}
That's \emph{not} new---except for the factor of $-4$, it's one of the two invariants we made from $\vv e$ and $\vv b$ back in Section \ref{sssec:lff}, but now we see where it comes from geometrically (and now we see that the double dot product of an ordinary dyadic and a pseudodyadic is a pseudoscalar). The other invariant was $e^2 - b^2$, and sure enough it's what we get when we double-dot $\inlinedy{\mathsf{F}}$ or $\inlinedy{\mathsf{G}}$ with \emph{itself}:
\begin{equation}\label{eq:fddf}
\begin{split}
\capdy{\mathsf{F}} : \capdy{\mathsf{F}} &= \mathrm{Tr} \left( \eta [ \capdy{\mathsf{F}} ] \eta [ \capdy{\mathsf{F}} ]^{\mathrm{T}} \right) \\[5pt]
&=
\mathrm{Tr}
\left(
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\
-e_x & 0 & b_z & -b_y \\
-e_y & -b_z & 0 & b_x \\
-e_z & b_y & -b_x & 0
\end{bmatrix}
\begin{bmatrix}
0 & e_x & e_y & e_z \\
e_x & 0 & -b_z & b_y \\
e_y & b_z & 0 & -b_x \\
e_z & -b_y & b_x & 0
\end{bmatrix}
\right) \\[5pt]
\capdy{\mathsf{F}} : \capdy{\mathsf{F}} &= 2(b^2 - e^2),
\end{split}
\end{equation}
and:
\begin{equation}\label{eq:gddg}
\begin{split}
\capdy{\mathsf{G}} : \capdy{\mathsf{G}} &= \mathrm{Tr} \left( \eta [ \capdy{\mathsf{G}} ] \eta [ \capdy{\mathsf{G}} ]^{\mathrm{T}} \right) \\[5pt]
&=
\mathrm{Tr}
\left(
\begin{bmatrix}
0 & -b_x & -b_y & -b_z \\
-b_x & 0 & -e_z & e_y \\
-b_y & e_z & 0 & -e_x \\
-b_z & -e_y & e_x & 0
\end{bmatrix}
\begin{bmatrix}
0 & b_x & b_y & b_z \\
b_x & 0 & e_z & -e_y \\
b_y & -e_z & 0 & e_x \\
b_z & e_y & -e_x & 0
\end{bmatrix}
\right) \\[5pt]
\capdy{\mathsf{G}} : \capdy{\mathsf{G}} &= 2 (e^2 - b^2)
\end{split}
\end{equation}
(double-dotting two pseudodyadics gives an ordinary scalar).

There's another Lorentz invariant we can make from a pair of four-dyads:
\begin{equation*}
(\vv Q \otimes \vv W) \cdot \cdot \, \mkern1mu (\vv S \otimes \vv U) \equiv (\vv Q \cdot \vv U) (\vv W \cdot \vv S) ,
\end{equation*}
where again the matrix trace is convenient for extending the operation to arbitrary four-dyadics $\inlinedy{\mathsf{D}}$ and $\inlinedy{\mathsf{K}}$ (same line of reasoning as before):
\begin{equation*}
\capdy{\mathsf{D}} \cdot \cdot \, \mkern1mu \capdy{\mathsf{K}} = \mathrm{Tr} \left( \eta [\capdy{\mathsf{D}}] \eta [ \capdy{\mathsf{K}} ] \right) = \mathrm{Tr} \left( [ \capdy{\mathsf{K}} ]^\mathrm{T} \eta [\capdy{\mathsf{D}}]^\mathrm{T} \eta \right) = \capdy{\mathsf{D}} \mkern0mu ^\mathrm{T} \mkern-2mu \cdot \cdot \, \mkern1mu \capdy{\mathsf{K}} \mkern0mu ^\mathrm{T} .
\end{equation*}
This second ``kind" of double dot product is likewise commutative, and in general it produces an altogether different scalar than the first kind does. But if at least one of the input-dyadics is symmetric then $\inlinedy{\mathsf{D}}  \cdot \cdot \inlinedy{\mathsf{K}} = \inlinedy{\mathsf{D}} : \inlinedy{\mathsf{K}}$ (because we can replace the symmetric dyadic with its transpose), and if at least one is antisymmetric then $\inlinedy{\mathsf{D}}  \cdot \cdot \inlinedy{\mathsf{K}} = - \inlinedy{\mathsf{D}} : \inlinedy{\mathsf{K}}$ (because we can replace the antisymmetric dyadic with its \emph{negative} transpose). Both the Faraday and Maxwell tensors are antisymmetric, so we don't get new electromagnetic invariants with the second kind of double dot product; rather, ${\inlinedy{\mathsf{F}}  \cdot \cdot \inlinedy{\mathsf{G}} = - \inlinedy{\mathsf{F}} : \inlinedy{\mathsf{G}}}$, and so on. 

\subsection{Single-Dotting Dyadics; Stress--Energy Tensors}

\subsubsection{Single-Dotting Dyadics; EM Stress--Energy Tensor}\label{sssec:sdfd}

We have a dot product between four-vectors, dot products between four-dyadics and four-vectors, and \emph{double} dot products between four-dyadics, but what about just a ``single" dot product between four-dyadics? If we have a dyad $\vv Q \otimes \vv W$ that we'd like to ``dot" with another dyad $\vv S \otimes \vv U$, our matrix notation (with $\eta$ for the dot!) naturally suggests:
\begin{equation*}
\begin{aligned}
\mkern0mu [ \vv Q \otimes \vv W ] \eta [ \vv S \otimes \vv U ] &= [\vv Q][\vv W]^{\mathrm{T}} \eta [\vv S][\vv U]^{\mathrm{T}} \\
&= (\vv W \cdot \vv S) \, [\vv Q] [\vv U]^{\mathrm{T}} \\[3pt]
(\vv Q \otimes \vv W) \cdot (\vv S \otimes \vv U) & \equiv (\vv W \cdot \vv S) (\vv Q \otimes \vv U) .
\end{aligned}
\end{equation*}
So the dot product between two four-dyads generates a new dyad, and the operation isn't commutative. If we want to dot dyadics that aren't simple dyads, we may put them in sum-of-dyads form and distribute:
\begin{equation*}
\begin{aligned}
&(\vv Q \otimes \vv W + \vv H \otimes \vv E + \dots ) \cdot (\vv S \otimes \vv U + \vv M \otimes \vv N + \dots ) \\[3pt]
&\qquad = (\vv Q \otimes \vv W) \cdot (\vv S \otimes \vv U) + (\vv Q \otimes \vv W) \cdot (\vv M \otimes \vv N) + (\vv Q \otimes \vv W) \cdot \, \dots \\
& \qquad \quad + (\vv H \otimes \vv E) \cdot (\vv S \otimes \vv U) + (\vv H \otimes \vv E) \cdot (\vv M \otimes \vv N) + (\vv H \otimes \vv E) \cdot \, \dots \\
& \qquad \quad + \dots
\end{aligned}
\end{equation*}
(a new dyadic). If we already know the components of the input-dyadics $\inlinedy{\mathsf{D}}$ and $\inlinedy{\mathsf{K}}$, it may be easier to calculate with matrices directly:
\begin{equation*}
[\capdy{\mathsf{D}} \cdot \capdy{\mathsf{K}}] = [\capdy{\mathsf{D}}] \eta [\capdy{\mathsf{K}}] .
\end{equation*}
We see that $\inlinedy{\mathsf{D}} \cdot \inlinedy{\upeta} = \inlinedy{\upeta} \cdot \inlinedy{\mathsf{D}} =  \inlinedy{\mathsf{D}}$, further ``justifying" the notion that the Minkowski metric dyadic is like the unit dyadic of spacetime. We also have $(\inlinedy{\mathsf{D}} \cdot \inlinedy{\mathsf{K}}) : \inlinedy{\upeta} = \inlinedy{\mathsf{D}} \mkern1mu \cdot \cdot \inlinedy{\mathsf{K}}$, which equals $- \inlinedy{\mathsf{D}} : \inlinedy{\mathsf{K}}$ in the case that $\inlinedy{\mathsf{D}}$ or $\inlinedy{\mathsf{K}}$ is antisymmetric (relevant for the Faraday and Maxwell tensors). Along the same lines, $ \inlinedy{\mathsf{A}} \mkern-2mu ^\mathrm{T} \cdot \inlinedy{\mathsf{D}} = - \inlinedy{\mathsf{A}} \cdot \inlinedy{\mathsf{D}}$ and $\inlinedy{\mathsf{D}} \cdot \inlinedy{\mathsf{A}} \mkern-2mu ^\mathrm{T} = - \inlinedy{\mathsf{D}} \cdot \inlinedy{\mathsf{A}}$ for antisymmetric $\inlinedy{\mathsf{A}}$.

Let's test out the operation on the Faraday and Maxwell tensors (Equations \ref{eq:emt} and \ref{eq:mt}). First $\inlinedy{\mathsf{F}} \cdot \inlinedy{\mathsf{G}}$:
\begin{equation*}
\begin{aligned}
\mkern0mu [\capdy{\mathsf{F}}] \eta [\capdy{\mathsf{G}}]
&=
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\
e_x & 0 & -b_z & b_y \\
e_y & b_z & 0 & -b_x \\
e_z & -b_y & b_x & 0
\end{bmatrix}
\begin{bmatrix}
0 & -b_x & -b_y & -b_z \\
-b_x & 0 & -e_z & e_y \\
-b_y & e_z & 0 & -e_x \\
-b_z & -e_y & e_x & 0
\end{bmatrix} \\[5pt]
&=
\begin{bmatrix}
\vv e \cdot \vv b & 0 & 0 & 0 \\
0 & - \vv e \cdot \vv b & 0 & 0 \\
0 & 0 & - \vv e \cdot \vv b & 0 \\
0 & 0 & 0 & - \vv e \cdot \vv b
\end{bmatrix}
=
- \dfrac{1}{4} \, ( \capdy{\mathsf{F}} : \capdy{\mathsf{G}} ) \eta
\end{aligned}
\end{equation*}
(we've used Equation \ref{eq:fddg}), and $\inlinedy{\mathsf{F}} \cdot \inlinedy{\mathsf{G}} = - ( \inlinedy{\mathsf{F}} : \inlinedy{\mathsf{G}} ) \inlinedy{\upeta} / 4$, a symmetric pseudodyadic. The ``inverse" product $\inlinedy{\mathsf{G}} \cdot \inlinedy{\mathsf{F}}$ actually gives the same pseudodyadic, even though the dot product between dyadics isn't generally commutative.

Now $\inlinedy{\mathsf{F}} \cdot \inlinedy{\mathsf{F}}$:
\begin{equation*}
\begin{aligned}
\mkern0mu [\capdy{\mathsf{F}}] \eta [\capdy{\mathsf{F}}]
&=
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\
e_x & 0 & -b_z & b_y \\
e_y & b_z & 0 & -b_x \\
e_z & -b_y & b_x & 0
\end{bmatrix}
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\
-e_x & 0 & b_z & -b_y \\
-e_y & -b_z & 0 & b_x \\
-e_z & b_y & -b_x & 0
\end{bmatrix} \\[5pt]
&=
\begin{bmatrix}
e^2 & (e_y b_z - e_z b_y) & (e_z b_x - e_x b_z) & (e_x b_y - e_y b_x) \\[1ex]
(e_y b_z - e_z b_y) & (b_y^2 + b_z^2 - e_x^2) & (-e_x e_y - b_x b_y) & (- e_x e_z - b_x b_z) \\[1ex]
(e_z b_x - e_x b_z) & (-e_x e_y - b_x b_y) & (b_z^2 + b_x^2 - e_y^2)  & (- e_y e_z - b_y b_z) \\[1ex]
(e_x b_y - e_y b_x) & (- e_x e_z - b_x b_z) & (- e_y e_z - b_y b_z) & (b_x^2 + b_y^2 - e_z^2)
\end{bmatrix} ,
\end{aligned}
\end{equation*}
or:
\begin{equation*}
\mkern0mu [\capdy{\mathsf{F}}] \eta [\capdy{\mathsf{F}}]
=
\begin{bmatrix}
e^2 & s_x & s_y & s_z \\[1ex]
s_x & (b^2 - b_x^2 - e_x^2) & \sigma_{x y} & \sigma_{x z} \\[1ex]
s_y & \sigma_{y x} & (b^2 - b_y^2 - e_y^2)  & \sigma_{y z} \\[1ex]
s_z & \sigma_{z x} & \sigma_{z y} & (b^2 - b_z^2 - e_z^2) 
\end{bmatrix} ,
\end{equation*}
where $\vv s = \vv e \times \vv b$ (Poynting vector), $\inlinedy{\upsigma} = u \mkern-5mu \inlinedy{\upiota} - \, \vv e \otimes \vv e - \vv b \otimes \vv b$ (Maxwell stress tensor), $u = (e^2 + b^2) / 2$ (electromagnetic energy density), and $\inlinedy{\upiota}$ is the Euclidean unit dyadic. Similarly, $\inlinedy{\mathsf{G}} \cdot \inlinedy{\mathsf{G}}$:
\begin{equation*}
\begin{aligned}
\mkern0mu [\capdy{\mathsf{G}}] \eta [\capdy{\mathsf{G}}]
&=
\begin{bmatrix}
0 & -b_x & -b_y & -b_z \\
b_x & 0 & e_z & -e_y \\
b_y & -e_z & 0 & e_x \\
b_z & e_y & -e_x & 0
\end{bmatrix}
\begin{bmatrix}
0 & -b_x & -b_y & -b_z \\
-b_x & 0 & -e_z & e_y \\
-b_y & e_z & 0 & -e_x \\
-b_z & -e_y & e_x & 0
\end{bmatrix} \\[5pt]
&=
\begin{bmatrix}
b^2 & s_x & s_y & s_z \\[1ex]
s_x & (e^2 - e_x^2 - b_x^2) & \sigma_{x y} & \sigma_{x z} \\[1ex]
s_y & \sigma_{y x} & (e^2 - e_y^2 - b_y^2)  & \sigma_{y z} \\[1ex]
s_z & \sigma_{z x} & \sigma_{z y} & (e^2 - e_z^2 - b_z^2) 
\end{bmatrix}
\end{aligned}
\end{equation*}
(not a pseudodyadic). The symmetric dyadics $\inlinedy{\mathsf{F}} \cdot \inlinedy{\mathsf{F}}$ and $\inlinedy{\mathsf{G}} \cdot \inlinedy{\mathsf{G}}$ are begging to be summed!
\begin{equation*}
[\capdy{\mathsf{F}}] \eta [\capdy{\mathsf{F}}] + [\capdy{\mathsf{G}}] \eta [\capdy{\mathsf{G}}]
= 2
\begin{bmatrix}
 u &  s_x &  s_y &  s_z \\[1ex]
 s_x &  \sigma_{x x} &  \sigma_{x y} &  \sigma_{x z} \\[1ex]
 s_y &  \sigma_{y x} &  \sigma_{yy}  &  \sigma_{y z} \\[1ex]
 s_z &  \sigma_{z x} &  \sigma_{z y} &  \sigma_{zz}
\end{bmatrix}.
\end{equation*}
Define:
\begin{equation}\label{eq:eset}
\boxed { \capdy{\mathsf{T}}_{\textrm{em}} \equiv \dfrac{1}{2} \, ( \capdy{\mathsf{F}} \cdot \capdy{\mathsf{F}} \, + \, \capdy{\mathsf{G}} \cdot \capdy{\mathsf{G}} ) } \, ,
\end{equation}
and we have a four-dyadic that combines $u$, $\vv s$, and $\inlinedy{\upsigma}$:
\begin{equation*}
[\capdy{\mathsf{T}}_{\textrm{em}}]
=
\begin{bmatrix}
u & [\vv s]^{\mathrm{T}} \\[1ex]
[\vv s] & [ \, \mkern-2mu \inlinedy{\upsigma} \mkern1mu ]
\end{bmatrix} .
\end{equation*}
This dyadic is symmetric and also traceless (${\inlinedy{\mathsf{T}}} _{\textrm{em}} : \inlinedy{\upeta} = 0$):
\begin{equation*}
\begin{split}
2 \mkern1mu \capdy{\mathsf{T}}_{\textrm{em}} : \inlinedy{\upeta} &= ( \capdy{\mathsf{F}} \cdot \capdy{\mathsf{F}}  ) : \inlinedy{\upeta} \, + \; ( \capdy{\mathsf{G}} \cdot \capdy{\mathsf{G}}  ) : \inlinedy{\upeta} \\
&= \capdy{\mathsf{F}} \cdot \cdot \, \mkern1mu \capdy{\mathsf{F}} \, + \, \capdy{\mathsf{G}} \cdot \cdot \, \mkern1mu \capdy{\mathsf{G}} \\
&=  - \capdy{\mathsf{F}} : \capdy{\mathsf{F}} - \capdy{\mathsf{G}} : \capdy{\mathsf{G}} ,
\end{split}
\end{equation*}
which vanishes by Equations \ref{eq:fddf} and \ref{eq:gddg}.\footnote{\label{fn:tlt}This tracelessness turns out to reflect the masslessness of the electromagnetic field.} Note that to get from $[\inlinedy{\mathsf{F}} \cdot \inlinedy{\mathsf{F}}]$ to $[{\inlinedy{\mathsf{T}}} _{\textrm{em}}]$ requires only the addition of the invariant $(b^2 - e^2) / 2 = ( \inlinedy{\mathsf{F}} : \inlinedy{\mathsf{F}} ) / 4$ to the top-left entry and $( \inlinedy{\mathsf{G}} : \inlinedy{\mathsf{G}} ) / 4 = - ( \inlinedy{\mathsf{F}} : \inlinedy{\mathsf{F}} ) / 4$ to the other three entries on the main diagonal. Likewise for getting from $[ \inlinedy{\mathsf{G}} \cdot \inlinedy{\mathsf{G}} ]$ to $[ {\inlinedy{\mathsf{T}}} _{\textrm{em}} ]$, but with the signs of the added invariants reversed. So there are several alternative ways to express ${\inlinedy{\mathsf{T}}} _{\textrm{em}}$ in terms of the Faraday and Maxwell tensors, all equivalent to Equation \ref{eq:eset}. Here are a few of them:
\begin{equation}\label{eq:eseta}
\begin{aligned}
\capdy{\mathsf{T}}_{\textrm{em}} &= \capdy{\mathsf{F}} \cdot \capdy{\mathsf{F}} +  \dfrac{1}{4} \, ( \capdy{\mathsf{F}} : \capdy{\mathsf{F}} ) \inlinedy{\upeta} \, = \, \capdy{\mathsf{F}} \cdot \capdy{\mathsf{F}} -  \dfrac{1}{4} \, ( \capdy{\mathsf{F}} \cdot \cdot \, \mkern1mu \capdy{\mathsf{F}} ) \inlinedy{\upeta} \\[5pt]
&= \capdy{\mathsf{G}} \cdot \capdy{\mathsf{G}} +  \dfrac{1}{4} \, ( \capdy{\mathsf{G}} : \capdy{\mathsf{G}} ) \inlinedy{\upeta}  \, = \,  \capdy{\mathsf{G}} \cdot \capdy{\mathsf{G}} -  \dfrac{1}{4} \, ( \capdy{\mathsf{G}} \cdot \cdot \, \capdy{\mathsf{G}} ) \inlinedy{\upeta}  \\[5pt]
&= \capdy{\mathsf{F}} \cdot \capdy{\mathsf{F}} -  \dfrac{1}{4} \, ( \capdy{\mathsf{G}} : \capdy{\mathsf{G}} ) \inlinedy{\upeta} \, = \, \capdy{\mathsf{G}} \cdot \capdy{\mathsf{G}} +  \dfrac{1}{4} \, ( \capdy{\mathsf{F}} \cdot \cdot \, \mkern1mu \capdy{\mathsf{F}} ) \inlinedy{\upeta}
\end{aligned}
\end{equation}
(expressions with just $\inlinedy{\mathsf{F}}$ are most common in the literature).\footnote{Because the Faraday and Maxwell tensors are antisymmetric, we have relationships like $\footnotedy{\mathsf{F}} \cdot \footnotedy{\mathsf{F}} \mkern-2mu^\mathrm{T} = \footnotedy{\mathsf{F}} \mkern-2mu^\mathrm{T} \cdot \footnotedy{\mathsf{F}} = - \footnotedy{\mathsf{F}} \cdot \footnotedy{\mathsf{F}}$, and we get even more expressions for ${\footnotedy{\mathsf{T}}}_{\textrm{em}}$ if we throw those into the mix. The signs of terms would also be affected if we'd chosen the ($-$$+$$+$$+$) sign convention instead of ($+$$-$$-$$-$). All this is to say, don't be alarmed to find different ``variations" on how ${\footnotedy{\mathsf{T}}}_{\textrm{em}}$ is written in other sources; they're reconcilable.}

Our new dyadic ${\inlinedy{\mathsf{T}}} _{\textrm{em}}$ is the \textbf{electromagnetic stress--energy tensor}. Its significance is revealed when we closely examine Equation \ref{eq:py} (Poynting's theorem, a statement of energy conservation) and Equation \ref{eq:tmc} (the corresponding statement of three-momentum conservation). Together, these expressions convey exactly the same information as:
\begin{equation}\label{eq:fmc}
\boxed{ \partialup \cdot \capdy{\mathsf{T}}_{\textrm{em}} = - \bm{\mathfrak{F}} } \,
\end{equation}
($\bm{\mathfrak{F}}$ being the Lorentz four-force density), because in matrix notation that's:
\begin{equation}\label{eq:fmcm}
\begin{bmatrix}
\partial^{ct} & [ \del ]^{\mathrm{T}} \mkern1mu
\end{bmatrix}
\begin{bmatrix}
u & [\vv s]^{\mathrm{T}} \\[1ex]
[\vv s] & [ \, \mkern-2mu \inlinedy{\upsigma} \mkern1mu ]
\end{bmatrix}
=
\begin{bmatrix}
- \dfrac{\dd \mathcal{P}_{\textrm{L}}}{\dd V} & \left[ - \dfrac{\dd \vv f_{\mathrm{L}}}{\dd V} \right] ^{\mkern-2mu \mathrm{T} \; }
\end{bmatrix}
\end{equation}
(see Equation \ref{eq:lffdc}). Equation \ref{eq:fmc} is a statement of the conservation of \emph{four-momentum} that explicitly accounts for the electromagnetic field. It's equivalent to Equation \ref{eq:lffd}, but our dyadic notation here is more transparent. The four-divergence of ${\inlinedy{\mathsf{T}}}_{\textrm{em}}$ vanishes when the fields are transferring no four-momentum to charged particles. We can express the right side of Equation \ref{eq:fmc} in terms of the fields and charges using $\bm{\mathfrak{F}} = \inlinedy{\mathsf{F}} \cdot \mkern2mu \vv J$ (Equation \ref{eq:lffdfd}):
\begin{equation*}
\partialup \cdot \capdy{\mathsf{T}}_{\textrm{em}} = \vv J \cdot \capdy{\mathsf{F}}
\end{equation*}
(by antisymmetry), or in terms of the fields alone (Equation \ref{eq:ime}):
\begin{equation*}
\partialup \cdot \capdy{\mathsf{T}}_{\textrm{em}} = ( \partialup \cdot \capdy{\mathsf{F}} ) \cdot \capdy{\mathsf{F}} .
\end{equation*}


\subsubsection{Total Stress--Energy Tensor; Four-Momentum Conservation}\label{sssec:tset}

We didn't set out looking for the electromagnetic stress--energy tensor. Almost serendipitously, it and its role in the conservation of the field's four-momentum (Equation \ref{eq:fmc}) revealed themselves to us when we experimented with the dyadic dot product. How convenient! We now deduce the existence of a more general stress--energy tensor---a dyadic field we'll label just ${\inlinedy{\mathsf{T}}}$---that satisfies a \emph{continuity equation} (see Section \ref{sssec:cont}), expressing the (local) conservation of \emph{all} four-momentum, electromagnetic and otherwise:
\begin{equation}\label{eq:set}
\partialup \cdot \capdy{\mathsf{T}} = \textrm{\mbox{\boldmath $\emptyset$}} \, .
\end{equation}
This ``total" stress--energy tensor assumes great importance in general relativity, as the gravitational source field. It's always symmetric but \emph{not} generally traceless (see Footnote \ref{fn:tlt}).

Although the \emph{electromagnetic} stress--energy tensor can be constructed from other quantities (Equations \ref{eq:eset} and \ref{eq:eseta}), there does not exist a general ``formula" for ${\inlinedy{\mathsf{T}}}$.\footnote{\label{fn:setgr}Actually, in general relativity, there \emph{is} a formula---a definition, rather---involving the functional derivative of the ``matter Lagrangian density" with respect to the metric tensor. But there's no general recipe from other physical quantities.} There are formulas for special cases, like ``dust" and perfect fluids, but ultimately the stress--energy tensor at a given event simply is what it is, determined by the configuration and interactions of whatever's there. For a given inertial observer, however, the \emph{components} of ${\inlinedy{\mathsf{T}}}$ always have an unambiguous meaning, and the meaning is the same as it is in the electromagnetic case: $\mathsfit T^{ct \mkern1.5mu ct}$ is the energy density $\dd E / \dd V$, $\mathsfit T^{ct \mkern1.5mu x}$ is the density of the $x$-component of three-momentum $\dd (c p_x) / \dd V$ (etc.),$\mathsfit T^{x \mkern1.5mu ct}$ is the flow (or flux density) of energy in the $x$-direction (etc., and by symmetry equal to $\mathsfit T^{ct \mkern1.5mu x}$),\footnote{Many sources give the opposite interpretation of $\mathsfit T^{ct \mkern1.5mu x}$ and $\mathsfit T^{x \mkern1.5mu ct}$. By symmetry it doesn't really matter, but our interpretation comes from how we wrote Equation \ref{eq:fmcm} (the quantities involved are explained after Equations \ref{eq:py} and \ref{eq:tmc}).} and then the ``space--space" components give the flow of each Cartesian component of three-momentum in each Cartesian direction (i.e., normal and shear stresses, which we won't elaborate on).

It follows that we can ``extract" the four-momentum $\vv P$ from the total stress--energy tensor. Start with $\vv B \cdot {\inlinedy{\mathsf{T}}} $, where $\vv B$ is some inertial observer's four-velocity, and calculate \emph{in that observer's rest frame}, such that ${ \vv B = \langle 1, \vv 0 \rangle }$:
\begin{equation*}
\begin{aligned}
\, [ \vv B ]^{\mathrm{T}} \eta [ \capdy{\mathsf{T}}  ]
&=
\begin{bmatrix}
1 & 0 & 0 & 0
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & -1 & 0 \\
0 & 0 & 0 & -1
\end{bmatrix}
\begin{bmatrix}
\mathsfit T^{ct \mkern1.5mu ct} & \mathsfit T^{ct \mkern1.5mu x} & \mathsfit T^{ct \mkern1.5mu y} & \mathsfit T^{ct \mkern1.5mu z} \\
\mathsfit T^{x \mkern1.5mu ct} & \mathsfit T^{x x} & \mathsfit T^{x y} & \mathsfit T^{x z}  \\
\mathsfit T^{y \mkern1.5mu ct} & \mathsfit T^{y x} & \mathsfit T^{y y} & \mathsfit T^{y z}  \\
\mathsfit T^{z \mkern1.5mu ct} & \mathsfit T^{z x} & \mathsfit T^{z y} & \mathsfit T^{z z} 
\end{bmatrix}
\\[3pt]
&=
\begin{bmatrix}
\mathsfit T^{ct \mkern1.5mu ct} & \mathsfit T^{ct \mkern1.5mu x} & \mathsfit T^{ct \mkern1.5mu y} & \mathsfit T^{ct \mkern1.5mu z}
\end{bmatrix}
\\[3pt]
&=
\left[ \dfrac{\dd \vv P}{\dd V} \right]^{\mathrm{T}}
\end{aligned}
\end{equation*}
(by the meaning of the relevant components established in the previous paragraph). This is \emph{not} a manifestly covariant expression, since $V$ is frame-dependent (more on this below). But it's clear that \emph{in this observer's rest frame}, the total four-momentum in a given spatial region at a single moment of coordinate time is $\int_V (\vv B \cdot {\inlinedy{\mathsf{T}}}) \dd V$, and it therefore must be the case that this volume-integral is indeed Lorentz-covariant (a four-vector), despite appearances.

There are actually two distinct things that someone might mean by the statement that the quantity ${\int_V (\vv B \cdot {\inlinedy{\mathsf{T}}}) \dd V}$ is Lorentz-covariant. They both turn out to be true.

The first, which we've demonstrated, is almost trivial but also a little subtle. The almost-trivial part is that if we perform a Lorentz boost and simply transform $V$ and the components of the tensors in the integrand, then in the new ``primed" frame the result will be the same four-vector (with different components). The subtlety is that the \emph{interpretation} of this four-vector in the primed frame will differ, because the transformation robs the integral's ``ingredients" of their straightforward physical significance: $\vv B$ won't have components $\langle 1, \vv 0 \rangle$ (it isn't the primed frame's four-velocity), the components of the integrand $\vv B \cdot {\inlinedy{\mathsf{T}}}$ will therefore include various combinations of the stress--energy tensor's components (not just the energy and momentum densities from the first row of $[{\inlinedy{\mathsf{T}}}]$), and, crucially, the ``volume" $V^{\prime}$ that's integrated over won't be purely spatial (it describes the same geometric \emph{spacetime region} as the unprimed $V$, but by the relativity of simultaneity it must acquire some temporal ``extent" from the boost). In the primed frame, then, the resultant four-vector will be the same geometric object, but it will represent the total ``something related to four-momentum that isn't four-momentum" within a region of space over some period of time. Probably the best interpretation is ``the four-vector that the unprimed observer interprets as the total four-momentum in their purely spatial volume $V$, which is the same patch of spacetime as my not-purely-spatial $V^{\prime}$." Not a quantity that the primed observer is likely to care much about.

It's important to understand that the spacetime region just considered \emph{is} three-dimensional, even though it has extent along all four primed coordinate axes. If that confuses you, think of a plane in three-dimensional Euclidean space that doesn't happen to be parallel to any of the axes in a particular Cartesian coordinate system. That it has extent along all three axes is irrelevant to the geometric fact that it's two-dimensional. A bit of terminology: an $(n - 1)$-dimensional surface in an $n$-dimensional space is called a \textbf{hypersurface}. A plane in three-dimensional Euclidean space is a hypersurface, and $V$ in our integral is a hypersurface in Minkowski spacetime. Specifically, it's a \emph{spacelike} hypersurface, since there exists an inertial frame for which it's purely spatial (its temporal extent can be Lorentz-transformed away).

The other thing one might mean by the Lorentz-covariance of our integral ${\int_V (\vv B \cdot {\inlinedy{\mathsf{T}}}) \dd V}$ is more interesting, but we haven't justified it yet, and it will take some explaining. Instead of Lorentz-transforming all the quantities in the integral, we'll integrate corresponding quantities over a \emph{completely different} spacelike hypersurface that encloses the same \emph{matter} as $V$, where ``matter" is meant in the most general possible sense (anything with energy, including electromagnetic energy, for example). But it will be the \emph{primed} frame for which this new hypersurface is purely spatial. We write: $\int_{V^\textrm{p}} (\vv B^{\textrm{p}} \cdot {\inlinedy{\mathsf{T}}} ^{\textrm{p}} ) \dd V^\textrm{p}$, where we avoid the prime symbol to make clear that the underlying quantities are new. So, $\vv B^{\textrm{p}}$ is the \emph{primed} observer's four-velocity, and $V^\textrm{p}$ is a purely spatial region in the \emph{primed} frame, related to $V$ only in that they contain the same ``stuff." Let's stipulate that the spacelike hypersurfaces $V$ and $V^\textrm{p}$ don't intersect (they have no events in common). Then they constitute ``opposite ends" of the hypersurface-boundary of a greater \emph{four-dimensional} spacetime region. To ensure that they enclose the same matter, we'll further stipulate that nothing enters or exits that greater \textbf{four-volume} \emph{except} through $V$ and $V^\textrm{p}$, or equivalently that the stress--energy tensor vanishes at all other parts of the boundary. This is all a fancy way of saying that ${\inlinedy{\mathsf{T}}}$ and ${\inlinedy{\mathsf{T}}} ^{\textrm{p}}$ pertain to the same isolated system sampled at an entirely different set of events, one set corresponding to a spatial volume at one moment for the first frame, the other corresponding to a spatial volume at one moment for the second frame.

Does ${\int_V (\vv B \cdot {\inlinedy{\mathsf{T}}} ) \dd V} = \int_{V^\textrm{p}} (\vv B^{\textrm{p}} \cdot {\inlinedy{\mathsf{T}}} ^{\textrm{p}} ) \dd V^\textrm{p}$? That's the question of Lorentz-covariance here. We've already established that each integral will be the total four-momentum in its corresponding volume, and that the volumes will contain the same matter, so we're really asking whether that matter will have the same total four-momentum after it ``evolves" internally ``between" $V$ and $V^{\textrm{p}}$. Obviously the answer is yes---the total four-momentum of an isolated system is conserved!

But what if we didn't already know that? Come to think of it, the only reason we know that the integral evaluates to a four-vector at all is our \emph{assertion} that the total stress--energy tensor's components represent the appropriate flows and densities. Even the tensor's existence and vanishing divergence were merely asserted. A true field-theory approach would first generate the stress--energy tensor and establish its divergencelessness from Lagrangian methods (which we'll soon do for the electromagnetic case in a shamefully hand-wavy fashion), then \emph{prove} that ${\int_V (\vv B \cdot {\inlinedy{\mathsf{T}}} ) \dd V}$ is a four-vector that satisfies ${\int_V (\vv B \cdot {\inlinedy{\mathsf{T}}} ) \dd V} = \int_{V^\textrm{p}} (\vv B^{\textrm{p}} \cdot {\inlinedy{\mathsf{T}}} ^{\textrm{p}} ) \dd V^\textrm{p}$; this then \emph{defines} the four-momentum of a composite system. The proof involves the spacetime divergence theorem, which we'll take for granted (it works just like the ``normal" divergence theorem, and it holds for tensors of any rank, as does the familiar one, by the way). First, recognize that $\vv B$ and $\vv B^\textrm{p}$ are themselves unit normal four-vectors for their respective spacelike hypersurfaces, one pointing ``inward" toward the greater four-volume that $V$ and $V^\textrm{p}$ participate in bounding, the other pointing ``outward." Then use the divergence theorem: the \emph{four-volume} integral of $\partialup \cdot {\inlinedy{\mathsf{T}}}$ taken over the greater spacetime region---\emph{manifestly a four-vector}---both vanishes (by Equation \ref{eq:set}) and equals the difference of our primed and unprimed hypersurface-integrals, as $V$ and $V^\textrm{p}$ are by construction the only ``porous" parts of the boundary. (We take their difference rather than their sum because we need both unit normals pointing outward.) It follows that our hypersurface-integrals are themselves four-vectors, and also that they're equal (they're a \emph{conserved} four-vector): ${\int_V (\vv B \cdot {\inlinedy{\mathsf{T}}}) \dd V} = \int_{V^\textrm{p}} (\vv B^{\textrm{p}} \cdot {\inlinedy{\mathsf{T}}} ^{\textrm{p}}) \dd V^\textrm{p}$. Now we see explicitly how Equation \ref{eq:set} functions as the continuity equation for four-momentum. Since the stress--energy tensor has no spacetime ``sources" or ``sinks," whatever quantity of it the matter carries into the four-volume through $V$ gets carried out by the same matter through $V^\textrm{p}$, and the spacetime divergence theorem confirms that ${\inlinedy{\mathsf{T}}}$ represents the spacetime ``flow" or ``flux density" of a four-vector that must be conserved, both globally (as we've just shown) and locally (by the form of Equation \ref{eq:set}, which tells us that transport through the bounding surface is the \emph{only} way that a spatial volume's energy and momentum can change). This vector is the system's four-momentum.\footnote{Stipulating that $V$ and $V^{\textrm{p}}$ not intersect helped with the formulation, but it isn't a real restriction. If $V$ and $V^{\textrm{p}}$ \emph{do} intersect, then we can just choose a third spacelike hypersurface $V^{\textrm{q}}$ that doesn't intersect either of them (but which still satisfies the ``same matter" criterion) and apply our argument twice---once with $V^{\textrm{q}}$ and $V$, and once with $V^{\textrm{q}}$ and $V^{\textrm{p}}$. This will show that the total four-momentum is the same for all three.}

So that's how it's done in classical field theory. Use Lagrangian methods to get the necessarily divergenceless stress--energy tensor, and extract from it (the volume-density of) a four-vector $\vv P$ whose local and global conservation is thereby established. Then with electrodynamics in particular, one might start with the electromagnetic ${\inlinedy{\mathsf{T}}}_{\textrm{em}}$, invoke conservation of \emph{total} four-momentum to write the \emph{form} of Equation \ref{eq:fmc} (equating $\partialup \cdot {\inlinedy{\mathsf{T}}}_{\textrm{em}}$ to the negative Lorentz four-force density), and use Maxwell's equations to express the Lorentz four-force density in terms of the four-current density $\vv J$ and the Faraday dyadic $\inlinedy{\mathsf{F}}$.\footnote{The Lorentz four-force $\vv F_{\mathrm{L}}$ can then be defined as the force-density's test-charge limit, neglecting higher-order ``recoil" terms (see Footnote \ref{fn:recoil}).} This density-first approach is the sophisticated and mathematically precise way to handle the four-momentum of anything other than a point particle.\footnote{And even for a point particle, one can use the Dirac delta function, though again, classical field theory is really about continua, not particles.} It's the \emph{only} satisfactory way to handle the four-momentum of electromagnetic fields, though sometimes one can avoid working directly with the stress--energy tensor by arguing from conservation and appealing to ``$E = pc$" for light, as we did in the early sections of this paper.


\subsection{Lagrangian Electrodyadics}

In Section \ref{sssec:ele}, we fed a well-chosen Lagrangian $L$ into the four-vector version of the Euler--Lagrange equation (Equation \ref{eq:ele}) and successfully derived the Lorentz four-force (and a Hamiltonian). In this section, we'll derive Maxwell's equations (and something analogous to a Hamiltonian) by feeding the appropriate Lagrangian \emph{density} $\mathcal{L}$ into the four-vector \emph{field} version of the Euler--Lagrange equation, which we'll notate like this:
\begin{equation}\label{eq:elef}
\partialup \cdot \capdy{\partialrm} {}^{\partialup \mkern2mu \otimes \vv A} \mathcal{L} = \partialup^{\vv A} \mathcal{L} .
\end{equation}

\subsubsection{Dyadic Operators; Electromagnetic Lagrangian Density}

This requires some explanation! Essentially, the four-potential field ${\vv A (\vv R)}$ is playing the role here that the particle's four-position $\vv R (ct_0)$ played in Equation \ref{eq:ele} (the role of ``coordinate"), and the gradient of the four-potential ($\partialup \otimes \vv A$) is playing the role that the particle's four-velocity $\vv B (ct_0) = \mathring{\vv R}$ played (the role of ``velocity").\footnote{As is always the case with a Lagrangian approach, the ``coordinate" and ``velocity" must be understood as \emph{independent} parameters, in the sense that differentiating with respect to $\vv A$ will mean treating $\partialup \otimes \vv A$ as a constant (and vice versa).} So on the right side, instead of the four-gradient $\partialup = \partialup^{\vv R}$, we have a new four-vector operator:
\begin{equation*}
\partialup^{\vv A} \equiv \left \langle \dfrac{\partial}{\partial A^{ct}} , \,  - \dfrac{\partial}{\partial A^x} , \, - \dfrac{\partial}{\partial A^y} , \, - \dfrac{\partial}{\partial A^z} \right \rangle ,
\end{equation*}
and on the left side we've got some sort of \emph{dyadic} operator $\inlinedy{\partialrm} {}^{\partialup \mkern2mu \otimes \vv A}$ that's taking the place of Equation \ref{eq:ele}'s $\mathring{\partialup} = \partialup^{\vv B} = \partialup^{\mathring{\vv R}}$. We've yet to define this dyadic operator, but we know that it must output a four-dyadic when it acts on the scalar $\mathcal{L}$---a four-dyadic playing the role of ``canonical momentum"---and the left side of Equation \ref{eq:elef} is the divergence of that four-dyadic (whereas the left side of Equation \ref{eq:ele} was the $ct_0$-derivative of the \emph{vector} resulting from the operation $\mathring{\partialup} L$).

(Yes, we're proceeding by analogy here. We'll get the right results, and I like to think there's something ``intuitive" about this way of doing things, but please understand that we're doing a lot of hand-waving. You should consult a real textbook that introduces the appropriate mathematical tools if you're interested in learning this material the right way. In any case, before reading on, you might want to look over the previous paragraph again and make sure the analogy between Equations \ref{eq:elef} and \ref{eq:ele} has sunk in.)

Defining our new dyadic operator is the same kind of task we faced when we defined the four-del back in Section \ref{sssec:fd}---namely, we have to get the signs of the partial derivatives right, and a transformation rule will tell us when we've done it. This time, the components of our operator must obey Equation \ref{eq:ltd} under a Lorentz boost. As before, let's make a naive guess\footnote{Perhaps you can make a better guess!} and see what happens (and for brevity we'll use a generic four-dyadic field $\inlinedy{\mathsf{D}} = \inlinedy{\mathsf{D}} (\vv R)$ instead of $\partialup \otimes \vv A$):
\begin{equation*}
\begin{bmatrix}
\capdy{\partialrm} {}^{\superdy{\mathsf{D}}} \\[-.375ex]
\end{bmatrix}
\stackrel{?}{=}
\begin{bmatrix}
\\[-2ex]
\dfrac{\partial}{\partial \mathsfit D^{ct \mkern1.5mu ct}} & \dfrac{\partial}{\partial \mathsfit D^{x \mkern1.5mu ct}} & \dfrac{\partial}{\partial \mathsfit D^{y \mkern1.5mu ct}} & \dfrac{\partial}{\partial \mathsfit D^{z \mkern1.5mu ct}} \\[2ex]
\dfrac{\partial}{\partial \mathsfit D^{ct \mkern1.5mu x}} & \dfrac{\partial}{\partial \mathsfit D^{x x}} & \dfrac{\partial}{\partial \mathsfit D^{y x}} & \dfrac{\partial}{\partial \mathsfit D^{z x}}  \\[2ex]
\dfrac{\partial}{\partial \mathsfit D^{ct \mkern1.5mu y}} & \dfrac{\partial}{\partial \mathsfit D^{x y}} & \dfrac{\partial}{\partial \mathsfit D^{y y}} & \dfrac{\partial}{\partial \mathsfit D^{z y}}  \\[2ex]
\dfrac{\partial}{\partial \mathsfit D^{ct \mkern1.5mu z}} & \dfrac{\partial}{\partial \mathsfit D^{x z}} & \dfrac{\partial}{\partial \mathsfit D^{y z}} & \dfrac{\partial}{\partial \mathsfit D^{z z}} \\[2ex]
\end{bmatrix}
\equiv
\dfrac{\partial}{\partial [ \capdy{\mathsf{D}} ]}
,
\end{equation*}
where we've defined the last expression as shorthand for that 4-by-4 in the middle. Note that we could have defined it as the \emph{transpose} of that 4-by-4---it's a matter of convention. The convention we've chosen is called the ``numerator" or ``Jacobian" layout; the other convention is the ``denominator" or ``Hessian" layout. We could work with either, but the numerator layout will save us a step.

Now we'll take the $[\inlinedy{\mathsf{D}}]$-derivative of $\inlinedy{\mathsf{D}} : \inlinedy{\mathsf{D}} = \mathrm{Tr} (\eta [ \inlinedy{\mathsf{D}} ] \eta [ \inlinedy{\mathsf{D}} ] ^{\mathrm{T}} )$, a Minkowski scalar, and see whether the result is the matrix representation of a four-dyadic. The procedure is to obtain the differential of the trace, with $\dd [ \inlinedy{\mathsf{D}} ]$ ``factored out." Everything else in the trace will then be the $[\inlinedy{\mathsf{D}}]$-derivative of $\inlinedy{\mathsf{D}} : \inlinedy{\mathsf{D}}$.\footnote{In the denominator layout, it would be the \emph{transpose} of everything else in the trace. Wikipedia's article on matrix calculus has a \href{https://en.wikipedia.org/wiki/Matrix_calculus\#Scalar-by-matrix_identities}{step-by-step example} of differentiating a trace with respect to a matrix; it uses the numerator layout.} Using the product rule and properties of the trace discussed in Section \ref{ssec:dd}, the differential is:
\begin{equation*}
\begin{aligned}
\dd \, \mathrm{Tr} \left( \eta [\capdy{\mathsf{D}}] \eta [ \capdy{\mathsf{D}} ]^{\mathrm{T}} \right) &= \mathrm{Tr} \left( \eta (\dd [\capdy{\mathsf{D}}]) \eta [ \capdy{\mathsf{D}} ]^{\mathrm{T}} + \eta [\capdy{\mathsf{D}}] \eta (\dd [ \capdy{\mathsf{D}} ]^{\mathrm{T}}) \right) \\[3pt]
&= \mathrm{Tr} \left( \eta [ \capdy{\mathsf{D}} ]^{\mathrm{T}} \eta (\dd [\capdy{\mathsf{D}}]) + (\dd [\capdy{\mathsf{D}}]) \eta^\mathrm{T} [\capdy{\mathsf{D}}]^\mathrm{T} \eta^{\mathrm{T}} \right) \\[3pt]
&= \mathrm{Tr} \left( ( \eta [ \capdy{\mathsf{D}} ]^{\mathrm{T}} \eta + \eta [\capdy{\mathsf{D}}]^\mathrm{T} \eta ) \; \dd [\capdy{\mathsf{D}}] \right)
\end{aligned}
\end{equation*}
(because $\eta$ is symmetric). With $\dd [ \inlinedy{\mathsf{D}} ]$ ``factored out," the $[\inlinedy{\mathsf{D}}]$-derivative of the scalar $\inlinedy{\mathsf{D}} : \inlinedy{\mathsf{D}}$ is everything else in the trace:
\begin{equation*}
\dfrac{\partial}{\partial [ \capdy{\mathsf{D}} ]} \, ( \capdy{\mathsf{D}} : \capdy{\mathsf{D}} ) = 2 \eta [\capdy{\mathsf{D}}]^\mathrm{T} \eta .
\end{equation*}
That is \emph{not} the matrix representation of a four-dyadic! The problem is those $\eta$'s. They throw the signs off, and the sixteen quantities $\eta [\inlinedy{\mathsf{D}}]^\mathrm{T} \eta$ do \emph{not} transform by Equation \ref{eq:ltd} (as you should verify).\footnote{Rather, they transform by the ``inverse" formula, ${ ( \eta [\footnotedy{\mathsf{D}}]^\mathrm{T} \eta ) ^\prime = ( \Lambda ^{-1} ) ^\mathrm{T} ( \eta [\footnotedy{\mathsf{D}}]^\mathrm{T} \eta ) \Lambda ^{-1} }$. The sixteen quantities $\eta [\footnotedy{\mathsf{D}}]^\mathrm{T} \eta$ are the components of a four-\emph{codyadic} (see Footnote \ref{fn:cov}).} So our naive guess was incorrect. But this is easily remedied: we just need to differentiate with respect to the matrix $\eta [\inlinedy{\mathsf{D}}] \eta$ instead of $[\inlinedy{\mathsf{D}}]$. Then using $\eta \eta = I_4$ (the identity matrix):
\begin{equation*}
\begin{aligned}
\dd \, \mathrm{Tr} \left( \eta [\capdy{\mathsf{D}}] \eta [ \capdy{\mathsf{D}} ]^{\mathrm{T}} \right) &= \dd \, \mathrm{Tr} \left( \eta [\capdy{\mathsf{D}}] \eta \eta \eta [ \capdy{\mathsf{D}} ]^{\mathrm{T}} \eta \eta \right) \\[3pt]
&= \mathrm{Tr} \left( \dd (\eta [\capdy{\mathsf{D}}] \eta ) I_4 [ \capdy{\mathsf{D}} ]^{\mathrm{T}} I_4 +  \eta [\capdy{\mathsf{D}}] I_4 \, \dd ( \eta [\capdy{\mathsf{D}}]^\mathrm{T} \eta ) \eta \right) \\[3pt]
&= \mathrm{Tr} \left( 2 [\capdy{\mathsf{D}}]^\mathrm{T} \; \dd (\eta [\capdy{\mathsf{D}}] \eta ) \right) ,
\end{aligned}
\end{equation*}
giving
\begin{equation*}
\dfrac{\partial}{\partial (\eta [ \capdy{\mathsf{D}} ] \eta)} \, ( \capdy{\mathsf{D}} : \capdy{\mathsf{D}} ) = 2 [\capdy{\mathsf{D}}]^\mathrm{T} ,
\end{equation*}
the matrix representation of a four-dyadic. Thus:
\begin{equation*}
\begin{bmatrix}
\capdy{\partialrm} {}^{\superdy{\mathsf{D}}} \\[-.375ex]
\end{bmatrix}
=
\dfrac{\partial}{\partial (\eta [ \capdy{\mathsf{D}} ] \eta)}
=
\begin{bmatrix}
\\[-2ex]
\dfrac{\partial}{\partial \mathsfit D^{ct \mkern1.5mu ct}} & -\dfrac{\partial}{\partial \mathsfit D^{x \mkern1.5mu ct}} & -\dfrac{\partial}{\partial \mathsfit D^{y \mkern1.5mu ct}} & -\dfrac{\partial}{\partial \mathsfit D^{z \mkern1.5mu ct}} \\[2ex]
-\dfrac{\partial}{\partial \mathsfit D^{ct \mkern1.5mu x}} & \dfrac{\partial}{\partial \mathsfit D^{x x}} & \dfrac{\partial}{\partial \mathsfit D^{y x}} & \dfrac{\partial}{\partial \mathsfit D^{z x}}  \\[2ex]
-\dfrac{\partial}{\partial \mathsfit D^{ct \mkern1.5mu y}} & \dfrac{\partial}{\partial \mathsfit D^{x y}} & \dfrac{\partial}{\partial \mathsfit D^{y y}} & \dfrac{\partial}{\partial \mathsfit D^{z y}}  \\[2ex]
-\dfrac{\partial}{\partial \mathsfit D^{ct \mkern1.5mu z}} & \dfrac{\partial}{\partial \mathsfit D^{x z}} & \dfrac{\partial}{\partial \mathsfit D^{y z}} & \dfrac{\partial}{\partial \mathsfit D^{z z}} \\[2ex]
\end{bmatrix} ,
\end{equation*}
and we've found a ``self-product" rule that goes:
\begin{equation}\label{eq:dyoprule1}
\capdy{\partialrm} {}^{\superdy{\mathsf{D}}} \, ( \capdy{\mathsf{D}} : \capdy{\mathsf{D}} ) = 2 \capdy{\mathsf{D}} {}^\mathrm{T} .
\end{equation}
As an exercise, use our matrix-calculus method to show that also:
 \begin{equation}\label{eq:dyoprule2}
\capdy{\partialrm} {}^{\superdy{\mathsf{D}}} \, ( \capdy{\mathsf{D}} : \capdy{\mathsf{D}} {}^\mathrm{T} ) = 2 \capdy{\mathsf{D}} .
\end{equation}

Returning to Equation \ref{eq:elef},
\begin{equation*}
\partialup \cdot \capdy{\partialrm} {}^{\partialup \mkern2mu \otimes \vv A} \mathcal{L} = \partialup^{\vv A} \mathcal{L},
\end{equation*}
let's try to come up with a Lagrangian density $\mathcal{L}$ that will give us Maxwell's equations. Since the identity $\partialup \cdot \, \star ( \partialup \wedge \vv A ) = \textrm{\mbox{\boldmath $\emptyset$}}$ hands us Equation \ref{eq:hme} ``for free" (see Section \ref{sssec:dedv}), we need only worry about Equation \ref{eq:ime} here: $\partialup \cdot \inlinedy{\mathsf{F}} = \vv J$. Our Lagrangian density should have one term for the ``free field" (absent the source field $\vv J$) and another term that accounts for the field's interaction with charges. For the interaction term, we can simply ``densitize" the interaction term $q (\vv B \cdot \vv A)$ that we used for the \emph{particle} Lagrangian $L$ in Section \ref{sssec:ele}. That gives $\vv J \cdot \vv A$,\footnote{Differentiate with respect to proper volume for the individual fluid-like ``sub-distributions," then sum. See Section \ref{sssec:lffd}.} which will vanish on the left side of Equation \ref{eq:elef} and evaluate to $\vv J$ on the right side (by the same kind of math we used to show that $\partialup ( \vv K \cdot \vv R ) = \vv K$ in Section \ref{sssec:li}).\footnote{That $\vv J \cdot \vv A$ isn't gauge-invariant sometimes causes confusion. In fact, the Lagrangian density doesn't \emph{have} to be gauge-invariant. The \emph{variation of the action} does, and the action is given by the integral of the Lagrangian density over spacetime, with the condition that variations of the $\vv A$ field vanish at the boundaries. Under a gauge transformation that takes $\vv A$ to $\vv A + \partialup \psi$ (for any spacetime scalar field $\psi$), the term $\vv J \cdot ( \partialup \psi )$ arises in $\mathcal{L}$. By a product rule, that's $\vv J \cdot ( \partialup \psi ) = \partialup \cdot (\psi \vv J) - \psi  ( \partialup \cdot \vv J )$. When varying the ``new" action, then, the spacetime integrals of $\partialup \cdot (\psi \vv J)$ and $\psi ( \partialup \cdot \vv J )$ had better contribute nothing! The first does because the spacetime divergence theorem turns it into an integral of $\psi \vv J$ over the bounding ``hypersurface," and variation of that integral vanishes by the aforementioned boundary condition. That's just math. The $\psi ( \partialup \cdot \vv J )$ term, however, vanishes only by the grace of the continuity equation (Equation \ref{eq:con}). This is \textbf{Noether's theorem} in action: \emph{charge-conservation and the ``symmetry" of gauge-invariance are two sides of the same coin}, a relationship we hinted at in Section \ref{sssec:fp}.}

We're halfway there. Now we need the left side of Equation \ref{eq:elef} to produce $\partialup \cdot \inlinedy{\mathsf{F}}$. The question is: what Lorentz invariant can we feed $\inlinedy{\partialrm} {}^{\partialup \mkern2mu \otimes \vv A}$ to get $\inlinedy{\mathsf{F}} = \partialup \otimes \vv A - (\partialup \otimes \vv A)^\mathrm{T}$? Equations \ref{eq:dyoprule1} and \ref{eq:dyoprule2} provide the answer---we feed it $ .5(\partialup \otimes \vv A):(\partialup \otimes \vv A)^\mathrm{T} - .5(\partialup \otimes \vv A):(\partialup \otimes \vv A) $. Some algebra shows that that's equivalently a quarter of the (negative) field invariant $ \inlinedy{\mathsf{F}} : \inlinedy{\mathsf{F}} $ (Equation \ref{eq:fddf}):
\begin{equation*}
\begin{aligned}
\capdy{\mathsf{F}} : \capdy{\mathsf{F}} &= \left( \partialup \otimes \vv A - (\partialup \otimes \vv A)^\mathrm{T} \right) : \left( \partialup \otimes \vv A - (\partialup \otimes \vv A)^\mathrm{T} \right) \\[2pt]
&= (\partialup \otimes \vv A):(\partialup \otimes \vv A) + (\partialup \otimes \vv A)^\mathrm{T} : (\partialup \otimes \vv A)^\mathrm{T}  - 2 (\partialup \otimes \vv A):(\partialup \otimes \vv A)^\mathrm{T} \\[2pt]
&= 2 (\partialup \otimes \vv A):(\partialup \otimes \vv A) - 2 (\partialup \otimes \vv A):(\partialup \otimes \vv A)^\mathrm{T},
\end{aligned}
\end{equation*}
so we can write the Lagrangian density for the electromagnetic field like this:
\begin{equation}\label{eq:emld}
\boxed{\mathcal{L} = - \dfrac{1}{4} \, ( \capdy{\mathsf{F}} : \capdy{\mathsf{F}} ) \, + \, \vv J \cdot \vv A} \, .
\end{equation}
(If we'd defined our dyadic operator according to the \emph{denominator} layout, then the $\inlinedy{\mathsf{F}} : \inlinedy{\mathsf{F}}$ term in Equation \ref{eq:emld} would have the opposite sign.)

\subsubsection{Canonical Electromagnetic Stress--Energy Tensor}

With our ``canonical momentum" $\inlinedy{\partialrm} {}^{\partialup \mkern2mu \otimes \vv A} \mathcal{L} = \inlinedy{\mathsf{F}}$, our ``velocity" $( \partialup \otimes \vv A )$, and our \emph{free-field} Lagrangian density $\mathcal{L} = - \inlinedy{\mathsf{F}} : \inlinedy{\mathsf{F}} / 4$, we should be able to construct a conserved quantity associated with the (sourceless) electromagnetic field, analogous to a Hamiltonian. Recall from Section \ref{sssec:ele} that our Hamiltonian for a particle in an electromagnetic field was $\vv B \cdot \vv P_{\mathrm{c}} - L$ (dot product of four-velocity and canonical four-momentum minus the Lagrangian), a Lorentz scalar with a vanishing $ct_0$-derivative. For the field itself, we'll combine the analogous quantities in a similar way, and the result should be a \emph{dyadic} with a vanishing \emph{four-divergence} (in free space). We'll call this dyadic the \textbf{canonical electromagnetic stress--energy tensor}, and since it's analogous to a Hamiltonian we'll label it $\inlinedy{\mathsf{H}}$. Its divergencelessness will signify the conservation of electromagnetic four-momentum; we should expect the ``normal" electromagnetic stress--energy tensor ${\inlinedy{\mathsf{T}}}_{\textrm{em}}$ to make an appearance.

We have to be careful, though: our ``velocity" dyadic $( \partialup \otimes \vv A )$ is neither symmetric nor antisymmetric, so order of multiplication matters when dotting it with the ``canonical momentum." Do we want $(\partialup \otimes \vv A) \cdot \inlinedy{\mathsf{F}}$, or the reverse? Or maybe we need the \emph{transpose} of $(\partialup \otimes \vv A)$? What about the sign of this term (or the other, for that matter)? A price we pay for being so informal here and working from analogy is that we aren't in a position to do more than guess. Then we can ``adjust" as needed as we go, by demanding that ${\inlinedy{\mathsf{T}}}_{\textrm{em}}$ be involved and that $\inlinedy{\mathsf{H}}$ be divergenceless (remembering that for a dyadic $\inlinedy{\mathsf{D}}$ that isn't symmetric or antisymmetric, a vanishing $\partialup \cdot \inlinedy{\mathsf{D}}$ doesn't imply a vanishing $\inlinedy{\mathsf{D}} \cdot \, \partialup _{\footnotedy{\mathsf{D}}} = \partialup \cdot \inlinedy{\mathsf{D}} {}^{\mathrm{T}} $, and vice versa). If this is a subject that interests you, I recommend learning index notation and consulting a real textbook.

That said, let's try it this way:
\begin{equation*}
\capdy{\mathsf{H}} \, \stackrel{?}{=} \, \left( \partialup \otimes \vv A \right) \cdot \capdy{\mathsf{F}} - \mathcal{L} \inlinedy{\upeta} ,
\end{equation*}
where the Minkowski metric dyadic enters as the ``unit dyadic" of spacetime. By the definition of the Faraday tensor (Equation \ref{eq:emta}), that's:
\begin{equation*}
\begin{split}
\left( \partialup \otimes \vv A \right) \cdot \capdy{\mathsf{F}} - \mathcal{L} \inlinedy{\upeta} &=  \left( \left( \partialup \otimes \vv A \right)^\mathrm{T} + \capdy{\mathsf{F}} \right) \cdot \capdy{\mathsf{F}} - \mathcal{L} \inlinedy{\upeta} \\
&= \left( \partialup \otimes \vv A \right)^\mathrm{T} \cdot \capdy{\mathsf{F}} \, + \, \capdy{\mathsf{F}} \cdot \capdy{\mathsf{F}} \, + \, \dfrac{1}{4} \, ( \capdy{\mathsf{F}} : \capdy{\mathsf{F}} ) \inlinedy{\upeta},
\end{split}
\end{equation*}
and we recognize the sum of the last two terms as the symmetric electromagnetic stress--energy tensor ${\inlinedy{\mathsf{T}}}_{\textrm{em}}$ that we've already discussed (Equation \ref{eq:eseta}):
\begin{equation*}
\capdy{\mathsf{H}} \, \stackrel{?}{=} \, \left( \partialup \otimes \vv A \right)^\mathrm{T} \cdot \capdy{\mathsf{F}} \, + \, \capdy{\mathsf{T}}_{\textrm{em}} .
\end{equation*}
Looks like we're on the right track. We know ${\inlinedy{\mathsf{T}}}_{\textrm{em}}$ to be divergenceless in the absence of sources (Equation \ref{eq:fmc}), and its symmetry means that its four-divergence vanishes ``both ways" (i.e., ${\partialup \cdot {\inlinedy{\mathsf{T}}}_{\textrm{em}} = {\inlinedy{\mathsf{T}}}_{\textrm{em}} \cdot \, \partialup _{{\footnotedy{\mathsf{T}}}\mkern-3mu_{\textrm{em}}} = \partialup \cdot {\inlinedy{\mathsf{T}}} {}^{\mathrm{T}}_{\textrm{em}} = \textrm{\mbox{\boldmath $\emptyset$}}}$), so we need only verify that $( \partialup \otimes \vv A )^\mathrm{T} \cdot \inlinedy{\mathsf{F}}$ \emph{or its transpose} is divergenceless. We'll now use matrix multiplication to demonstrate that we guessed wrong at the start, and that it's the \emph{transpose} of $( \partialup \otimes \vv A )^\mathrm{T} \cdot \inlinedy{\mathsf{F}}$ that's divergenceless.

The claim, then, is that $\partialup \cdot ( ( \partialup \otimes \vv A )^\mathrm{T} \cdot \inlinedy{\mathsf{F}} )^{\mathrm{T}} = \partialup \cdot ( \inlinedy{\mathsf{F}} {}^{\mathrm{T}} \cdot ( \partialup \otimes \vv A ) )  = \textrm{\mbox{\boldmath $\emptyset$}}$, and consequently that the canonical electromagnetic stress--energy tensor is actually $\inlinedy{\mathsf{H}} = \inlinedy{\mathsf{F}} {}^{\mathrm{T}} \cdot ( \partialup \otimes \vv A ) + {\inlinedy{\mathsf{T}}}_{\textrm{em}} = {\inlinedy{\mathsf{T}}}_{\textrm{em}} - \inlinedy{\mathsf{F}} \cdot \, ( \partialup \otimes \vv A ) $ (because the Faraday tensor is antisymmetric). In matrix notation, the dyadic $ \inlinedy{\mathsf{F}} \cdot \, ( \partialup \otimes \vv A )$ is:
\begin{equation*}
\mkern0mu [\capdy{\mathsf{F}}] \eta [\partialup] [\vv A]^{\mathrm{T}} =
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\
e_x & 0 & -b_z & b_y \\
e_y & b_z & 0 & -b_x \\
e_z & -b_y & b_x & 0
\end{bmatrix}
\begin{bmatrix}
\partial^{ct} A^{ct} & \partial^{ct} A^x & \partial^{ct} A^y & \partial^{ct} A^z \\
- \partial^x A^{ct} & - \partial^x A^x & - \partial^x A^y & - \partial^x A^z  \\
- \partial^y A^{ct} & - \partial^y A^x & - \partial^y A^y & - \partial^y A^z \\
- \partial^z A^{ct} & - \partial^z A^x & - \partial^z A^y & - \partial^z A^z
\end{bmatrix} .
\end{equation*}
Taking the four-divergence $\partialup \cdot (\inlinedy{\mathsf{F}} \cdot \, ( \partialup \otimes \vv A ))$ means putting $[\partialup]^\mathrm{T}\eta$ in front:
\begin{equation*}
\begin{bmatrix}
\partial^{ct} & [ \del ]^\mathrm{T}
\end{bmatrix}
\left(
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\
e_x & 0 & -b_z & b_y \\
e_y & b_z & 0 & -b_x \\
e_z & -b_y & b_x & 0
\end{bmatrix}
\begin{bmatrix}
\partial^{ct} A^{ct} & \partial^{ct} A^x & \partial^{ct} A^y & \partial^{ct} A^z \\
- \partial^x A^{ct} & - \partial^x A^x & - \partial^x A^y & - \partial^x A^z  \\
- \partial^y A^{ct} & - \partial^y A^x & - \partial^y A^y & - \partial^y A^z \\
- \partial^z A^{ct} & - \partial^z A^x & - \partial^z A^y & - \partial^z A^z
\end{bmatrix}
\right)
\end{equation*}
(using $\del = \langle - \partial^x , - \partial^y , - \partial^z \rangle$). We won't write out the full 4-by-4 in the parentheses, but if we visualize carrying out that matrix-multiplication first, we see that each component of the four-vector $\partialup \cdot (\inlinedy{\mathsf{F}} \cdot \, ( \partialup \otimes \vv A ))$ will involve just a single column of the rightmost matrix in the above expression, and that any such component $\mu$ (where $\mu$ can be $ct$, $x$, $y$, or $z$) is then:
\begin{equation*}
\begin{split}
\Big( \partialup \cdot \big( \capdy{\mathsf{F}} \cdot \, ( \partialup \otimes \vv A ) \big) \Big)^{\mu} &= \partial^{ct} \, \bigl( e_x \partial^x A^{\mu} + e_y \partial^y A^{\mu} + e_z \partial^z A^{\mu} \bigr) \\[-.9ex]
& \; - \partial^x \left( e_x \partial^{ct} A^{\mu} + b_z \partial^y A^{\mu} - b_y \partial^z A^{\mu} \right) \\
& \; - \partial^y \left( e_y \partial^{ct} A^{\mu} - b_z \partial^x A^{\mu} + b_x \partial^z A^{\mu} \right) \\
& \; - \partial^z \left( e_z \partial^{ct} A^{\mu} + b_y \partial^x A^{\mu} - b_x \partial^y A^{\mu} \right)
\end{split},
\end{equation*}
or:
\begin{equation*}
\Big( \partialup \cdot \big( \capdy{\mathsf{F}} \cdot \, ( \partialup \otimes \vv A ) \big) \Big)^{\mu} = - \partial^{ct} ( \vv e \cdot \del A^{\mu} ) \, + \, \del \cdot \left( (\partial^{ct} A^{\mu}) \vv e \, + \, \vv b \times \del A^{\mu} \right) ,
\end{equation*}
and I suppose we're dabbling in index notation after all. Expanding, and using some vector-calculus product rules, that's:
\begin{equation*}
-(\partial^{ct} \vv e) \cdot \del A^{\mu} - \vv e \cdot \partial^{ct} \del A^{\mu} + (\partial^{ct} A^{\mu})(\del \cdot \vv e) + \vv e \cdot \del \partial^{ct} A^{\mu} + (\del \times \vv b) \cdot \del A^{\mu}
\end{equation*}
(we've omitted the last term $ - (\del \times \del A^{\mu}) \cdot \vv b
$, which vanishes because it contains the curl of a gradient). The second and fourth terms cancel, leaving:
\begin{equation*}
-(\partial^{ct} \vv e) \cdot \del A^{\mu} + (\partial^{ct} A^{\mu})(\del \cdot \vv e) + (\del \times \vv b) \cdot \del A^{\mu}.
\end{equation*}
Finally, by the free-space condition $\vv J = \textrm{\mbox{\boldmath $\emptyset$}}$, Maxwell's equations give $\del \cdot \vv e = 0$ and $\del \times \vv b = \partial^{ct} \vv e$ (Equations \ref{eq:me}), and we get:
\begin{equation*}
\Big( \partialup \cdot \big( \capdy{\mathsf{F}} \cdot \, ( \partialup \otimes \vv A ) \big) \Big)^{\mu} = 0 ,
\end{equation*}
as promised. So $\inlinedy{\mathsf{H}} = {\inlinedy{\mathsf{T}}}_{\textrm{em}} - \inlinedy{\mathsf{F}} \cdot \, ( \partialup \otimes \vv A )$ and $\partialup \cdot \inlinedy{\mathsf{H}} = \textrm{\mbox{\boldmath $\emptyset$}}$ in free space, and we've indeed arrived at the conservation of electromagnetic four-momentum from a Lagrangian approach. To express $\inlinedy{\mathsf{H}}$ in terms of the quantities we started with (``canonical momentum" $\inlinedy{\mathsf{F}}$, ``velocity" $ \partialup \otimes \vv A $, and the free-field Lagrangian density $\mathcal{L} = - \inlinedy{\mathsf{F}} : \inlinedy{\mathsf{F}} / 4$), we can reverse our steps with some algebra and obtain $\inlinedy{\mathsf{H}} = - \inlinedy{\mathsf{F}} \cdot \, ( \partialup \otimes \vv A) {}^{\mathrm{T}} - \mathcal{L} \inlinedy{\upeta}$.

In practice, there's little use for $\inlinedy{\mathsf{H}}$, other than as a step on the way to deriving the symmetric and gauge-invariant ${{\inlinedy{\mathsf{T}}}_{\textrm{em}} \equiv \inlinedy{\mathsf{H}} +  \inlinedy{\mathsf{F}} \cdot \, ( \partialup \otimes \vv A )}$ (we've gone in reverse order).\footnote{As mentioned in Footnote \ref{fn:setgr}, in general relativity one obtains the symmetric (total) stress--energy tensor $\inlinedy{\mathsf{T}}$ \emph{directly}.} For example, we'll soon see that only ${\inlinedy{\mathsf{T}}}_{\textrm{em}}$ will do for expressing the conservation of electromagnetic \emph{angular} momentum in a manifestly covariant fashion.

\subsection{Angular Momentum (Finally); Rank-3 Tensors}

We haven't yet discussed \emph{angular} momentum. Let's fix that. In Newtonian mechanics, it's defined as $\vv l_{\mathrm{classical}} = \vv r \times \vv p_{\mathrm{classical}}$, where $\vv r$ is the three-position (relative to some origin) and $\vv p_{\mathrm{classical}}$ is the classical three-momentum. We care about angular momentum because it's additive and conserved, and it turns out that it remains so in special relativity if we just replace the classical momentum with its relativistic counterpart:
\begin{equation*}
\vv l \equiv \vv r \times \vv p c
\end{equation*}
(we could drop the $c$ to give $\vv l$ its conventional units, but we won't).

Having established that Equation \ref{eq:set} is a continuity equation, where the vanishing divergence of the stress--energy dyadic $\inlinedy{\mathsf{T}}$ expresses the conservation of four-momentum, we might ask whether it's possible to likewise express the conservation of \emph{angular} momentum as the vanishing four-divergence of some sort of ``angular stress--energy tensor." The answer is yes, but first we must get our bearings by expressing angular momentum in a manifestly covariant form. 

\subsubsection{Angular Momentum Four-Dyadic}

We do this by noting that $\vv l$ is a pseudovector whose information is actually more ``naturally" conveyed in the guise of its Hodge dual, a (conserved) three-dyadic that isn't a pseudotensor:
\begin{equation*}
\star \vv l =  \star ( \vv r \times \vv p c ) = \vv r \wedge \vv p c .
\end{equation*}
From here, the covariant formulation practically writes itself:
\begin{equation*}
\capdy{\mathsf{L}} \equiv \vv R \wedge \vv P
\end{equation*}
(where $\vv R$ is the four-position relative to some spacetime origin and $\vv P$ is the four-momentum), as in matrix notation that's
\begin{equation*}
[ \capdy{\mathsf{L}} ]
=
\begin{bmatrix}
0 & [ - \vv n ]^\mathrm{T} \\[1ex]
[ \vv n ] & [ \star \vv l ]
\end{bmatrix} ,
\end{equation*}
with $\vv n \equiv E \vv r - (ct)(\vv p c)$, a conserved three-vector.

The three-vector $\vv n$ isn't of much practical use, but its conservation gives us a mathematical expression of something important that we already know: for a closed system, the vanishing time-derivative of $\sum \vv n$ (summed over all system constituents) means that the center-of-momentum frame has a constant velocity. It also establishes a relationship between the center-of-momentum frame and the system's \textbf{center of energy}, a time-dependent position given by $(\sum E \vv r)/(\sum E)$. It all works like this:
\begin{equation*}
\begin{split}
\dfrac{\dd}{\dd (ct)} \sum \left[ E \vv r -  \left( ct \right) \left( \vv p c \right) \right] &= \vv 0 \\
\dfrac{\dd}{\dd (ct)} \sum E \vv r &= \dfrac{\dd}{\dd (ct)} \sum \left( ct \right) \left( \vv p c \right) \\
&=  \sum \dfrac{\dd}{\dd (ct)} \left[ \left( ct \right) \left( \vv p c \right) \right] \\
&=  \sum \vv p c
\end{split}
\end{equation*}
(by momentum-conservation), and dividing through by $\sum E$ (which on the left side can go inside the derivative by energy-conservation):
\begin{equation*}
\dfrac{\dd}{\dd (ct)} \, \dfrac{ \sum E \vv r }{\sum E} =  \dfrac{\sum \vv p c}{\sum E}.
\end{equation*}
On the right, we have the total system momentum over the total system energy, by conservation a \emph{constant} vector for a closed system, equal to the system's $\vvbeta$ (Equation \ref{eq:19})---i.e., the normalized velocity of the system's center-of-momentum frame (relative to whatever inertial frame we're working in). On the left side, we have the time-derivative of the system's \emph{center of energy}. A closed system's center of energy therefore has a \emph{constant} time-derivative and moves inertially ``with" the center-of-momentum frame. The rest frame of a closed system is the rest frame of its center of energy.\footnote{In the Newtonian limit, where rest energy (mass) can be treated as additive, the center of energy indeed reduces to the center of mass. Generally in relativistic dynamics, however, ``center of mass" is \emph{meaningless}.}

The antisymmetric $\inlinedy{\mathsf{L}}$ is clearly close in form to the Faraday dyadic (see Equation \ref{eq:ftc}). Following our earlier procedure for the electromagnetic case, we identify the Hodge dual $\star \inlinedy{\mathsf{L}}$ as the pseudodyadic with components
\begin{equation*}
[ \star \capdy{\mathsf{L}} ]
=
\begin{bmatrix}
0 & [ \vv l ]^\mathrm{T} \\[1ex]
[ - \vv l ] & [ \star \vv n ]
\end{bmatrix}
\end{equation*}
(cf. Equation \ref{eq:gtc}). Continuing in the same vein, we can extract from our dyadics a pair of Lorentz invariants: $\inlinedy{\mathsf{L}} : \star \inlinedy{\mathsf{L}} =  4 ( \vv n \cdot \vv l )$ (cf. Equation \ref{eq:fddg}), which \emph{for a particle} is trivially $0$ (because it involves dotting a cross product with its constituent vectors), and $\inlinedy{\mathsf{L}} : \inlinedy{\mathsf{L}} = - \star \inlinedy{\mathsf{L}} : \star \inlinedy{\mathsf{L}} = 2 ( l^2 - n^2 )$ (cf. Equations \ref{eq:fddf} and \ref{eq:gddg}), which doesn't vanish but which we must remember is \emph{not} Poincar\'e-invariant (its invariance depends on a fixed spacetime origin).

\emph{For a system}, the situation with the invariants is more complicated, because then they're $( \sum \inlinedy{\mathsf{L}} ) : ( \sum \star \inlinedy{\mathsf{L}} )$ and $( \sum \inlinedy{\mathsf{L}} ) : ( \sum \inlinedy{\mathsf{L}} )$, and the first of them does \emph{not} necessarily vanish. This is analogous to how a system's rest energy (mass) isn't the sum of the rest energies of its constituents, but is rather given by the square root of $(\sum \vv P) \cdot (\sum \vv P)$. The angular-momentum invariants aren't additive.\footnote{As far as I'm aware, these invariants aren't often \emph{used} for anything. They're rarely mentioned in the literature, and I won't attempt to interpret their physical significance.}

\subsubsection[Angular Stress--Energy Tensor and Conservation]{``Angular Stress--Energy Tensor" and Conservation}

Just as the conservation of four-momentum $\vv P$ subsumes that of energy and three-momentum, the conservation of $\inlinedy{\mathsf{L}}$ subsumes that of angular momentum and $\vv n$. But four-momentum is a \emph{vector} (a rank-1 tensor), and $\inlinedy{\mathsf{L}}$ is a \emph{dyadic} (rank-2). So while the tensor whose vanishing divergence expresses the conservation of four-momentum is a dyadic ($\inlinedy{\mathsf{T}}$), the tensor whose vanishing divergence expresses the conservation of the dyadic $\inlinedy{\mathsf{L}}$ must be a rank-3 tensor (the ``angular stress--energy tensor"), which our notation frankly isn't equipped to handle.\footnote{One really must learn index notation to go deeper with this material; see Section \ref{ssec:index} below for more on why.} Without getting into the full machinery of higher-order tensors, let's sketch this out just a bit.

The tensor product is associative. This means that for three four-vectors we'd have $(\vv Q \otimes \vv W) \otimes \vv Y = \vv Q \otimes (\vv W \otimes \vv Y) = \vv Q \otimes \vv W \otimes \vv Y$, a rank-3 tensor whose $4^3 = 64$ components are all possible combinations $Q^\alpha W^\beta Y^\gamma$, where each index can be $ct$, $x$, $y$, or $z$. By linearity, we could have $\vv Q \otimes \inlinedy{\mathsf{D}}$ for some dyadic $\inlinedy{\mathsf{D}}$, and its 64 components would be all possible combinations  $Q^\alpha \mathsfit D^{\beta \gamma}$.

Now, we could further do $(\vv Q \wedge \vv W) \otimes \vv Y = (\vv Q \otimes \vv W - \vv W \otimes \vv Q) \otimes \vv Y$. What if we wanted to do this same operation, but we were given $(\vv W \otimes \vv Y)$ as a dyad and told not to remove its parentheses? That might sound like a silly question, but the point is that an expression like $\vv Q \wedge (\vv W \otimes \vv Y)$ is ambiguous until we establish a convention or invent more notation: what does $\vv Q$ get wedged with, and what's the order of the products involved? For our limited purposes, it will suffice to establish the convention that ${\vv Q \wedge (\vv W \otimes \vv Y) = (\vv Q \wedge \vv W) \otimes \vv Y}$. The upshot is that now we can wedge with a dyadic: $\vv Q \wedge (\vv W \otimes \vv Y + \vv Z \otimes \vv U) = (\vv Q \wedge \vv W) \otimes \vv Y + (\vv Q \wedge \vv Z) \otimes \vv U$. Then for some $\inlinedy{\mathsf{D}}$, we can write $\vv Q \wedge \inlinedy{\mathsf{D}}$ without ambiguity (if desired, decompose $\inlinedy{\mathsf{D}}$ into a sum of dyads and play the distribution game).

Since the angular-momentum dyadic is $\vv R \wedge \vv P$, the rank-3 ``angular stress--energy tensor" is simply $\vv R \wedge {\inlinedy{\mathsf{T}}} $.\footnote{Note that by symmetry, we don't have to choose between ${\inlinedy{\mathsf{T}}} $ and its transpose. The symmetry is crucial, as we'll see in a moment; the non-symmetric \emph{canonical} stress--energy tensor obtained via Lagrangian methods won't do here.} We want to see how its ``divergence" vanishes, and how this expresses the conservation of $\inlinedy{\mathsf{L}}$. The divergence of a rank-3 tensor must produce a dyadic (a rank-2 tensor), but it can be taken three different ways, just as there are two ways to take the divergence of a dyadic (${\partialup \cdot \inlinedy{\mathsf{D}}}$ vs. $\partialup \cdot \inlinedy{\mathsf{D}} {}^{\mathrm{T}}$). In this case, we need to choose the (unique) divergence that has no terms involving $\partialup \cdot \vv R$---only terms involving $\vv R$ or its \emph{gradient}. Our notation still isn't up to the task, but by antisymmetry and the product rule, the specified divergence of $\vv R \wedge {\inlinedy{\mathsf{T}}} $ will inevitably yield four terms that we \emph{can} say something about (schematically):\footnote{Our inadequate notation leaves us ``guessing" again about some of the details, but none that matter. All we care about is that everything adds to zero.} two like ``$\vv R \otimes (\partialup \cdot {\inlinedy{\mathsf{T}}} )$" and ``$\vv R \otimes (\partialup \cdot {\inlinedy{\mathsf{T}}} {}^{\mathrm{T}} )$," which both vanish because the stress--energy tensor is divergenceless and symmetric, and then a pair that can be grouped like ``$(\partialup \otimes \vv R) \cdot ({\inlinedy{\mathsf{T}}} - {\inlinedy{\mathsf{T}}} {}^{\mathrm{T}} )$," which likewise vanishes by symmetry. Zero!

From a field-theory perspective, one really wants the \emph{spatial-volume density} of the angular-momentum dyadic $\inlinedy{\mathsf{L}}$, which can be integrated to get the total $\inlinedy{\mathsf{L}}$ in a region. This will be just like Section \ref{sssec:tset}, where we identified $\vv B \cdot {\inlinedy{\mathsf{T}}} $ as the volume density of the four-momentum vector in the frame whose four-velocity is $\vv B$, such that the volume-integral of $\vv B \cdot {\inlinedy{\mathsf{T}}} $ (in that frame) gives the \emph{total} four-momentum in a region. Importantly, we found that, thanks to the divergencelessness of the stress--energy tensor, $\int_V (\vv B \cdot {\inlinedy{\mathsf{T}}} ) \dd V$ returns the \emph{same} value in any frame for a given isolated system (where $\vv B$ is always \emph{that frame's} four-velocity and $V$ is any purely spatial volume \emph{in that frame} which encloses the system and nothing else). With angular momentum, very little changes; we just put $\vv R \, \wedge$ in front of everything. The rank-3 ``angular stress--energy tensor" is $\vv R \wedge {\inlinedy{\mathsf{T}}}$, and then we have $\vv R \wedge (\vv B \cdot {\inlinedy{\mathsf{T}}})$ for the spatial volume density of $\inlinedy{\mathsf{L}}$. So:
\begin{equation*}
\capdy{\mathsf{L}}_{\textrm{total}} = \int_V \left( \vv R \wedge \left( \vv B \cdot {\capdy{\mathsf{T}}} \right) \right) \dd V = \int_V \left( \vv R \wedge \dfrac{\dd \vv P}{\dd V} \right) \dd V,
\end{equation*}
where the last equality holds only in the frame in question. But again, the key point is that for the system under consideration, \emph{all} inertial observers can set up their own ``version" of the first integral, and because the divergence of $\vv R \wedge {\inlinedy{\mathsf{T}}}$ vanishes, they'll arrive at the same \emph{value}, which in their own frame reduces to their own ``version" of the last equality above. This makes it possible to \emph{define} system $\inlinedy{\mathsf{L}}$ as the thereby globally conserved $\sum \vv R \wedge \vv P$. And the divergencelessness of $\vv R \wedge {\inlinedy{\mathsf{T}}}$ itself tells us that the only way a spatial volume's angular momentum changes is via transport through the boundary; that's \emph{local} conservation.

\subsubsection{Electromagnetic Angular Momentum}

Next, plugging in the \emph{electromagnetic} stress--energy tensor (Equation \ref{eq:eset}), we get (in the relevant frame):
\begin{equation*}
\capdy{\mathsf{L}}_{\textrm{total em}} = \int_V \left( \vv R \wedge \langle u, \vv s \rangle \right) \dd V
\end{equation*}
for the electromagnetic field's total angular-momentum four-dyadic in a region, $u$ being the electromagnetic energy density $(e^2 + b^2)/2$ and $\vv s$ being the Poynting vector $\vv e \times \vv b$. Careful: $u$ and $\vv s$ do \emph{not} transform like four-vector components under a Lorentz boost! \emph{In this frame} they are \emph{equal} to the components of a four-vector we're interested in, and for that reason we've used the notation $\langle u, \vv s \rangle$ (with the ``in-this-frame-only" caveat), but the components of this four-vector in a ``primed" frame will \emph{not} be $u^\prime$ and $\vv s ^\prime$. The quantities $u$ and $\vv s$ are of course part of the electromagnetic stress--energy \emph{dyadic} and transform accordingly. In matrix notation, the wedge product $\vv R \wedge \langle u, \vv s \rangle$ is:
\begin{equation*}
\begin{bmatrix}
0 & [ - (u \vv r - ct \mkern1mu \vv s ) ]^\mathrm{T} \\[1ex]
[ u \vv r - ct \mkern1mu \vv s ] & [ \vv r \wedge \vv s ]
\end{bmatrix} .
\end{equation*}
Remembering that we have:
\begin{equation*}
[ \capdy{\mathsf{L}} ]
=
\begin{bmatrix}
0 & [ - \vv n ]^\mathrm{T} \\[1ex]
[ \vv n ] & [ \star \vv l ]
\end{bmatrix} ,
\end{equation*}
with ${\vv n = E \vv r - (ct)(\vv p c)}$ and $\vv l = \vv r \times \vv pc$ (the angular-momentum pseudovector), we obtain (for the frame in question):
\begin{equation*}
\vv n_{\textrm{total em}} = \int_V \left( u \vv r - ct \mkern1mu \vv s \right) \dd V
\end{equation*}
and
\begin{equation*}
\vv l_{\textrm{total em}} = \int_V \left( \vv r \times \vv s \right) \dd V .
\end{equation*}
In free space, these field quantities are conserved.

Here comes a puzzle.

When we studied light in Section \ref{sssec:li}, we made passing reference to elliptical polarization (Footnote \ref{fn:pol}), noting that an elliptically polarized wave can always be expressed as a sum of solutions whose amplitude vectors are real (called \emph{linearly} polarized waves). An idealized elliptically polarized wave has the property that its electric and magnetic fields $\vv e$ and $\vv b$ together \emph{rotate} around the three-wavevector $\vv k$ as the wave propagates in the $\vv{\hat k}$ direction, always maintaining the relationship $\vv e = \vv b \times \vv{\hat k}$ that we established. In general, the value $e = b$ still oscillates during a cycle; in the special case that it remains constant, the wave is \emph{circularly} polarized.

There \emph{must} be angular momentum associated with the rotation of the fields in an elliptically polarized wave. Moreover, that angular momentum must in some sense be an ``intrinsic" property of the wave, and can't be made to vanish by choosing a different spatial origin or making a gauge transformation. The puzzle is this: if the electromagnetic angular momentum density is $\vv r \times (\vv e \times \vv b)$, and if an elliptically polarized wave shares with a linearly polarized wave the relationship ${\vv e = \vv b \times \vv{\hat k}}$, then how can there possibly be an ``intrinsic" contribution in the one case but not the other? The fields' derivatives aren't involved here. For a given field-magnitude, ${\vv r \times (\vv e \times \vv b) = \vv r \times e^2 \vv{\hat k}}$ whether the wave is elliptically or linearly polarized. How can we account for the angular momentum associated with the ``rotation" of an electromagnetic wave?

First, we're right: in the scenario described, our (correct) formula for the electromagnetic angular momentum density \emph{cannot} account for the intrinsic rotation of an elliptically polarized light wave. There's nothing wrong with our math or our reasoning.

What's wrong is our \emph{physics}. The kinds of waves we've been discussing are \emph{idealized} solutions. A true monochromatic plane wave would be infinite and uniform. Real-world waves aren't like that. At the very least, real waves taper off eventually in the transverse directions. For that to happen---for there to be a ``boundary" in the transverse directions where the fields vanish---the $\vv e$ and $\vv b$ fields \emph{cannot} be everywhere strictly perpendicular to $\vv k$! That's true regardless of polarization. The question is then: if we integrate $\vv r \times (\vv e \times \vv b)$ over a volume that contains a whole ``wave packet," will the wave's polarization affect our results? The answer is yes. If the wave is non-linearly polarized, then the total electromagnetic angular momentum in the region may include a contribution that doesn't depend on $\vv r$; if the wave is linearly polarized, then it will not.

To \emph{extract} from $\vv r \times (\vv e \times \vv b)$ the contribution that doesn't depend on $\vv r$, we start by writing it like $\vv r \times (\vv e \times (\del \times \vv a)) = \vv r \times (\del_{\vv a} (\vv e \cdot \vv a) - (\vv e \cdot \del) \vv a)$. The first term truly depends on $\vv r$ and might be interpreted as the ``orbital" electromagnetic angular momentum density. The second term, which we'll write like $- ( \vv e \cdot \del_{\vv a} ) ( \vv r \times \vv a )$, \emph{looks} like it depends on $\vv r$, but we'll soon see that its volume-integral doesn't (in free space, assuming the fields vanish at the boundaries). Step one is to consider this product rule:
\begin{equation*}
\del \cdot \bigl( \vv e \otimes \left( \vv r \times \vv a \right)  \bigr) = \left( \del \cdot \vv e \right) \left( \vv r \times \vv a \right) + \left( \vv e \cdot \del_{\vv r} \right) \left( \vv r \times \vv a \right) +  \left( \vv e \cdot \del_{\vv a} \right) \left( \vv r \times \vv a \right) ,
\end{equation*}
where we spot the negative of our $- ( \vv e \cdot \del_{\vv a} ) ( \vv r \times \vv a )$ all the way on the right side. By the free-space condition and Maxwell's equations, $\del \cdot \vv e = 0$, and we can rewrite $- ( \vv e \cdot \del_{\vv a} ) ( \vv r \times \vv a )$ like this: $ ( \vv e \cdot \del_{\vv r} ) ( \vv r \times \vv a ) - \del \cdot ( \vv e \otimes ( \vv r \times \vv a ) )$. But $(\vv e \cdot \del)\vv r$ (directional derivative of position with respect to $\vv e$) is just $\vv e$ (write out the components if you need convincing, but think about it geometrically), and so we're left with $(\vv e \times \vv a)$ minus a divergence. Upon volume-integration, the divergence theorem turns the divergence term into a \emph{surface} integral of $\vv e \otimes ( \vv r \times \vv a )$, and that vanishes by our boundary condition, giving $\int_V (\vv e \times \vv a) \dd V$ for the total ``intrinsic" (or ``spin") electromagnetic angular momentum in the region, which does not depend on $\vv r$. There's a big caveat: this term isn't gauge-invariant! It's true that \emph{whether it vanishes} is a gauge-invariant fact (and it will vanish for linear polarization), so in that sense it has \emph{some} significance, but its gauge-dependence means it isn't a physically measurable quantity. Only the \emph{total} angular momentum $\int_V (\vv r \times (\vv e \times \vv b)) \dd V$ is physically meaningful in classical electrodynamics. It can't be unambiguously decomposed into ``orbital" and ``spin" parts.


\subsection{More on Tensors; The Limits of Dyadic Notation}\label{ssec:index}

From our half-hearted foray into rank-3 tensors in the previous section, it's obvious that our notation is poorly suited for tensors of rank higher than 2. What gives?

A tensor of rank-$k$ is really a special type of \emph{function} that takes up to $k$ vectors as input and returns a tensor of rank $(k - n)$ as output, where $n$ is the number of vectors supplied.\footnote{\label{fn:ten}In more advanced treatments, this definition is modified so that a tensor can also take \emph{covectors} as input (we've mentioned covectors a few times in passing, first in Footnote \ref{fn:cv}). Each of a tensor-function's input-slots is restricted to accepting either a vector or a covector, but it must be one or the other (though a given tensor can have a mix of vector-only slots and covector-only slots). For instance, a vector takes a covector as input, but a vector cannot take another vector as input (there is no dot product yet). The metric tensor---if introduced---creates a one-to-one mapping between vectors and covectors. This allows for each input-slot of a tensor to be ``converted" into an input-slot of the other kind via the metric. Technically this ``conversion" results in a different tensor, but \emph{effectively} it means that each slot can accept either type of input, vector or covector (not just one or the other). When the metric is used in this way to effectively allow a vector to act on another vector (which really means feeding the metric tensor this pair of vectors, or, equivalently, using the metric to map one vector to its covector counterpart and then feeding this covector the other vector), we have a dot product. \emph{Our} approach takes the existence of the metric and the dot product for granted, and ignores covectors altogether.} For example, a dyadic $\inlinedy{\mathsf{D}}$ is a tensor of rank-2, which makes it a function that takes up to two vectors as input and returns either a tensor of rank-$1$ (a vector) when supplied with one vector, or a tensor of rank-$0$ (a scalar) when supplied with two. Feeding it a single vector $\vv Q$ corresponds to what we've called \emph{dotting} it with $\vv Q$, but we already know that there are two ways to do this: $\vv Q \cdot \inlinedy{\mathsf{D}} \neq \inlinedy{\mathsf{D}} \cdot \, \vv Q$. In function-speak, this means that the \emph{order} in which vectors are supplied to a tensor matters; if we write $\inlinedy{\mathsf{D}}$ as the function $D(\, \cdot \, , \, \cdot \,)$ and $\vv Q$ as the function $Q(\, \cdot \, )$ (where the dots signify empty slots for inputting vectors), then $D(Q, \, \cdot \,)$ and $D(\, \cdot \, , Q)$ are two \emph{different} vectors. With the \emph{tensor product}, which produces a tensor of rank $(k + l)$ when taken between tensors of rank-$k$ and rank-$l$, order likewise matters, as we know from our dyadic product: $Q(\, \cdot \, ) \otimes W(\, \cdot \, ) \neq W(\, \cdot \, ) \otimes Q(\, \cdot \, )$.

We could probably add new symbols to our dyadic notation to better accommodate rank-3 tensors, but it would be cumbersome. How would we keep track of the \emph{order} of all the function-slots once we're beyond rank-2? Our notation doesn't \emph{have} slots. Rank-2 is okay, since we can distinguish between a dot on the left and a dot on the right (two sides for two slots), we can define a simple transpose, and we can display the components in a square matrix. Once we hit rank-3, however, we're out of ``sides" (unless we bring up and down into the mix), we don't have a straightforward ``transpose," and we'd need a cubic array to display the components. The problem compounds with each new rank. The added machinery would not be pretty!\footnote{And what of the need to accommodate covectors in more advanced treatments, as described in Footnote \ref{fn:ten}? This is sometimes a problem in function notation, too.}

Now, that's not to say that the function notation we've just introduced solves the problem entirely. It doesn't. For instance, what about our \emph{double} dot product, which takes two dyadics to a scalar? How can $D(\, \cdot \, , \, \cdot \,)$ and $K(\, \cdot \, , \, \cdot \,)$ produce a rank-0 tensor? Well, we can put the dyadics into sum-of-dyads form and add up dot-product pairs of the dyads' constituent vectors, just as we showed at the beginning of Section $\ref{ssec:dd}$. But how can this be expressed in the function notation? Not easily! First, we have to define an operation called \textbf{contraction}, which for a pair of \emph{vectors} means forming $\vv Q \cdot \vv W$ from the tensor product $Q(\, \cdot \, ) \otimes W(\, \cdot \, )$, reducing the tensor's rank by two (contraction always reduces the rank by two). We say that we contract the first and second slots of the dyad $Q(\, \cdot \, ) \otimes W(\, \cdot \, )$. To extend the notion of contraction to rank-2 tensors generally, we do the sum-of-dyads thing, so that given $\inlinedy{\mathsf{D}} = \vv Q \otimes \vv W + \vv S \otimes \vv U + \ldots \,$, the contraction on the first and second slots of $D(\, \cdot \, , \, \cdot \,)$ is just $\vv Q \cdot \vv W + \vv S \cdot \vv U + \ldots \,$. For the \emph{double} dot product between $D(\, \cdot \, , \, \cdot \,)$ and $K(\, \cdot \, , \, \cdot \,)$, we have to contract over two different pairs of slots (a double contraction). This is a case where our dyadic notation is clearly better, since once we've defined our double dot products, we can write $\inlinedy{\mathsf{D}} : \inlinedy{\mathsf{K}}$ and $\inlinedy{\mathsf{D}} \cdot \cdot \inlinedy{\mathsf{K}}$, which are succinct and unambiguous. In function notation, we'd first take the tensor product $D(\, \cdot \, , \, \cdot \,) \otimes K(\, \cdot \, , \, \cdot \,)$ to form a tensor of rank-4, but we'd then need to somehow \emph{specify} which slot-pairs get contracted. For $\inlinedy{\mathsf{D}} : \inlinedy{\mathsf{K}}$, we're contracting the first and third slots and also the second and fourth slots of  $D(\, \cdot \, , \, \cdot \,) \otimes K(\, \cdot \, , \, \cdot \,)$. Maybe we'd notate that like ${\textrm{cont}_{1,2}(\textrm{cont}_{1,3}(D(\, \cdot \, , \, \cdot \,) \otimes K(\, \cdot \, , \, \cdot \,)))}$, since after the first (inner) contraction there are only two slots. That's a bit confusing. Instead we could do  ${\inlinedy{\mathsf{D}} : \inlinedy{\mathsf{K}} = \textrm{cont}_{1,3; \, 2,4}(D(\, \cdot \, , \, \cdot \,) \otimes K(\, \cdot \, , \, \cdot \,))}$, signaling both contractions simultaneously, and we'd have $\inlinedy{\mathsf{D}} \cdot \cdot \inlinedy{\mathsf{K}} = \textrm{cont}_{1,4; \, 2,3}(D(\, \cdot \, , \, \cdot \,) \otimes K(\, \cdot \, , \, \cdot \,))$.\footnote{The \emph{single} dot product between dyadics would be $\inlinedy{\mathsf{D}} \cdot \inlinedy{\mathsf{K}} = \textrm{cont}_{2,3}(D(\, \cdot \, , \, \cdot \,) \otimes K(\, \cdot \, , \, \cdot \,))$.} Better, but still not great, and it wouldn't scale well with tensor-rank and more contractions. So while function notation has the virtues of clarity, explicit slots, and consistency across tensors of all ranks, we see that there are some operations that it can handle only clumsily.

Index notation is pretty close to a panacea. It has a learning curve, but it remedies every shortcoming of other notations that we've named, and then some.\footnote{For example, it uses ``upstairs" and ``downstairs" indices to make the vector/covector distinction (see Footnote \ref{fn:ten}).} It's got one ``weakness," which is also what makes it so powerful: with it, one manipulates \emph{components} of tensors almost exclusively (without choosing any particular coordinate system), as opposed to the tensors themselves.\footnote{A variation on index notation, called \emph{abstract} index notation, provides a way to regard some indexed objects as tensors rather than tensor-components.} Conceptually there's maybe something unsatisfying about this, especially when it becomes necessary to introduce components of non-tensors; for this reason, it's nice to complement index notation with other notations when feasible. But the index notation boasts compact, commutative terms, it's extremely scalable, and it comes with a mechanism for contraction that couldn't be sleeker or simpler. Once one learns the rules and masters a few basic techniques, one finds that difficult-seeming calculations almost write themselves! The notation often \emph{tells you} what to do, and prevents you from making many types of mistakes. If I've sold you and you're ready, I recommend Pavel Grinfeld's online book, \emph{An Introduction to Tensor Calculus}, accessible at \url{https://grinfeld.org/}.

This section on ``electrodyadics" (patent pending) is partly an exercise in going as far as reasonably possible without getting too deep into tensor analysis. Of course I hope that readers find it educational, but anyone looking to go further with physics \emph{must} learn index notation.

\clearpage

\section*{References}\label{sec:r}
\addcontentsline{toc}{section}{References}

Like anyone who writes something like this, I've been influenced by too many sources to remember. Here, I'll discuss some that left an imprint on this document in specific ways I can point to.

Before I started studying physics seriously, I learned a great deal from Matt Strassler's blog for non-specialists, and especially the article \href{https://profmattstrassler.com/articles-and-posts/particle-physics-basics/mass-energy-matter-etc/mass-and-energy/}{Mass and Energy}. Some of my framing in Section \ref{sec:rd} certainly owes something to Strassler, who probably does ``advanced" science communication in physics better than anyone.

My general approach to dynamics is \emph{most} strongly influenced by Lev B. Okun, \emph{Energy and Mass in Relativity Theory} (Singapore: World Scientific, 2009). In fact, while the idea to use rest energy ``instead of" mass was my own, I later discovered that I wasn't the first to consider it, and that Okun himself had explicitly rejected it (p. 262 of the cited book). Well, what he \emph{really} rejected was the notion that the physics world should ditch the word ``mass" altogether. To be clear, I do not advocate for that. My emphasis on rest energy in the context of special relativity is merely pedagogical (and personal preference).

Committing to lowercase for three-vectors (including the electric and magnetic fields) and uppercase for four-vectors is straight from Wolfgang Rindler, \emph{Relativity: Special, General, Cosmological}, 2nd ed. (Oxford: Oxford, 2006). Another influence notation-wise is Richard A. Mould, \emph{Basic Relativity} (New York: Springer, 1994). Mould uses the naught subscript consistently for ``proper" quantities (actually he uses a little square, like $E_{{}_\square}$), and he makes sure to use \emph{upright} Greek characters when they're for vectors (similar to my $\vvbeta$ and $\vvzeta$). The germ of some of my ideas about ``handling" $c$ comes from Mould, too. Part of his strategy is to use a special symbol for the quantity $ct$. I found that confusing (not least because he chooses $\tau$, which is often reserved for proper time), but it did get me thinking carefully about which $c$'s to ``hide" and what notation to introduce to that end. Eventually I landed on hiding most $c^{-1}$'s by defining ``normalized" quantities like $\vvbeta$ and $\vvzeta$, and using the overdot and over-circle notation for $ct$- and $ct_0$-derivatives.

My interest in doing electrodynamics ``relativity-first" was initially sparked by the ``relativity \emph{very early}" treatment in Melvin Schwartz, \emph{Principles of Electrodynamics} (New York: McGraw-Hill, 1972). The book is a gem. I cribbed from its Chapter 5 the easy ``$.5 ( \inlinedy{\mathsf{F}} \cdot \inlinedy{\mathsf{F}} + \inlinedy{\mathsf{G}} \cdot \inlinedy{\mathsf{G}} )$" method of obtaining the electromagnetic stress--energy tensor (see Section \ref{sssec:sdfd}).

My handling of electromagnetism also bears Richard Feynman's influence, in at least two ways. First, I use his three-del subscript notation (see Footnote \ref{fn:fsn}). Second, I introduce the four-del \emph{directly as a four-vector operator}, forgoing discussion of covectors (see \href{http://www.feynmanlectures.caltech.edu/II_25.html#Ch25-S3}{Volume 2, Chapter 25} of \emph{The Feynman Lectures on Physics}). These may seem like small things, but they proved essential for maximizing the accessibility of the presentation without sacrificing its manifestly covariant approach. Actually, the key was to \emph{combine} them by applying the subscript notation to the four-del. This allowed me to define the ``differential four-vector triple product" (Section \ref{sssec:tp}) and cover both the Maxwell equations and the Lorentz force entirely in the language of four-vectors.

A more recent text that helped me better frame electrodynamics as a classical field theory is Robert M. Wald, \emph{Advanced Classical Electromagnetism} (Princeton: Princeton, 2022). In particular, it emphasizes the fundamental nature of \emph{continua} in this domain, even though charge is in reality discrete; accordingly, it treats the Lorentz force on a particle as a limiting case of the Lorentz force \emph{density} (which is itself ``downstream" from the stress--energy tensor and the field equations). While the structure of Sections \ref{sec:rem} and \ref{sec:dy} doesn't hew to Wald's schema, I note in several places what the more sophisticated line of attack would be.

Section \ref{sec:dy} on ``electrodyadics" is \emph{nearly} original in its simultaneous reliance on dyadics and avoidance of covectors and index notation. But not quite, it turns out! After writing the core of it, I discovered an unpublished(?) paper that advocates for something close to this approach on pedagogical grounds that more or less mirror my own reasoning: Jonas Larsson and Karl Larsson, ``An introduction to relativistic electrodynamics, Part I: Calculus with 4-vectors and 4-dyadics" (Ume\r{a} University, 2018), downloadable at {\url{https://urn.kb.se/resolve?urn=urn:nbn:se:umu:diva-152747}}. One thing I did borrow from Larsson and Larsson is the ``quasi-vector"/``quasi-dyadic" concept introduced in Section \ref{sssec:hd} to define the Hodge dual of the Faraday tensor geometrically, though they use different notation and terminology (the ``quasi-" business is my own invention). Another source that uses that concept, but simply as a means of illuminating the nature of the electric and magnetic fields themselves, is Kip S. Thorne and Roger D. Blandford, \emph{Modern Classical Physics: Optics, Fluids, Plasmas, Elasticity, Relativity, and Statistical Physics} (Princeton: Princeton, 2017), 72--74.

A work that \emph{did} inspire my overall strategy for Section \ref{sec:dy} is E. G. Peter Rowe, \emph{Geometrical Physics in Minkowski Spacetime} (London: Springer, 2001), which uses dyadics but doesn't avoid index notation or covectors and the like. It's an excellent book that deserves to be better known, though it was published in an unfinished state after Rowe's \href{https://en.wikipedia.org/wiki/December_1998_tourist_kidnappings_in_Yemen}{tragic 1998 death} (Wikipedia).

\end{document}