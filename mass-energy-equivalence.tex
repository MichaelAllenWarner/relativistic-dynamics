\documentclass[12pt]{article}
\usepackage{gensymb}
\usepackage[tbtags]{amsmath}
\usepackage{bm, upgreek}
\usepackage{amssymb}
\usepackage{hyperref}
\hypersetup{bookmarksnumbered,colorlinks=true, linkcolor=yaleblue, urlcolor=yaleblue}
\usepackage{xcolor}
\definecolor{dimgray}{rgb}{0.41, 0.41, 0.41}
\definecolor{airforceblue}{rgb}{0.36, 0.54, 0.66}
\definecolor{yaleblue}{rgb}{0.06, 0.3, 0.57}
\definecolor{tealblue}{rgb}{0.21, 0.46, 0.53}
\usepackage{textcomp}
\usepackage{centernot}
\usepackage{ragged2e}
\usepackage{esvect}
\renewcommand{\vv}[1]{\mathbf{#1}}
\newcommand{\dd}[1]{\mathrm{d}#1}
\newcommand{\vvbeta}{\bm{\upbeta}}
\newcommand{\hatbeta}{\bm{\hat{\upbeta}}}
\newcommand{\vvomega}{\bm{\upomega}}
\newcommand{\vvphi}{\bm{\upphi}}
\newcommand{\hatphi}{\bm{\hat{\upphi}}}
\newcommand{\vvzeta}{\bm{\upzeta}}
\newcommand{\hatzeta}{\bm{\hat{\upzeta}}}
\newcommand{\vvsigma}{\bm{\upsigma}}
\newcommand{\del}{\boldsymbol{\nabla}}
\newcommand{\tightoverset}[2]{%
  \mathop{#2}\limits^{\vbox to -.5ex{\kern-0.75ex\hbox{$#1$}\vss}}}
\newcommand{\inlinedy}[1]{\tightoverset{\text{\tiny$\bm\leftrightarrow$}}{#1}}
\newcommand{\fnoverset}[2]{%
  \mathop{#2}\limits^{\vbox to -.5ex{\kern-0.85ex\hbox{$#1$}\vss}}}
\newcommand{\footnotedy}[1]{\fnoverset{\text{\tiny$\bm\leftrightarrow$}}{#1}}
\newcommand{\capdy}[1]{ \overset{ \text{\tiny$\bm\leftrightarrow$} }{\vphantom{\text{\small{A}}}\smash{#1}} }
\usepackage{tkz-euclide}
\usetkzobj{all}
\usepackage{tikz}
\usepackage{tkz-tab}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hypcap}
\usepackage{cancel}
\interfootnotelinepenalty=10000
\DeclareFontEncoding{FML}{}{}%
\DeclareFontSubstitution{FML}{futm}{m}{it}%
\DeclareSymbolFont{fourier}{FML}{futm}{b}{it}%
\DeclareMathSymbol{\partialup}{\mathord}{fourier}{130} % Upright partial derivative symbol

\begin{document}

\title{Energy, Momentum, and Four-Vectors:\\A Gentle, ``Massless" Introduction to\\Relativistic Dynamics and Electromagnetism}
\author{Michael Allen Warner}
\date{Last updated: \today}
\maketitle

\section{Preface}\label{sec:p}

This is the gentlest introduction to relativistic dynamics you'll find. It assumes that you're already familiar with basic relativistic kinematics, but we'll review what we need in Section \ref{ssec:sr}. Through Section \ref{sec:ra}, the only calculus we'll use is simple single-variable differentiation. Section \ref{sec:ak} involves some integration, and Section \ref{sec:rem} has vector calculus. The rest of the math is mostly high-school stuff.

I've made a couple of unconventional choices in notation:
\begin{itemize}
\item Time-derivatives are taken with respect to $ct$, not $t$. This applies to Newton's overdot notation, too. For example, where $\vv r$ is position and $\vv v$ velocity, $\dot{\vv r} \neq \dd \vv r / \dd t = \vv v$. Instead, $\dot{\vv r} = (\dd \vv r / \dd t)/c \equiv \vvbeta$. Likewise, where $\vv a$ is acceleration, $\ddot{\vv r} = \dot{\vvbeta} = (\dd \vvbeta / \dd t)/c = \vv a / c^2 \equiv \vvzeta$. Once I introduce the dimensionless $\vvbeta$ (and the dimensionful $\vvzeta$), I treat it not as shorthand but as the natural way to express velocity. I have no use for $\vv v$ (or $\vv a$).
\item For $c t_0$-derivatives (where $ct_0$ is \emph{proper} time), I use a little circle instead of a dot: $\mathring{\vv r} = (\dd \vv r / \dd t_0)/c$.
\end{itemize}
I'll remind you of this dot and circle business along the way.

The ``massless" in the title is half tongue-in-cheek. The serious bit is that beyond Newtonian mechanics, I don't use the word \emph{mass} or the symbol $m$. This does set my approach apart---in a helpful way, I hope you'll agree---but you'll see that it's just a linguistic and notational preference. There's nothing radical about it.

On vector notation: vectors are boldface, their magnitudes are italic (like any scalar), a vector in component form is enclosed by angle brackets (not parentheses), and squaring a vector means taking its dot product with itself, which is equivalent to squaring its magnitude. So velocity is $\vv v = \langle v_x, v_y, v_z \rangle$, speed is $v$, and $\vv v^2 = \vv v \cdot \vv v = v^2$. Occasionally italics aren't suitable for notating a vector's magnitude. A good example is the vector $\Delta \vv r$, because $\Delta r$ already signifies the change in $r$ (magnitude of position $\vv r$). In such cases we'll indicate the vector's magnitude with double bars instead (e.g., $\Vert \Delta \vv r \Vert$).


\section[Einstein's Thought Experiment without Einstein]{Einstein's Thought Experiment\\without Einstein}
 
\subsection{So This Body Emits Two Light Waves \ldots}
 
In Einstein's original ``$E=mc^2$" paper,\footnote{Einstein, A.: Ist die Tr\"agheit eines K\"orpers von seinem Energieinhalt abh\"angig? Ann.\ Phys.\ \textbf{18}, 639--641 (1905). English translation here: \url{https://www.fourmilab.ch/etexts/einstein/E_mc2/www/}.} he proposes a thought experiment in which a body at rest briefly emits two equally energetic light waves in opposite directions at the same time.\footnote{We treat them as idealized monochromatic plane waves, though we'll soon temporarily invoke the photon model as a shortcut (later we'll reproduce our results without it).} With the aid of his newly developed special theory of relativity, he considers the situation from the perspectives of two observers: one at rest relative to the body, the other moving with an arbitrary constant velocity.
 
The paper is concise---almost perfunctory---but it's also rather subtle. We're going to walk through the thought experiment twice. The first time we'll feign ignorance of special relativity, putting ourselves in the shoes of a physicist in mid-1905. We'll fail, but our failure will be instructive. On the second pass, we'll retrace Einstein's steps (more or less).

Diving right in, what can we say about that light-emitting body if we have Newtonian physics and Maxwell's equations (plus the Planck--Einstein relation), but no special relativity? To start, classical mechanics gives us linear momentum
\begin{equation}\label{eq:1}
\vv p=m\vv v \quad \text{\footnotesize{ (classical)}}
\end{equation}
and kinetic energy
\begin{equation}\label{eq:2}
E_{\mkern.5mu \textrm{k}}= \frac{1}{2} \, m v ^2 \quad \text{\footnotesize{ (classical)}},
\end{equation}
where $m$ is mass and $\vv{v}$ velocity. We also have the \textbf{conservation} laws for mass, momentum, and total energy $E$ (i.e., $\Delta m_{\mathrm{system}} = \Delta E_{\mathrm{system}} = 0$, and $\Delta \vv p_{\mathrm{system}} = \vv 0$), valid for \textbf{closed} systems that don't interact with the outside world. Truly closed systems only exist in thought experiments, though. For an \textbf{open} system, any change of $m_{\mathrm{system}}$, $E_{\mathrm{system}}$, or $\vv p_{\mathrm{system}}$ is equal-and-opposite to a corresponding change in the environment, meaning that these three conserved quantities are also \textbf{additive}:\footnote{In this paper we distinguish between conservation and additivity, but you should be aware that many treatments lump both properties together to define \emph{conservation} (sometimes only tacitly). Their ``conserved" is our ``additive and conserved."} a system's mass, momentum, and total energy are the sums (respectively) of the masses, momenta, and energies of the system's constituents.\footnote{\label{fn:ad}This is how things stand without special relativity, anyway. If we find that any of this additivity breaks down, we'll want to take note.} In particular, a system's total energy is the sum of \emph{all} energy contributions, kinetic and otherwise.

Because Equations \ref{eq:1} and \ref{eq:2} depend on velocity, a closed system's momentum and total energy remain unchanged only for inertial observers. Different inertial observers generally don't agree on the \emph{value} of a closed system's momentum or total energy, but they agree that the value doesn't change.

And then there's Maxwell. From him, we know that light is a wave that carries energy and momentum in directly proportional magnitudes, even though it has no mass. In other words---and this should make us deeply uncomfortable---light is ``exempt" from Newton's mass-dependent Equations \ref{eq:1} and \ref{eq:2} (which would return values of zero!), but it still carries energy and momentum
\begin{equation}\label{eq:3}
E \propto p >0 \quad \text{\footnotesize{ (for light)}} .
\end{equation}
Yet, in early 1905, Einstein proposed (correctly) that, \emph{pace} Maxwell, the energy (and momentum) of a \textbf{photon}---a corpuscle of light---is directly proportional to the light's \emph{frequency} $\nu$, an idea that Max Planck had floated in 1900:
\begin{equation}\label{eq:4}
E \propto \nu \quad \text{\footnotesize{ (for photons)}}.
\end{equation}
We're committing a minor sin by mentioning photons at all in our non-quantum context, but this Planck--Einstein relation will allow us to take a shortcut, and in Section \ref{sec:rem} we'll redeem ourselves by replicating our results without it. Besides, it's not as though Planck and Einstein were just making things up; Equation \ref{eq:4} explained empirical results that Maxwell's wave model couldn't, such as the photoelectric effect.

Now on to the thought experiment!

First we consider things from the perspective of the observer at rest relative to the body. In this observer's frame, the body's initial speed is zero, which means its initial momentum and kinetic energy are also zero. The light waves, however, carry energy, and that energy had to come from somewhere. Did it come from the body's kinetic energy?

No, because the $E_{\mkern.5mu \textrm{k}}$ was already zero, and Equation \ref{eq:2} tells us that $E_{\mkern.5mu \textrm{k}}$ can't be negative (mass is always positive). Here's another argument that leads to the same conclusion: By Equation \ref{eq:3}, our equally energetic light waves carry momenta that are equal in magnitude. Since those momenta have opposite direction, the vectors cancel, leaving the body's momentum unaffected. The massless light likewise leaves the body's \emph{mass} unaffected.\footnote{\label{fn:ma}Or so we think. If it turns out that mass isn't additive, then we'll have to be open to the possibility that the body's mass \emph{is} affected, even though light is massless.} If neither the body's momentum nor its mass changes, then neither does its velocity (Equation \ref{eq:1})---the body remains at rest. And if neither its mass nor its velocity changes, then neither does its kinetic energy (Equation \ref{eq:2}).

\subsection{Systems and Rest Energy All the Way Down}\label{ssec:sy}

This energy carried off by the light waves---where did it come from? All we really know is that it came from the resting body itself, and not from the body's kinetic energy, which remains zero for our first observer. By conservation and additivity of energy, anything at rest that can radiate energy without moving must already have some to lose, \emph{independent} of its motion.

That observation may strike you as trivial, especially if you're familiar with the concept of \textbf{internal energy} from thermodynamics. But it isn't trivial, and the category of energy we're introducing is more general than internal energy. Internal energy specifically refers to the kinetic and potential energy associated with the molecules of a substance, transferable by work or heat (or substance transfer). By contrast, we haven't specified what energy was lost by the body---molecular kinetic? potential? something else?---and it was transferred not by work or heat but by electromagnetic radiation.\footnote{Some people count radiation as ``heat" in certain circumstances, but that's beside the point.}

The energy we're talking about is something of a black box. Simply put, it's \emph{any and all energy a system has that's independent of its motion}. Because this energy is independent of the system's motion, all observers will agree that it's there and how much of it there is.\footnote{\label{fn:re}At this stage, don't worry about how they'd measure this energy. We'll get there.} To an observer who shares a body's rest frame, the body has zero $E_{\mkern.5mu \textrm{k}}$, and this new category of energy constitutes the \emph{entirety} of the body's total energy $E$. So we can think of it as the total energy a system has when it's at rest. For that reason, it's usually called \textbf{rest energy} (or sometimes \textbf{proper energy}). Let's give it the symbol $E_0$. By the additivity of energy, we can now say of any system:
\begin{equation}\label{eq:5}
\boxed{E=E_0+E_{\mkern.5mu \textrm{k}}} \, ,
\end{equation}
where different observers disagree on the velocity-dependent $E_{\mkern.5mu \textrm{k}}$ but agree on $E_0$. In the system's rest frame, Equation \ref{eq:5} becomes
\begin{equation}\label{eq:6}
E=E_0 \quad \textrm{\footnotesize{ (for $E_{\mkern.5mu \textrm{k}}=0$)}}.
\end{equation}
A quantity like rest energy whose value everyone agrees on is said to be \textbf{invariant}.\footnote{\label{fn:inv}Really, one must specify the \emph{context} in which a quantity is invariant. Take kinetic energy: it certainly isn't invariant with respect to a change in observer velocity (a \textbf{boost}), but it \emph{is} invariant under rotation of an observer's spatial axes. In this paper, when we say that a quantity is invariant, it's implied that we mean with respect to observer velocity. Also: in more advanced treatments, the word \textbf{scalar} is reserved for \emph{invariant} quantities (rank-0 tensors, technically), but we'll use the looser definition that a scalar is \emph{any} physical quantity describable by number (and unit) alone, in contrast with vectors. So we'll regard total energy and kinetic energy as scalars, and rest energy as an \emph{invariant} scalar.} Mass is another invariant, and (spoiler alert) we'll soon discover a close connection between mass and rest energy.

If your instinct is to resist Equation \ref{eq:5} on the grounds that an open system may also have potential energy owing to its position relative to some outside body, remember that the potential energy in that case is a property not of the open system or the outside body, but rather of the greater system that encompasses both. It's accounted for in the greater system's \emph{rest energy}. Equation \ref{eq:5} works. Resistance is futile.\footnote{Okay, one caveat: we're assuming that the potential energy isn't associated with the kind of gravitational field that would require the \emph{general} theory of relativity to analyze. Actually, let's broaden that caveat---for this entire paper, \emph{assume the absence of gravity}!}

Again, we've stipulated nothing about the ``inner workings" of rest energy. From the point of view of an outside observer trying to find a body's total energy $E$, all that matter are the values $E_{\mkern.5mu \textrm{k}}$ and $E_0$; the physical phenomena behind that $E_0$ are irrelevant. But suppose we're curious and peer inside. What do we see? Equation \ref{eq:6} and the additivity of energy tell us only that whatever we see, if we sum up all the ``inside" energy contributions to find $E$ in the body's rest frame, then we'll get exactly the same value that anyone ``outside" gets for $E_0$. Those energy contributions could be anything. Maybe the body is full of gas molecules of various types, in which case the molecules have kinetic energy associated with their motion and some (negligible) potential energy associated with their relative positions, but each molecule \emph{also has its own rest energy}, further itemizable as the kinetic, potential, and rest energies associated with its constituent atoms and their relative positions. Regardless of the details, we must add up \emph{all} the ``inside" energy contributions---kinetic energies, potential energies, rest energies---to calculate the body's total energy $E$ in its rest frame. If we're thorough, then Equation \ref{eq:6} guarantees that our sum will match the number that outside observers assign $E_0$.

It's worth emphasizing that anything or any group of things can be regarded as a system. And if a system has a rest frame, then it has rest energy. But what \emph{is} a rest frame? It's self-explanatory when a non-rotating system is confined to a fixed volume (like the body in Einstein's thought experiment); then it's just the frame in which the system's bounds aren't moving. More generally, though, it's the inertial frame in which all the momenta within a system sum to a zero vector. Another name for it is the \textbf{center-of-momentum frame}.\footnote{In Newtonian physics, this frame is identical to the center-of-\emph{mass} frame. Not so in relativistic mechanics (hint: Footnote \ref{fn:ma}). It's momentum that we care about here.} If a system isn't confined to a fixed volume, this special frame can be difficult or altogether unfeasible to find, but in principle one can still determine it.\footnote{\label{fn:rl}Unless the system \emph{has no rest frame}, and thus no rest energy ($E = E_{\mkern.5mu \textrm{k}}$). Such systems are \textbf{restless}, and we'll discuss them more in Section \ref{ssec:em}.} When we speak of a system's velocity, we are referring to the velocity of its rest frame relative to some observer.

Now, imagine the universe as systems within systems within systems. In Einstein's thought experiment, the closed system consists at first of only the body, but then of the body plus the two light waves. And although the body loses rest energy, the \emph{closed system} does not. Equation \ref{eq:6} tells us that to find the system's rest energy, we find the system's total energy in its rest frame, which, after emission, means accounting for the body's rest energy \emph{and also the energy carried away by the light waves}! That's why our closed system's rest energy doesn't change. A closed system's rest energy is both invariant \emph{and conserved}, because total energy is conserved for all inertial observers, including those in the system's rest frame.

If we ``zoom in" and regard the \emph{body} as our (open) system, we'll find that it does lose rest energy, but that Equations \ref{eq:5} and \ref{eq:6} still hold. So an \emph{open} system's rest energy is invariant but \emph{not} conserved, which again is just another way of saying that the total energy of an open system can change, and that observers in the rest frame aren't blind to this. Unlike total energy, however, rest energy is \emph{not} additive! Rest energy is the sum of \emph{all} energy contributions in the rest frame, not just the sum of the constituent rest energies.

If we zoom in further and treat, say, a CO$_2$ molecule in the body as our system, we can again describe its energy using Equations \ref{eq:5} and \ref{eq:6}. The molecule consists of carbon and oxygen atoms with their own rest and kinetic energies, and there's also some (comparatively negligible) potential energy associated with the chemical bonds keeping them together.\footnote{\label{fn:be}This \textbf{binding energy} is actually negative, since energy must be invested to decompose a bound system. And binding energy certainly isn't negligible at the level of an individual atom or nucleus! For instance, the rest energy of a bound proton--neutron system (a \textbf{deuteron}) is measurably less than the sum of the rest energies of the two constituent nucleons---by about 2.224 mega-electron-volts (MeV), which is well over four times the rest energy of an electron.} To find the molecule's total energy $E$ in its rest frame, we just add up all of those energy contributions, and, lo and behold, our tally matches what outside observers call the molecule's rest energy $E_0$.

We can keep zooming in. When we get to the smallest bound systems, we find that kinetic and potential energies contribute substantially to system rest energy.\footnote{To be clear, not all potential energy is negative like binding energy is (see Footnote \ref{fn:be}). Potential energy associated with an \emph{attractive} force is negative, but potential energy associated with a \emph{repulsive} force is positive. Either way, we're constrained here to define the potential energy such that it approaches zero in the limit that the system's constituents are infinitely far apart. That's because we're no longer only interested in \emph{differences} in potential energy. The invariance of rest energy demands that all ``inside" energy contributions have an unambiguous value.} In fact, only some 1\% of a proton's or neutron's rest energy comes from the rest energies of its constituent quarks. The other 99\% of the ``inside" energy contributions are kinetic and potential, in roughly equal measure.\footnote{\label{fn:mwm}Because the rest energies of nucleons in turn constitute almost all of the rest energy of the macroscopic world, we can say that, at heart, the rest energy of everything around us is mainly ``non--rest energy" within protons and neutrons.} And although it takes quantum chromodynamics to suss out the details, Equations \ref{eq:5} and \ref{eq:6} work just fine for nucleons.

We can even take an elementary particle like an electron as our system. We can't speak of an electron having an ``inside," but it obeys Equations \ref{eq:5} and \ref{eq:6} nevertheless, and it has a rest energy that's now known to arise from the Higgs mechanism (about 0.511 MeV).

It's systems and rest energy all the way down!

Let's zoom back out, lest we forget that the beauty of the rest-energy concept is that it permits us to \emph{ignore} the energy contributions at the ``zoomed-in" levels. If we're concerned with a macroscopic system, there's no need to add up all the kinetic, potential, and rest energies associated with the elementary particles. Thanks to our new equations and the additivity of total energy, those contributions are automatically included in the system's invariant $E_0$. We just need a way to measure it---and we'll have one soon (I promise!). Once we know $E_0$, it's a cinch to find the system's total energy $E$ in any frame: merely add its velocity-dependent $E_{\mkern.5mu \textrm{k}}$, as per Equation \ref{eq:5}.

Equations \ref{eq:5} and \ref{eq:6} are simple but profound. Get to know them.

\subsection{The Catch}\label{ssec:tc}

Now let's return to Einstein's thought experiment and ponder how things are for the observer moving at some constant velocity relative to the body.

In this observer's rest frame, the body starts with non-zero velocity, momentum, and kinetic energy. What changes when the light is emitted? Certainly not the body's velocity---if it's constant for one observer, it's constant for the other. And since the body's mass is likewise unaffected (but see Footnote \ref{fn:ma}), the moving observer says there's no change in the body's momentum or kinetic energy, either. The energy lost to the light waves must have come from the body's rest energy, as before.

So it seems that the observer at rest and the moving observer are in agreement: the body's kinetic energy and momentum remain the same, and the light waves carry away combined energy equal to $-\Delta E_0$ (where $\Delta E_0$ is the invariant change in the body's rest energy, a negative value). Lovely.

Lovely but wrong!

First, any pair of things emitted in opposite directions according to the source do \emph{not} travel in opposite directions according to a moving observer (unless that observer's motion is parallel to the paths of the emitted things, but we'll tackle that in the next paragraph). Rather, each ``gains" a component of velocity in the direction of the source's motion.\footnote{We can calculate this component with Newtonian mechanics, as long as the source isn't moving too fast (and even if it is, the qualitative argument suffices here).} This means that our light waves don't travel in opposite directions for the moving observer. Consequently, their momentum vectors can't cancel, and by conservation the body's momentum must change! And if the body's momentum changes for the moving observer, so too must its mass or velocity (hint: it isn't the velocity), and thus also its kinetic energy.

Second, we've neglected Maxwell. Light is an electromagnetic wave, and the Doppler effect tells us that measured frequency depends on the observer's and wave's velocity vectors and the cosine of the angle between them.\footnote{Granted, the classical Doppler effect wrongly assumes that the light wave propagates at different speeds in different frames. But redshift had been observed and recognized as a Doppler phenomenon decades before Einstein arrived at the correct \emph{relativistic} Doppler formula. So the qualitative argument suffices here and isn't ahistorical.} Except for the special case that our moving observer's motion is perpendicular to the velocity vectors of both light waves according to the body (but see the previous paragraph), that cosine will of course be \emph{different} for each wave, and the waves will therefore have different frequencies in our moving observer's rest frame. By Equations \ref{eq:3} and \ref{eq:4}, different frequencies mean that the photon energies and momenta corresponding to one light wave will differ from those corresponding to the other, and again, by conservation, the body's momentum must change (and its mass\textinterrobang).

Where was the flaw in our ``lovely but wrong" analysis? We've unsubtly hinted at it several times now, so let's just say it outright: the culprit is mass! Specifically, it's our Newtonian assumption that mass is additive. Recall our unease when we introduced Equation \ref{eq:3}---we were justifiably suspicious that light could carry energy and momentum but be exempt from Equations \ref{eq:1} and \ref{eq:2}. Something had to give, didn't it? Our attempt to work out Einstein's thought experiment without special relativity has led us to the crux of the matter. If something massless like light can carry momentum (verified experimentally in 1901), and if momentum is indeed additive and conserved, then, in general, the mass of a system is \emph{not} the sum of the masses of its constituents. The body's mass must change when it emits the massless light waves.

But how is that possible? Isn't mass just the ``amount of matter"? Maybe not! Stay tuned.



\section[Einstein's Thought Experiment \emph{with} Einstein]{Einstein's Thought Experiment\\ \emph{with} Einstein}

\subsection{New Ground Rules: Special Relativity Review}\label{ssec:sr}

When Einstein wrote about this thought experiment, he'd already published his first paper on special relativity.\footnote{\label{fn:ep}Einstein, A.: Zur Elektrodynamik bewegter K\"orper. Ann.\ Phys.\ \textbf{17}, 891--921 (1905). English translation here: \url{https://www.fourmilab.ch/etexts/einstein/specrel/www/}.} In that earlier work, he posited a cosmic speed limit---i.e., that the speed of light in vacuum $c$ is invariant---and derived many of its consequences. Let's review some of them. We'll take the Lorentz transformation as a given and go from there.\footnote{Wikipedia has some straightforward derivations of the Lorentz transformation, including this classic one: \url{https://en.wikipedia.org/wiki/Derivations_of_the_Lorentz_transformations\#Spherical_wavefronts_of_light}.} Once we have the relativistic Doppler formula in hand, we'll come back to the thought experiment.


\subsubsection{The Lorentz Transformation and Standard Configuration}\label{sssec:lt}
First, we can no longer use the \textbf{Galilean transformation} and its inverse (on the right side below) to ``convert" space and time coordinates between inertial frames:
\begin{equation*}
\begin{aligned}
t^{\prime} &= t& t&= t^\prime \\
x^{\prime} &= x - vt& \qquad x &= x^\prime + vt^\prime\\
y^{\prime} &= y& y&= y^\prime\\
z^{\prime} &= z& z&= z^\prime .
\end{aligned}
\end{equation*}
Here the frames are in \textbf{standard configuration} for a \textbf{boost} along the coincident $x$/$x^\prime$-axes. That means that the Cartesian axes of each frame are parallel to the corresponding axes of the other, the origins coincide at $t=t^\prime=0$, and the ``primed" frame moves with speed $v$ in the positive ``unprimed" $x$-direction (equivalently, the unprimed frame moves with speed $v$ in the negative $x^\prime$-direction, so that the \textbf{relative speed} $v$ is really the magnitude of two different velocity vectors). Instead, we must use the \textbf{Lorentz transformation} and \emph{its} inverse:
\begin{equation}\label{eq:lt}
\begin{aligned}
ct^{\prime} &= \dfrac{ct - \beta x}{\sqrt{1- \beta ^2}}& ct &= \dfrac{ct^\prime + \beta x^\prime}{\sqrt{1- \beta ^2}} \\[3pt]
x^{\prime} &= \dfrac{x - \beta \mkern1mu c t}{\sqrt{1- \beta ^2}}& \qquad x &= \dfrac{x^\prime + \beta \mkern1mu c t^\prime}{\sqrt{1- \beta ^2}}\\
y^{\prime} &= y& y&= y^\prime \\
z^{\prime} &= z& z&= z^\prime,
\end{aligned}
\end{equation}
where again the frames are in standard configuration.\footnote{A more general transformation drops the stipulations about the axes and origins, but our rotation-free standard-configuration boosts are much simpler and have everything we need to study the ``new" physics of special relativity.} Here we've introduced the \textbf{normalized speed} $\beta$, which is the magnitude of the \textbf{normalized velocity} $\vvbeta=\vv v/c$. By \emph{normalized}, we mean that $\beta$ is a dimensionless number between 0 and 1.

Don't think of $\vvbeta$ as shorthand! The universe has a speed limit, and all speeds \emph{really are} fractions of it. Plus, working with $\vvbeta$ reveals mathematical elegance that's masked when working with $\vv v$. A good example is the symmetry of the $ct^\prime$ and $x^\prime$ equations above; if we use $v$ instead of $\beta$ (or $t$ instead of $ct$, for that matter), the symmetry of the numerators is hidden:
\begin{equation*}
ct^\prime = \frac{ct - \dfrac{vx}{\vphantom{V_v} c}}{\sqrt{1 - \dfrac{v^2}{c^2}}} \qquad x^\prime = \frac{x - vt}{\sqrt{1 - \dfrac{v^2}{c^2}}},
\end{equation*}
or worse:
\begin{equation*}
t^\prime = \frac{t - \dfrac{vx}{\vphantom{V_v} c^2}}{\sqrt{1 - \dfrac{v^2}{c^2}}} \qquad x^\prime = \frac{x - vt}{\sqrt{1 - \dfrac{v^2}{c^2}}}.
\end{equation*}
From now on we spurn $\vv v$. We'll refer to the classical limit not as $v \ll c$, but as $\beta \ll 1$. We regard $\vvbeta$ as the ``natural" way to express velocity. It goes hand in hand with measuring distance and time in the same unit.
 
Because the Lorentz transformation is linear (space and time coordinates are to first power only), it also works for \emph{differences} in the coordinates between two \textbf{events} (an event being a location in space at a moment in time, or a point in \textbf{spacetime}): just replace the likes of $x$ and $ct$ with those of $\Delta x$ and $c \mkern.5mu \Delta t$ (or of $\dd x$ and $c \, \dd t$ if need be).\footnote{\label{fn:in}In physics it's normal to interpret the derivative as a ratio of infinitesimals. Some find this objectionable, but it's a great heuristic that works just fine for well-behaved single-variable situations (not for partial derivatives, though).} We now adopt the standard shorthand $\gamma \equiv (1 - \beta^2)^{-1/2}$ (called the \textbf{Lorentz factor}):
\begin{equation}\label{eq:lt2}
\begin{aligned}
c \mkern.5mu \Delta t^{\prime} &= \gamma \left( c \mkern.5mu \Delta t - \beta \Delta x \right) & c \mkern.5mu \Delta t &= \gamma \left( c \mkern.5mu \Delta t^\prime + \beta \Delta x^\prime \right) \\
\Delta x^{\prime} &= \gamma \left( \Delta x - \beta \mkern1mu c \mkern.5mu \Delta t \right) & \qquad \Delta x &= \gamma \left( \Delta x^\prime + \beta \mkern1mu c \mkern.5mu \Delta t^\prime \right) \\
\Delta y^{\prime} &= \Delta y& \Delta y&= \Delta y^\prime \\
\Delta z^{\prime} &= \Delta z& \Delta z&= \Delta z^\prime ,
\end{aligned}
\end{equation}
with the ``inverse" transformation on the right. Equations \ref{eq:lt2} are arguably a better definition of ``the" Lorentz transformation than Equations \ref{eq:lt}---they're more useful in practice, and they don't require the chosen spatial origins of the frames to coincide at $ct = ct^\prime = 0$ (though the other conditions of standard configuration are necessary). In any case, we'll later broaden our usage so that ``a" Lorentz transformation refers to how \emph{any} four scalar quantities transform between inertial frames, provided that they transform together in the same way that differences in coordinates do in Equations \ref{eq:lt2}.



\subsubsection{Time Dilation, Length Contraction, Invariant Interval}\label{sssec:td}

From the Lorentz transformation for differences in the coordinates between events, we can derive many important results. The most novel is probably the inequality of $c \mkern.5mu \Delta t^{\prime}$ and $c \mkern.5mu \Delta t$, which holds even when one of them equals zero, as long as $\Delta x$ and $\Delta x^\prime$ don't also equal zero (this is the \textbf{relativity of simultaneity}). Let's now examine three further results that we'll use in this paper: \textbf{time dilation}, \textbf{length contraction}, and the invariance of the \textbf{spacetime interval} between two events, $\Delta s \equiv [(c \mkern.5mu \Delta t)^2 - ( \Delta \vv r )^2]^{1/2}$, where $\vv r$ is position and ${\Delta \vv r = \langle \Delta x, \Delta y, \Delta z \rangle}$ is the separation vector.\footnote{We'll call $\Delta \vv r$ \emph{displacement} when referring to the physical motion of something, and \emph{separation} when speaking in the abstract of a vector ``connecting" two points.}

When $\Delta x^{\prime} = 0$ in Equations \ref{eq:lt2}, the expression for time dilation emerges: $c \mkern.5mu \Delta t = \gamma \mkern2mu c \mkern.5mu \Delta t^{\prime}$. In the special case that $\Delta x^{\prime} = \Delta y^{\prime} = \Delta z^{\prime} = 0$, the primed time $ct^{\prime}$ corresponds to the time logged by the wristwatch of an inertial traveler (real or imagined) who is physically present at both events. This invariant ``wristwatch time" is what we call the traveler's \textbf{proper time}, and we label it $c \mkern.5mu t_0$. Hence, $c \mkern.5mu \Delta t = \gamma \mkern2mu c \mkern.5mu \Delta t_0$ (where $ct$ is now actually the coordinate time in an \emph{arbitrary} inertial frame, standard configuration or no). For a traveler whose velocity isn't constant, we can still use infinitesimals: $c \, \dd t = \gamma \mkern2mu c \, \dd t_0$ (more on this in Section \ref{ssec:ds}).

To derive length contraction, we'll use time dilation.\footnote{If one instead uses the Lorentz transformation directly, one must remember that length is the distance between an object's endpoints \emph{at a single moment in coordinate time}. So in a given inertial frame, an observer measures the spatial separation of two \emph{simultaneous} events---one located at each of the object's endpoints.} Say a ruler has length $L_0$ in its rest frame (this is its \textbf{proper length}), and that a clock moving at speed $\beta$ relative to the ruler traverses its length in a time $c \mkern.5mu \Delta t$ (the ruler's rest frame is unprimed). We then have $\beta = L_0 / (c \mkern.5mu \Delta t)$. In the clock's (primed) rest frame, the entire length of the ruler $L^\prime$ passes the clock's location in a time $c \mkern.5mu \Delta t^\prime$, giving $\beta = L^\prime / (c \mkern.5mu \Delta t^\prime)$. Now we've got two expressions for the relative speed $\beta$. Put them together: $L_0 = L^\prime \mkern2mu c \mkern.5mu \Delta t / (c \mkern.5mu \Delta t^\prime) = \gamma L^\prime$ (by time dilation). So an object's length when moving is shorter than its proper length by a factor of $\gamma$. Note that an object's \emph{volume} when moving is likewise scaled down from its ``proper volume" by a factor of $\gamma$, since only the length along the axis of relative motion is contracted.

As for the invariance of the spacetime interval $\Delta s$ (aka ``the interval"), we can ignore the $y$ and $z$ coordinates in standard configuration without sacrificing generality:
\begin{equation*}
(c \mkern.5mu \Delta t^{\prime})^2 - (\Delta x^{\prime})^2 = \big[ \gamma \left( c \mkern.5mu \Delta t - \beta \Delta x \right) \big] ^2 - \big[ \gamma \left( \Delta x - \beta \mkern1mu c \mkern.5mu \Delta t \right) \big]^2 ,
\end{equation*}
which, after some algebra, yields:
\begin{equation*}
\begin{aligned}
(c \mkern.5mu \Delta t^{\prime})^2 - (\Delta x^{\prime})^2 &= \gamma^2 \left( 1 - \beta^2 \right) \left[ (c \mkern.5mu \Delta t)^2 - ( \Delta x )^2 \right] \\[2pt]
&= (c \mkern.5mu \Delta t)^2 - ( \Delta x )^2
\end{aligned}
\end{equation*}
(because $1 - \beta^2 = \gamma^{-2}$). So $(\Delta s)^2 = (c \mkern.5mu \Delta t)^2 - ( \Delta \vv r )^2$ is invariant. In case you're rusty:
\begin{itemize}
\item When $(\Delta s)^2<0$, the interval is \textbf{spacelike}, and $\lvert (\Delta s)^2 \rvert ^{1/2} = \Delta s / \mathrm{i}$ represents the \textbf{proper distance} between two events, which is their spatial separation in an inertial frame for which they occur simultaneously (there are infinitely many such frames, since motion perpendicular to the line connecting the locations of simultaneous events doesn't ``break" their simultaneity). Nothing---not even a light signal---can travel from one event to the other. The events cannot have a causal relationship, and their chronological order depends on one's frame of reference.
\item When $(\Delta s)^2>0$, the interval is \textbf{timelike}, and $\lvert(\Delta s)^2\rvert ^{1/2} = \Delta s $ represents the \textbf{inertial proper time interval} between the events, which is the proper time $c \mkern.5mu \Delta t_0$ that elapses for an inertial traveler who journeys from one event to the other.\footnote{The qualifier \emph{inertial} is crucial because non-inertial journeys between timelike-separated events are possible, too. But among all travelers who make the journey, it's the inertial one who ages the most en route. This maximal aging---the inertial traveler's elapsed proper time---is the spacetime interval between the events.} Equivalently, it's the coordinate time that elapses between the events in the unique inertial frame for which they occur at the same location. The events may be causally related, and their chronological order is fixed.
\item When $(\Delta s)^2=0$, the interval is \textbf{lightlike}, meaning that there's no frame (inertial or otherwise) in which the events occur simultaneously or at the same location. Only a signal moving at the speed of light can travel from one event to the other. The events may be causally related (by such a signal), and their chronological order is fixed.
\end{itemize}


\subsubsection{Velocities, Angles, and the Aberration of Light}\label{sssec:va}
In deriving the relativistic transformation rules for velocity components, we must not confuse the relative speed of two inertial observers in standard configuration with their measurements of some third party's velocity. To that end, let's append the subscript ``rel" to the observers' relative speed for now ($\beta_{\textrm{rel}}$), and also to the corresponding Lorentz factor: $\gamma_{\textrm{rel}} = (1 - \beta_{\textrm{rel}}^2)^{-1/2}$. Then $\vvbeta$ and $\vvbeta^{\prime}$ are the observers' measurements of the third party's velocity. To find how the components of $\vvbeta$ transform into those of $\vvbeta^{\prime}$ (and vice versa), we divide $\Delta x^\prime$ and $\Delta x$ (etc.) by $c \mkern.5mu \Delta t^\prime$ and $c \mkern.5mu \Delta t$, respectively, and evaluate the limits of the quotients as $c \mkern.5mu \Delta t^\prime \rightarrow 0$ and $c \mkern.5mu \Delta t \rightarrow 0$ (equivalent conditions, since we're not dealing with spacelike-separated events). For example:
\begin{equation*}
\begin{aligned}
\beta_x^\prime &\equiv \dfrac{1}{c} \, \dfrac{\dd x^\prime}{\dd t^\prime} = \lim_{\substack{c \mkern.5mu \Delta t^\prime \to 0 \\[1.5pt] [c \mkern.5mu \Delta t \to 0]}} \, \frac{\Delta x^\prime}{c \mkern.5mu \Delta t^\prime}\\[6pt]
&= \lim_{\substack{\\ c \mkern.5mu \Delta t \to 0}} \, \frac{\gamma_{\textrm{rel}} \left( \Delta x - \beta_{\textrm{rel}} \, c \mkern.5mu \Delta t \right)}{\gamma_{\textrm{rel}} \left( c \mkern.5mu \Delta t - \beta_{\textrm{rel}} \, \Delta x \right)}
\end{aligned}
\end{equation*}
(by Equations \ref{eq:lt2}), and dividing numerator and denominator by $c \mkern.5mu \Delta t$:
\begin{equation*}
\begin{aligned}
\beta_x^\prime &= \frac{\lim\limits_{\substack{\\ c \mkern.5mu \Delta t \to 0}} \left( \dfrac{\Delta x}{c \mkern.5mu \Delta t} - \beta_{\textrm{rel}} \right) \hfill}{\lim\limits_{\substack{\\ c \mkern.5mu \Delta t \to 0}} \left( 1 - \beta_{\textrm{rel}} \, \dfrac{\Delta x}{c \mkern.5mu \Delta t} \right) }\\[7pt]
&= \frac{\beta_x - \beta_{\textrm{rel}}}{1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}}},
\end{aligned}
\end{equation*}
and so on. Or we can take a shortcut: sidestep the limits and just ``divide" the infinitesimals in the first place (see Footnote \ref{fn:in}). Either way, we end up with the following transformation for velocity components:
\begin{equation}\label{eq:vt}
\begin{aligned}
\beta_x^{\prime} &= \frac{\beta_x - \beta_{\textrm{rel}}}{1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}}}& \qquad \beta_x &= \frac{\beta_x^{\prime} + \beta_{\textrm{rel}}}{1 + \beta_x^{\prime} \mkern1mu \beta_{\textrm{rel}}}\\[5pt]
\beta_y^{\prime} &= \frac{\beta_y}{\gamma_{\textrm{rel}} \left( 1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}} \right)}& \qquad \beta_y &= \frac{\beta_y^{\prime}}{\gamma_{\textrm{rel}} \left( 1 + \beta_x^{\prime} \mkern1mu \beta_{\textrm{rel}} \right) }\\[5pt]
\beta_z^{\prime} &= \frac{\beta_z}{\gamma_{\textrm{rel}} \left( 1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}} \right)}& \qquad \beta_z &= \frac{\beta_z^{\prime}}{\gamma_{\textrm{rel}} \left( 1 + \beta_x^{\prime} \mkern1mu \beta_{\textrm{rel}} \right)}.
\end{aligned}
\end{equation}

These vector components can be positive or negative, though standard configuration guarantees that $ \beta_x \geq \beta_x^\prime $.\footnote{Equal only if they're both $1$ or $-1$, as with a light wave moving parallel to the $x^{[\prime]}$-axis.} We see that even the components of velocity \emph{perpendicular} to the observers' axis of relative motion are nontrivially transformed. This is in contrast to classical Galilean relativity, where $\beta_y^{\prime} = \beta_y$ and $\beta_z^{\prime} = \beta_z$. In both theories, of course, the unprimed observer measures a different angle $\theta$ between $\vvbeta$ and the positive $x$-direction than the primed observer does ($\theta^\prime$) between $\vvbeta^{\prime}$ and the positive $x^\prime$-direction (assuming the angle isn't $0$ or $\pi$ in both frames). And in both theories, relationships between $\theta$ and $\theta^{\prime}$ can be found with simple trigonometry and the appropriate transformation of velocity components. Using the cosine function and Equations \ref{eq:vt}, we get:
\begin{equation*}
\begin{aligned}
\beta^\prime \cos \theta^\prime &= \beta_x^\prime &\qquad \beta \cos \theta &= \beta_x \\[3pt]
&= \dfrac{\beta_x - \beta_{\textrm{rel}}}{1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}}} &\qquad &= \dfrac{\beta_x^\prime + \beta_{\textrm{rel}}}{1 + \beta_x^\prime \mkern1mu \beta_{\textrm{rel}}} \\[3pt]
\beta^\prime \cos \theta^\prime &= \frac{\left( \beta \cos \theta \right) - \beta_{\textrm{rel}}}{1 - \beta_{\textrm{rel}} \left( \beta \cos \theta \right)} &\qquad \beta \cos \theta &=\frac{\left( \beta^\prime \cos \theta^\prime \right) + \beta_{\textrm{rel}}}{1 + \beta_{\textrm{rel}} \left( \beta^\prime \cos \theta^\prime \right)} .
\end{aligned}
\end{equation*}
When the third party whose velocity is being measured is a light wave, we have $\beta = \beta^\prime = 1$, and we obtain the formula for the \textbf{aberration of light}:
\begin{equation}\label{eq:7}
\cos \theta^\prime =\frac{\cos \theta - \beta_{\textrm{rel}}}{1 - \beta_{\textrm{rel}} \cos \theta} \qquad \cos \theta =\frac{\cos \theta^\prime + \beta_{\textrm{rel}}}{1 + \beta_{\textrm{rel}} \cos \theta^\prime}.
\end{equation}

Equations \ref{eq:7} are pertinent to Einstein's thought experiment because source and moving observer don't agree on the angle between an emitted light wave and the axis of their relative motion. In the observer's frame, the light is ``tilted" toward the direction of the source's velocity vector, and by more than Galilean relativity would predict. Near the end of Section \ref{ssec:tc}, we alluded qualitatively to the fact that two light waves emitted in opposite directions according to the source do \emph{not} generally travel in opposite directions in other frames. Now we can actually crunch the numbers, even for observers moving near the speed of light relative to the source.

Because all traces of $\vvbeta$ and $\vvbeta^{\prime}$ have disappeared, we're free to drop the ``rel" subscript when we apply Equations \ref{eq:7}.


\subsubsection{Relativistic Doppler Effect}\label{sssec:rdf}
Imagine two frames of reference in standard configuration once again. This time, a source of light at rest with respect to the unprimed frame moves with normalized speed $\beta$ (and Lorentz factor $\gamma$) in the negative $x^\prime$-direction, and an observer sits stationary at the primed origin. (The source need not travel \emph{on} the $x^\prime$ axis---only parallel to it.)

For a brief moment, the source emits light directly toward the observer. To the observer, the velocity vector of the approaching light makes an angle $\theta^\prime$ with the positive $x^\prime$-direction (obtuse if the source is approaching, acute if the source is receding). To the source, the light's velocity vector makes an angle $\theta$ with the positive $x$-direction (which is also the direction of the observer's motion---remember this). Thus, the angles $\theta^\prime$ and $\theta$ are related by Equations $\ref{eq:7}$ (substitute $\beta$ for $\beta_{\textrm{rel}}$).

In the source's frame, the light oscillates with a period $c \mkern.5mu \Delta t$ equal to its wavelength $\lambda$. By time dilation, the period in the \emph{observer's} frame is $c \mkern.5mu \Delta t^\prime = \gamma \lambda$. During this time interval $\gamma \lambda$, the light travels toward the observer a distance $\gamma \lambda$, while the \emph{source} is displaced a distance $\gamma \lambda \, \beta$ in the negative $x^\prime$-direction. But we only care about the \emph{radial component} of the source's displacement (i.e., the component that's parallel to the light's path toward the observer at the primed origin, defined as negative if it's directed toward the observer and positive if it's directed away from the observer). A bit of trigonometry tells us that this radial displacement is $\gamma \lambda (\beta \cos \theta^\prime)$, regardless of whether the source is on the negative or positive $x^\prime$-side of the $y^\prime$/$z^\prime$-plane.\footnote{To keep the concepts and notation simple, we've neglected any change in $\theta^\prime$ during the time interval $\gamma \lambda$. This is no crime, but we should acknowledge that the angle actually does change continuously (unless it's $0$ or $\pi$), and that consequently our results will be time-dependent---\emph{even over the course of a single cycle}. If we preferred, we could work with infinitesimals.} Specifically: if the source is on the negative $x^\prime$-side (receding from the observer), the radial component is positive because $\cos \theta^\prime$ is positive; and if the source is on the positive $x^\prime$-side (approaching the observer), the radial component is negative. (If the source is \emph{on} the $y^\prime$/$z^\prime$-plane, then the radial component is zero because $\cos \theta^\prime = 0$.)

So in the observer's frame, the source emits the light wave's second peak a time $\gamma \lambda$ after emitting the first, but the source has also moved toward or away from the observer a distance $| \gamma \lambda \, \beta \cos \theta^\prime |$ during that time. The second peak therefore trails the first by a distance equal to the sum of $\gamma \lambda$ (the distance light travels in the observer's frame between peak-emissions) and $\gamma \lambda \, \beta \cos \theta^\prime$ (the appropriately signed radial component of the source's concurrent displacement). Naturally, that trailing distance is just the wavelength $\lambda^\prime$ measured in the observer's frame:
\begin{equation*}
\begin{aligned}
\lambda^\prime &= \gamma \lambda + \gamma \lambda \, \beta \cos \theta^\prime \\
&= \gamma \lambda \left( 1 + \beta \cos \theta^\prime \right).
\end{aligned}
\end{equation*}

For our purposes, it will be more useful to express this \textbf{relativistic Doppler formula} in terms of the unprimed angle $\theta$ (measured in the \emph{source's} frame), and in terms of photon energy. By Equations \ref{eq:7} (aberration):
\begin{equation}\label{eq:rdw}
\begin{split}
\lambda^\prime &= \gamma \lambda \left( 1 + \beta \, \frac{\cos \theta - \beta}{1 - \beta \cos \theta} \right) \\[3pt]
&= \frac{\lambda}{\sqrt{1 - \beta^2}} \left( \frac{ 1 - \beta \cos \theta + \beta \cos \theta - \beta^2 }{1 - \beta \cos \theta} \right) \\[3pt]
\lambda^\prime &= \frac{\lambda}{\gamma \left( 1 - \beta \cos \theta \right)}.
\end{split}
\end{equation}
The frequency is inversely proportional to the wavelength, so:
\begin{equation}\label{eq:rdf}
\nu^{\prime} = \gamma \mkern1mu \nu \left( 1- \beta \cos\theta \right).
\end{equation}
And by Equation \ref{eq:4} for photons:
\begin{equation}\label{eq:8}
\boxed{E^{\prime} = \gamma E \left(1-\beta \cos\theta\right)} \quad \text{\footnotesize{ (for light)}} .
\end{equation}
Bingo! In words: if we know the energy $E$ of (the photons in) a light wave as measured in the source's frame, as well as the angle $\theta$ that the light's direction of propagation makes with a moving observer's velocity vector $\vvbeta$ (again, as measured in the source's frame), then we can calculate the energy $E^\prime$ of (the photons in) the light wave as measured in the \emph{observer's} frame. As you may suspect, Equation \ref{eq:8} will be the key to overcoming our earlier impasse.

Actually, our photonic parentheticals in the previous paragraph weren't necessary: Equation \ref{eq:8} works for Maxwell's wave model of light, too, though demonstrating this requires digging a bit deeper into classical electromagnetism (we'll do this at the end of Section \ref{sec:rem}). Heuristically, we can reason that if the energies of all the photons transform this way, then the energy of the light wave must also. As promised when we introduced Equation \ref{eq:4}, we've used the Planck--Einstein relation to take a shortcut. From now on we'll drop the photon talk.

If you're wondering how the observers in Einstein's thought experiment can possibly detect both waves (because nobody can be in more than one place at once!), recall that a frame of reference isn't just a single observer. We pretend that each inertial frame is full of magically incorporeal measuring devices: a grid of arbitrarily precise rulers, synchronized clocks, and whatever else might come in handy, like spectrometers for analyzing light waves passing through. Every measurement is made at a place and time specified by the imagined clock-and-ruler lattice, and everyone \emph{at rest in that frame} agrees on the measurement's where and when. Further, a measurement like $E^\prime$ in Equation \ref{eq:8} that depends only on relative velocity is \emph{itself} valid for the whole frame.

So when we apply Equation \ref{eq:8} to our thought experiment, we can arrange for the moving observer to personally detect one of the light waves with a spectrometer, but then the other wave must be detected by a \emph{second} spectrometer elsewhere in the frame, along the wave's path. Or we can disregard the observer altogether and simply speak of two spectrometers. It doesn't matter. The important thing is that the spectrometers move with the same velocity $\vvbeta$ relative to the light-emitting body. And since the light waves propagate in opposite directions from the body's perspective, the angles they form with $\vvbeta$ must differ by $\pi$. How convenient! The cosines of such angles sum to zero (i.e., $\cos \mkern1.5mu (\theta+\pi)=-\cos\theta$), and that's just what we'll need for all the $\theta$'s to disappear when we use Equation \ref{eq:8} to add the $E^{\prime}$ of one light wave to the $E^{\prime}$ of the other. Apparently this Einstein fellow knew what he was doing. But we're getting ahead of ourselves.

\subsection [Retracing Einstein's Steps (More or Less): Relativistic Energy and $m=E_0/c^2$] {Retracing Einstein's Steps (More or Less):\\Relativistic Energy and \boldmath{$m=E_0/c^2$}}\label{ssec:re}

Let's take stock. When we analyzed the thought experiment without special relativity, we hit a roadblock: on the one hand, the additivity of mass implies that the observers should agree that the light waves have equal-and-opposite momenta and carry away a combined energy equal to $-\Delta E_0$ (the rest energy lost by the body); on the other hand, Maxwell and Doppler say that \emph{only} our rest observer can find that to be the case. Now we have a path forward---prioritize Maxwell and Doppler, and retrace Einstein's steps (more or less) in deriving a new equation for kinetic energy. Then we'll tackle momentum.

We stand by the logic behind Equations \ref{eq:5} and \ref{eq:6}. The concept of invariant rest energy is crucial. In fact, Einstein formulated it in precisely the same context that we did. He didn't draw attention to it, but his whole ``$E=mc^2$" derivation hangs on it. In light of the relativity of simultaneity, however, we must be careful when discussing the rest energy of an \emph{open} composite system. If the body in our thought experiment is an extended object that emits the two light waves from its extremities simultaneously \emph{in its own rest frame}, then in general the moving observer will say that the waves are \emph{not} emitted simultaneously. The exception is when the moving observer's trajectory is perpendicular to the waves' velocity vectors according to the body, but aside from that special case, the moving observer will say that some time elapses between the emissions of the waves. During that gap, the observers do \emph{not} agree on the body's rest energy! \emph{After} the gap, they do. So when we use $\Delta E_0$ below, we'll mean the difference in the body's rest energy before and after the emission of both waves for both observers. That way it's invariant.

Now, we're looking for a speed-dependent expression for $E_{\mkern.5mu \textrm{k}}$ that reduces to Equation \ref{eq:2} in the classical limit but works for arbitrary $\beta$. To find it, we'll apply Equations \ref{eq:5} and \ref{eq:8} to Einstein's thought experiment.

We know that for the observer at rest relative to the body, the equal-but-opposite light waves carry combined energy $-\Delta E_0$. If we say that each light wave carries energy $E_{\ell}$ in the body's rest frame ($\ell$ for light), then we have:
\begin{equation}\label{eq:9}
2E_{\ell}=-\Delta E_0.
\end{equation}
Simple.

What about the observer moving relative to the body?

Still pretty simple. As we concluded at the end of the previous section, the angle $\theta$ that we use in Equation \ref{eq:8} for one light wave must be $(\theta + \pi)$ for the other. We don't assume that the waves carry equal energy in this frame---actually, Equation \ref{eq:8} says that they don't, unless $\theta = \pm \, \pi / 2$---so we distinguish their energies as $E_{\ell \, 1}^{\prime}$ and $E_{\ell \, 2}^{\prime}$. From Equations \ref{eq:8} and \ref{eq:9}:
\begin{equation}\label{eq:10}
\begin{split}
E_{\ell \, 1}^{\prime} + E_{\ell \, 2}^{\prime} &= \gamma E_{\ell} \left(1-\beta\cos\theta \right) + \gamma E_{\ell} \big[ 1 - \beta \cos \mkern1.5mu ( \theta + \pi ) \big] \\[2pt]
&= \gamma E_{\ell} \left(1 - \beta\cos\theta + 1 + \beta\cos\theta \right) \\[2pt]
&= 2 \gamma E_{\ell} \\[3pt]
E_{\ell \, 1}^{\prime} + E_{\ell \, 2}^{\prime} &= - \, \gamma \Delta E_0.
\end{split}
\end{equation}

Now we have $E_{\ell \, 1}^{\prime} + E_{\ell \, 2}^{\prime}$ in terms of $\Delta E_0$ and $\beta$ (remember that $\gamma$ is a function of $\beta$). So after our inertial body emits the light waves without changing its velocity, all observers agree on the amount of rest energy it's lost, but moving observers measure the light waves as carrying more (combined) energy than observers in the body's rest frame do---by the Lorentz factor, in fact.\footnote{Don't mistake this for a blanket statement about the relativistic Doppler effect. Equation \ref{eq:8} certainly allows for a moving observer to measure a light wave as \emph{less} energetic than an observer in the body's rest frame does. Indeed, $E_{\ell \, 1}^\prime$ or $E_{\ell \, 2}^\prime$ may be less than $E_{\ell}$, but then the other is greater than $E_{\ell}$ by a larger amount such that $E_{\ell \, 1}^{\prime} + E_{\ell \, 2}^{\prime} > 2E_{\ell}$.} We'll have to account for that extra energy, since conservation demands that our moving observer measure equal values for the total energy lost by the body ($-\Delta E^{\prime}$) and the energy carried away by the light:
\begin{equation}\label{eq:11}
E_{\ell \, 1}^{\prime} + E_{\ell \, 2}^{\prime}=-\Delta E^{\prime}.
\end{equation}
Here comes the fun part. Substitute Equation \ref{eq:11} into Equation \ref{eq:10}:
\begin{equation*}
\Delta E^{\prime} = \gamma \Delta E_0,
\end{equation*}
and drop the deltas (kosher if we throw in a constant---call it $C$):
\begin{equation*}
E^{\prime} = \gamma E_0 + C.
\end{equation*}
When $\beta = 0$ (i.e., when $\gamma = 1$), that's just $E^{\prime} = E_0 + C$, but by Equation \ref{eq:6} we know that $E^{\prime} = E_0$ in that case, so $C=0$:
\begin{equation*}
E^{\prime} = \gamma E_0.
\end{equation*}

Look at this beautiful new equation for total energy! It might not be immediately apparent, but that result is good for \emph{any} system with a rest frame.\footnote{As mentioned in Footnote \ref{fn:rl}, some systems are ``restless" and have no rest frame or rest energy. We'll discuss these in Section \ref{ssec:em}.} Why? First, because there's nothing special about the energy carried by electromagnetic radiation. Energy is energy, so we would have arrived at this equation regardless of what mechanism caused the body to lose some of its $E_0$. But second (and here's the kicker), this result holds \emph{even if the body's velocity is changing}. Calculus permits us to model an accelerating body's trajectory as a succession of infinitesimally short and \emph{instantaneously inertial} mini-trajectories. For every such mini-trajectory, the body's $\beta$ (and $\gamma$) is momentarily fixed in a given frame---the condition stipulated by our thought experiment---and our new relation naturally applies.\footnote{It's a common misconception that analyzing accelerated motion requires the mathematical tools of general relativity. Basic calculus suffices, in both classical mechanics and special relativity. More on these ``instantaneous" mini-trajectories in Section \ref{ssec:ds}.} This is why Einstein's setup is so clever. Its symmetry facilitates analysis but leads to general results. It doesn't matter how a system's $\gamma$ and $E_0$ came to be what they are, or how they're changing over time. An inertial observer simply uses their current values when calculating the system's total energy.

Since our result is a general one, let's ditch the prime symbol and make it official:
\begin{equation}\label{eq:12}
\boxed{E = \gamma E_0} \quad \textrm{\footnotesize{ (for $E_0 \neq 0$)}}.
\end{equation}
When $\gamma=1$ ($\beta=0$), Equation \ref{eq:12} becomes Equation \ref{eq:6}. And as $\beta \rightarrow 1$, $E$ increases without bound. As for kinetic energy, which suddenly seems like an afterthought, by Equations \ref{eq:5} and \ref{eq:12} we have:
\begin{equation}\label{eq:13}
\boxed{E_{\mkern.5mu \textrm{k}} = E_0 (\gamma - 1)} \quad \textrm{\footnotesize{ (for $E_0 \neq 0$)}}.
\end{equation}
Done.

Note that kinetic energy now depends on velocity and \emph{rest energy}. That solves our problem, at least as far as energy is concerned: from the perspective of our moving observer, the body's velocity doesn't change when it emits the light waves, but its rest energy does, which means that its non-zero \emph{kinetic} energy does, too. So all observers agree on the body's $\Delta E_0$, but moving observers also measure a decrease in its $E_{\mkern.5mu \textrm{k}}^\prime$. And where did that extra energy go? Into the light waves! That's what Einstein's relativistic Doppler shift told us. The moving observer measures the light waves in toto as \emph{more energetic} than the rest observer does, and that ``extra" energy is exactly counterbalanced by the measured decrease in the body's kinetic energy: $-\Delta E_{\mkern.5mu \textrm{k}}^\prime = (E_{\ell \, 1}^{\prime} + E_{\ell \, 2}^{\prime}) - 2E_{\ell}$. For both observers, then, total energy of the closed system is conserved.

Equation \ref{eq:12} should remind you of $c \mkern.5mu \Delta t=\gamma c \mkern.5mu \Delta t_0$, the equation for time dilation (where $ct_0$ is proper time). In fact, kinetic energy is a ``relativistic effect" in precisely the way that time dilation is. You can tell from Equations \ref{eq:12} and \ref{eq:13} that when $\beta \ll 1$, $E \approx E_0$ and $E_{\mkern.5mu \textrm{k}} \rightarrow 0$, just as $c \mkern.5mu \Delta t \approx c \mkern.5mu \Delta t_0$ and $c \mkern.5mu \Delta t - c \mkern.5mu \Delta t_0 \rightarrow 0$. Even at speeds we'd consider extremely fast by everyday standards, a body's rest energy dwarves its kinetic energy. Only when $\beta \centernot{\ll} 1$ does kinetic energy contribute appreciably to total energy, and it doesn't exceed rest energy until $\beta > \sqrt{3}/2 \approx .866$, which you can verify by setting $E_{\mkern.5mu \textrm{k}}=E_0$ in Equation \ref{eq:13}. By the same token, only when $\beta > \sqrt{3}/2$ is $c \mkern.5mu \Delta t - c \mkern.5mu \Delta t_0 > c \mkern.5mu \Delta t_0$.

But there's an important practical difference between time dilation and kinetic energy: although each vanishes in the classical limit \emph{as a proportion of its corresponding ``total" quantity} ($c \mkern.5mu \Delta t$ and $E$, respectively), we have no difficulty detecting kinetic energy when $\beta \ll 1$, as Equation \ref{eq:2} makes clear.\footnote{It's actually the gargantuan \emph{rest energy} that escaped the notice of physicists before Einstein! But it was hiding in plain sight, going by another name \dots} So once Einstein derived Equation \ref{eq:13}, his final step was to compare its (non-zero!) low-$\beta$ approximate value to Equation \ref{eq:2}. He did this by using the binomial theorem to expand the Lorentz factor $\gamma = (1-\beta^2)^{-1/2}$. Fittingly, it was Newton who developed the procedure for arbitrary exponent:
\begin{equation*}
(a+x)^n = a^n + na^{n-1}x + \frac{n(n-1)}{2!} \, a^{n-2}x^2 + \frac{n(n-1)(n-2)}{3!} \, a^{n-3}x^3 + \dots ,
\end{equation*}
convergent if $|x/a| < 1$. In our case, $a=1$, $x=-\beta^2$, and $n=-1/2$. Because our exponent isn't a positive integer, we end up with an infinite series:
\begin{equation*}
\left(1-\beta^2\right)^{-1/2}=1 + \dfrac{1}{2}\,\beta^2 +  \frac {3}{8} \, \beta^4 + \frac{5}{16} \, \beta^6 + \frac{35}{128} \, \beta^8 + \dots
\end{equation*}
Usually we'll use $\gamma \approx 1$ when speaking of the ``classical limit," but that would lead to $E_{\mkern.5mu \textrm{k}} \approx 0$ here, which won't do. Instead we include the second term, and we can stop there since $\beta^2 \gg \beta^4$ when $\beta \ll 1$:
\begin{equation*}
\gamma \approx 1 + \dfrac{1}{2}\,\beta^2.
\end{equation*}
Now we plug that into Equation \ref{eq:13}:
\begin{equation}\label{eq:14}
\begin{split}
E_{\mkern.5mu \textrm{k}} &=E_0 (\gamma - 1)\\[5pt]
&\approx E_0 \left[ \left( 1 + \dfrac{1}{2}\,\beta^2 \right) -1 \right] \\[5pt]
E_{\mkern.5mu \textrm{k}} &\approx \frac{1}{2} \mkern2mu E_0 \mkern.5mu \beta^2 \quad \textrm{\footnotesize{ (for $\beta \ll 1$)}}.
\end{split}
\end{equation}
Equating Equations \ref{eq:2} and \ref{eq:14} then gives us Einstein's famous result,
\begin{equation}\label{eq:15}
\boxed{m = \frac{E_0}{c^2}} \, ,
\end{equation}
better known as ``$E = mc^2$" (mind the subscript).\footnote{Here's how Einstein originally put it: ``If a body gives off the energy $L$ in the form of radiation, its mass diminishes by $L / c^2$. \ldots The mass of a body is a measure of its energy-content."} Excellent!

Equation \ref{eq:15} does \emph{not} mean that mass and rest energy can be ``converted" into one another. It means that they're \emph{exactly the same thing} expressed in different units, like $\vvbeta$ and $\vv v$ are. Therefore anything we know about one of them applies to the other. Notably:
\begin{itemize}
\item Mass is an invariant that quantifies a system's total energy in its rest frame. A ``restless" system is a massless system (see Footnote \ref{fn:rl}).
\item Since rest energy isn't additive, mass isn't additive either, though they're approximately so in the classical limit (see Footnote \ref{fn:ad}). We surmised as much in Section \ref{ssec:tc}, but now we see why.
\item Similarly, we now see why the body in Einstein's thought experiment lost mass when it emitted the massless (restless) light waves: it lost rest energy, and mass \emph{is} rest energy! We must abandon the idea that mass is the ``amount of matter" (an ill-defined concept in the first place).
\item The mass of everything around us comes almost entirely from the kinetic and potential energies ``inside" nucleons, rather than from the masses of elementary particles (see Footnote \ref{fn:mwm}).
\item We were looking for a way to measure a system's rest energy (see Footnote \ref{fn:re}). Well, now we have one: weigh it!
\item Since mass is the resistance to change in velocity in Newtonian physics, so is rest energy. We'll discuss this more in the following section.
\item Since mass is the gravitational ``charge" in Newtonian physics, so is rest energy: $f_{\mathrm{g}} = G(m_a m_b)/r^2 = (G/c^4)(E_{0 \mkern.5mu a} E_{0 \mkern.5mu b})/r^2$.\footnote{Makes the dimensional analysis more transparent, doesn't it? The constant $G / c^4$ also happens to appear in Einstein's field equations in general relativity. It's the inverse Planck force (that is, the Planck length divided by the Planck energy).}
\end{itemize}

The \textbf{equivalence of mass and rest energy} presents us with a redundancy. Keeping them both around and treating them as different concepts is kind of like reserving the word \emph{temperature} for thermometer-readings in Fahrenheit and \emph{hotness} for thermometer-readings in Celsius, and carrying on as though the distinction were physically meaningful. We certainly \emph{could} take a ``more the merrier" approach and use them both, but why should we when our purpose is to lay down the foundation of relativistic mechanics? Let's take a ``less is more" approach instead, and stick with $E_0$. We're kicking $m$ to the curb with $\vv v$. Of course, we can always replace $E_0$ and $\vvbeta$ with $mc^2$ and $\vv v/c$ if the need or inclination arises.


\section{The Energy--Momentum Relation}
\subsection{Relativistic Momentum}\label{ssec:rm}

Using Equation \ref{eq:15} and $\vv v = \vvbeta c$, we can rewrite Equation \ref{eq:1} with $E_0$ and $\vvbeta$:
\begin{equation}\label{eq:16}
\vv p c \approx E_0 \vvbeta \quad \textrm{\footnotesize{ (for $\beta \ll 1$)}}.
\end{equation}
We've put the ``spare" $c$ on the left side so that the equation has the units of energy ($\vvbeta$ is dimensionless, remember). We'll find it convenient to express energy and momentum in the same unit---they're our additive-and-conserved scalar and vector!---so from now on it's always $\vv p c$ for us, never $\vv p$ alone.\footnote{We could go further and introduce a new \emph{symbol} that equals $\vv p c$, allowing us to carry around fewer $c$'s. We could do the same for $ct$, moreover. Tempting as this is, time and momentum are of such importance that it's best to use the same notation that everyone else uses. Convention trumps elegance this time. In Section \ref{ssec:ff}, however, we'll be a bit bolder with a less central quantity (power).}

Now, we'd like to find a definition of $\vv p c$ that works for \emph{arbitrary} $\vvbeta$. If we're being good epistemologists, we must admit that we don't yet know whether such a universal quantity even exists! Assuming it does, it will help to ask ourselves, ``What \emph{is} momentum, anyway?" First and foremost, it's our motion-related additive-and-conserved vector, and we'll insist that it remain so as we try to generalize it. Consequently, we can't posit an upper limit on the magnitude of $\vv p c$ like we already have with $\beta$. To see why, consider what additivity entails: a system of two particles with identical momenta must have a magnitude of momentum twice that of either particle alone; and if we double the number of particles with identical momenta in the system, then the system's magnitude of momentum doubles also, ad infinitum. So Equation \ref{eq:16} will have to change to allow $p c$ to grow arbitrarily large as $\beta \rightarrow 1$. Something must replace the invariant $E_0$, because the current limitation $pc < E_0$ precludes additivity.

Good. What else do we know about momentum?

In Newtonian mechanics, momentum is velocity scaled by mass, which acts as the \emph{resistance to change} in velocity. Equation \ref{eq:16} expresses the same relationship in different units: in the classical limit, momentum (in energy units) is (normalized) velocity $\vvbeta$ scaled by rest energy $E_0$. Because the tendency to resist change in velocity is called \textbf{inertia}, we might say that rest energy (or mass) is the measure of inertia when $\beta \ll 1$, though ``measure of inertia" isn't really a technical term.

What about when $\beta \centernot{\ll} 1$? Can rest energy be the measure of inertia at \emph{all} relative speeds? If by ``measure of inertia" one indeed means ``resistance to change in velocity," then the answer is \emph{no}! Were resistance to change in velocity invariant, raising a system's $\beta$ from $0$ to $0.001$ would raise it from $0.999$ to $1$ in some other frame. Since nothing can be accelerated to the speed of light, that's a nonstarter.

Along similar lines, whatever takes the place of $E_0$ in the generalization of Equation \ref{eq:16} must \emph{increase without bound} as $\beta \rightarrow 1$. Otherwise we'd have superluminal velocities at sufficiently large $p c$. So this $E_0$-replacement reduces to $E_0$ in the classical limit but is also some monotonic function of speed. That being the case, \emph{momentum and velocity cannot be directly proportional like they are in Newtonian mechanics}.

It follows, by the way, that a \emph{change} in momentum over time (``force") does \emph{not} imply a simple proportional change in velocity as it does when $\beta \ll 1$. We won't actually do that calculus for a while (see Sections \ref{ssec:pf} and \ref{sec:in}), but already we see that special relativity complicates the relationship between force and acceleration. For now we'll withhold judgment on whether the mystery $E_0$-replacement we're looking for can reasonably be called the ``measure of inertia" for arbitrary $\vvbeta$.

As for the matter at hand, we \emph{can} make an educated guess about our mystery quantity. After all, we've already encountered a (very important additive-and-conserved) monotonic function of speed that both reduces to $E_0$ in the classical limit and increases without bound as $\beta \rightarrow 1$: total energy $E$ (see Equation \ref{eq:12}). If our educated guess is correct, then relativistic momentum is $\vv p c = E \vvbeta = \gamma E_0 \vvbeta$. Are we on the right track? Well, $E \vvbeta$ certainly reduces to Equation \ref{eq:16} when $\beta \ll 1$, because then $\gamma \approx 1$. Explicitly:
\begin{equation*}\begin{split}
\vv p c &\stackrel{?}{=}\gamma E_0 \vvbeta \\
&= E_0 \mkern.5mu \beta \hatbeta \left( 1 + \dfrac{1}{2} \, \beta ^2 +  \dfrac {3}{8} \, \beta ^4 + \dots  \right) \\[3pt]
&= E_0 \hatbeta \left( \beta + \dfrac{1}{2} \, \beta^3 + \dots \right)
\end{split}\end{equation*}
(where $\hatbeta$ is the unit vector $\vvbeta / \beta$). Since $\beta \gg \beta^3$ when $\beta \ll 1$, we ignore everything but the first term and recover Equation \ref{eq:16}.

Okay, but is the vector $E \vvbeta$ conserved? There's only one way to know for sure: put it to the test.\footnote{If it's \emph{assumed} that there exists an additive-and-conserved vector quantity that reduces to Equation \ref{eq:16} in the classical limit, then it's possible to show that that vector is $E \vvbeta$ (or, in ``momentum" units, $E \vvbeta/c = \gamma m \vv v$), and not some other vector that happens to behave correctly at the limiting values of $\beta$. Most textbooks do this via the ``Lewis and Tolman" thought experiment involving an elastic collision. But conservation laws are ultimately \emph{empirical} results. That goes for the conservation of total energy, too: we derived Equations \ref{eq:12} and \ref{eq:13} by \emph{assuming} that energy is conserved in special relativity. (Don't worry, it is.)} Easier said than done! Fortunately, we don't have to do it ourselves; particle accelerators run such experiments daily, with values of $\beta$ that approach one. The result? $E \vvbeta$ is conserved every time! So let's make it official:
\begin{equation}\label{eq:17}
\boxed{\vv p c = \gamma E_0 \vvbeta}  \quad \textrm{\footnotesize{ (for $E_0 \neq 0$)}},
\end{equation}
or
\begin{equation}\label{eq:18}
\boxed{\vv p c = E \vvbeta} \, ,
\end{equation}
where $E$ reduces to $E_0$ in the classical limit (cf.\ Equation \ref{eq:16}).\footnote{Notice that Equation \ref{eq:18} has no $E_0 \neq 0$ stipulation? Be suspicious but read on.}

We can apply what we've learned to Einstein's thought experiment. Equation \ref{eq:17} tells us that in the moving observer's frame, the body's velocity can remain unchanged if and only if its $\Delta p c = \gamma \beta \Delta E_0$ ($= \gamma v c \mkern.5mu \Delta m$). That must be what happens. Both light waves carry away momentum, but the momentum the body loses is all $E_0$ (or $m$), \emph{even though neither wave has rest energy}. If the body's speed were low enough for us to use Equation \ref{eq:16} (Equation \ref{eq:1}), we'd have $\Delta p c \approx \beta \Delta E_0$ ($ = v c \mkern.5mu \Delta m$), and the takeaway would still be that the rest energy of a system isn't equal to the sum of the rest energies of its constituents. Rest energy (mass) isn't additive. \emph{Total} energy is.

Hey, we're really making progress! Now we have an equation relating our additive-and-conserved scalar $E$ to our additive-and-conserved vector $\vv p c$. And Equation \ref{eq:18} gives us a new perspective on momentum: constants aside, it's the velocity vector scaled by total energy. The conservation of momentum is nothing but the conservation of energy-times-velocity. Both $E$ and $E \vvbeta$ are additive and conserved.

Finally, we can view Equation \ref{eq:18} as an expression of $\vvbeta$:
\begin{equation*}
\vvbeta= \dfrac{\vv p c}{E}.
\end{equation*}
So velocity is momentum over total energy. That's the additive-and-conserved vector over the additive-and-conserved scalar. \emph{Constant} velocity being displacement $\Delta \vv r$ per time elapsed $c \mkern.5mu \Delta t$, we find a curious relationship among momentum, energy, space, and time:
\begin{equation*}
\dfrac {\vv p c}{E} = \dfrac{\Delta \vv r}{c \mkern.5mu \Delta t} \quad \textrm{\footnotesize{ (for $\vvzeta \equiv \dot{\vvbeta} = \vv 0$)}}.\footnote{Remember that we're using the dot to indicate $ct$-derivatives, in keeping with $\vvbeta = \dot{\vv r}$. So $\vvzeta=(\dd \vvbeta / \dd t)/c = \vv a / c^2$, and we'll call it the ``normalized" acceleration (its magnitude is $\zeta$). Note, though, that it isn't dimensionless---hence the square quotes. It inherits the ``normalized" label to distinguish it from the vanilla acceleration $\vv a = \dd \vv v / \dd t$.}
\end{equation*}
To generalize for instantaneous velocity when the ``normalized" acceleration $\vvzeta \neq \vv 0$, we use infinitesimal changes in $\vv r$ and $ct$ (or take the limit as $c \mkern.5mu \Delta t \rightarrow 0$):
\begin{equation}\label{eq:19}
\vvbeta = \dfrac {\vv p c}{E} = \dfrac{\dd \vv r}{\dd t} \, \dfrac{1}{c}.
\end{equation}


\subsection{The Energy--Momentum Relation and Restlessness}\label{ssec:em}

As for a relation between $E$ and $\vv p c$, we can go a step further than Equation \ref{eq:18}. The trick is to combine Equations \ref{eq:12} and \ref{eq:17} into a single expression that relates these additive-and-conserved quantities to each other by way of the invariant $E_0$. First we square our equations to liberate $\beta^2$ from the radical sign (within $\gamma$):
\begin{equation*}
E^2 = (\gamma E_0)^2
\end{equation*}
and
\begin{equation*}
(\vv p c)^2 = (\gamma E_0 \vvbeta)^2.
\end{equation*}
Then, noting the useful identity
\begin{equation}\label{eq:20}
\gamma^{-2} = 1 - \beta^2,
\end{equation}
we see that all we need to do is subtract one square from the other:
\begin{equation*}
\begin{split}
E^2 - (\vv p c)^2 &= (\gamma E_0)^2 - (\gamma E_0 \vvbeta)^2 \\[2pt]
&= (\gamma E_0)^2 \, (1 - \beta^2) \\[2pt]
&= E_0^2.
\end{split}
\end{equation*}
Voil\`a:
\begin{equation}\label{eq:21}
\boxed{E^2=E_0^2+(\vv p c)^2} \, .
\end{equation}

Mission accomplished! That's the \textbf{energy--momentum relation}, and it's one of the most important equations in physics. Perhaps you noticed that it evinces a Pythagorean relationship: as illustrated in Figure \ref{f:1}, a system's total energy can be represented as the hypotenuse of a right triangle whose shorter legs represent the system's rest energy and magnitude-of-momentum.\footnote{This Pythagorean interpretation has some elegance and it's okay for now, but don't get too attached to it. Later we'll view the energy--momentum relation from another perspective that will prove more fruitful yet!} For a system at rest, the momentum leg shrinks to zero and our triangle becomes a straight line, reducing Equation \ref{eq:21} to Equation \ref{eq:6} ($E=E_0$).

\begin{figure}[h]
\centering
\caption{Equation \ref{eq:21} as a right triangle}
\label{f:1}
\vspace{10pt}
\begin{tikzpicture}[thick]
\coordinate (O) at (0,0);
\coordinate (A) at (4,0);
\coordinate (B) at (0,2);
\coordinate (K) at (.25, 1.9);
\draw (O)--(A)--(B)--cycle;
%\draw[help lines] (0,0) grid (4,3);
\tkzLabelSegment[below](O,A){\textit{$E_0$}}
\tkzLabelSegment[left](O,B){\textit{$p c$}}
\tkzLabelSegment[xshift =.1cm, yshift=0cm](A,B){\textit{$E$}}
\tkzMarkRightAngle[size=0.5,opacity=.4](A,O,B)% square angle here
\end{tikzpicture}
\end{figure}

What about when $E_0 = 0$? We haven't really addressed this head-on yet, but earlier we mentioned in passing that some systems are massless or ``restless" (Footnote \ref{fn:rl})---i.e., they have no rest energy because they have no rest frame. What does it mean for a system to have no rest frame? It means that no frame of reference exists for which the momenta of the system's constituents sum to zero. Put differently, all observers agree that the system's $E \vvbeta \neq \vv 0$ (Equation \ref{eq:18}). Assuming that $E > 0$ (wouldn't be much of a system, otherwise!), this implies that the system's $\beta > 0$ for \emph{everyone}. Nobody can catch up to it. And special relativity tells us that that can only be true if the system travels at the universal speed limit, $\beta = 1$. \emph{A restless system travels at the speed of light}, and vice versa.\footnote{In Section \ref{ssec:sy}, we remarked that a system's velocity is the velocity of its rest frame relative to an observer. Obviously that's nonsense if the system is restless and \emph{has} no rest frame. In this special case, \emph{every constituent of the system} travels at $\beta = 1$ in the same direction.} By Equation \ref{eq:18}, then, $\vv p c = E \hatbeta$ when $E_0 = 0$ (where $\hatbeta$ is a unit vector).

Turning our attention back to Figure \ref{f:1}, we see that for $c$-faring things like light that have no rest frame, the rest-energy leg shrinks to zero, the triangle again collapses to a straight line, and Equation \ref{eq:21} reduces to
\begin{equation}\label{eq:22}
E=p c \quad \textrm{\footnotesize{ (for $E_0=0$)}},
\end{equation}
which is the magnitude of our vectorial relation $\vv p c = E \hatbeta$.

``Hold your horses!" you say. ``What of Equations \ref{eq:12} and \ref{eq:17}? Shouldn't $E_0 = 0$ mean that $E = 0$ and $\vv p c = \vv 0$?"

Well, that would be consistent with Equation \ref{eq:22}, but the answer is no: when we remember to plug $\beta = 1$ into the Lorentz factor in Equations \ref{eq:12} and \ref{eq:17}, we end up not with $E = p c = 0$, but rather with $E = 0/0$ and $\vv p c = \vv 0 / 0$, which are undefined. So Equations \ref{eq:12} and \ref{eq:17} tell us nothing about restless systems, but they also don't rule out the \emph{possibility} that such systems have energy and momentum, provided that $\beta = 1$.

``Fair enough," you concede, ``but you're still pulling the wool over my eyes! You're applying Equations \ref{eq:18} and \ref{eq:21} to restless systems, but we \emph{derived} those equations using Equations \ref{eq:12} and \ref{eq:17}, which \emph{don't} apply to restless systems!"

You've got me there. Guilty as charged.

My defense is a three-parter.

First, \emph{if} Equations \ref{eq:18} and \ref{eq:21} cover restless systems, then they must do so according to the constraint established by Equations \ref{eq:12} and \ref{eq:17}: $\beta = 1$. All we've done to Equation \ref{eq:18} is impose that limitation on it, making $\vvbeta$ a unit vector.

Second, it works! Equation \ref{eq:22} is the same result that Maxwell's equations lead to for electromagnetic radiation, which we know to be massless. It's the full version of Equation \ref{eq:3}---we only needed the proportionality at the time---and it's the relation that quantum mechanics gives for massless particles like photons. So yes, the logic we used was unsound, but Equations \ref{eq:18} and \ref{eq:21} \emph{do} return the right answer for restless systems. It's a happy bonus, and the universal applicability of these two equations makes them treasures.\footnote{Of course, if $E = p c$ is all we've got, then it isn't very useful. We need some additional information to determine the \emph{value} of a restless system's $E$ and $p c$. The conservation principle often comes in handy here (as in Einstein's thought experiment), as do classical electromagnetism and quantum mechanics, both of which relate $E$ and $p c$ to other quantities (amplitude in classical electromagnetism; wavelength and frequency in quantum mechanics).}

Third, it's not \emph{really} just a happy bonus. We've been constructing relativistic dynamics piecemeal, altering Newtonian equations as we go. But it turns out that if you take a top-down approach instead, you can derive Equation \ref{eq:21} in a more general way, leaving no doubt that it applies when $E_0 = 0$.\footnote{This would involve using Noether's theorem to define energy and momentum broadly as the additive-and-conserved quantities associated with the translational symmetries of time and space. Then you'd define rest energy as the difference of their squares and demonstrate that it's invariant.}

A word of caution: traveling at $\beta = 1$ is perfectly compatible with being a \emph{constituent} of a system with a rest frame. We already know this from Einstein's thought experiment, where we treated the body and the two light waves as one closed system with rest energy (and kinetic energy for moving observers). The key is that a system's rest frame is the inertial frame in which its aggregate momentum is zero. In the thought experiment, neither individual light wave has such a frame, because each travels at $\beta = 1$ in a single direction. Since nobody can catch up to it, nobody can say its momentum is zero. In a sense, nobody can even \emph{start} to catch up to it. Its speed is $c$ for everybody. But the two light waves together regarded as a system \emph{do} have a rest frame, because momentum is additive, meaning that a system's momentum is the vector sum of the momenta of its constituents---magnitude and direction. As long as the light waves weren't emitted in exactly the same direction, there exists an inertial frame in which their momenta cancel.



\section{Vectorless Magnitudes:\\ The Tail Wagging the Dog}

If we rearrange Equation \ref{eq:21} like so,
\begin{equation}\label{eq:23}
E_0^2=E^2-(\vv p c)^2,
\end{equation}
we notice something marvelous: the invariance of the left side---the squared rest energy---guarantees the invariance of the right side. Inertial observers in relative motion will disagree on a system's momentum and total energy measured separately, but they'll agree on the difference of their squares, $E^2-(\vv p c)^2$. That's an \emph{incredibly} powerful tool for analyzing a system's dynamics. It allows us to crunch the numbers for whichever frame most simplifies the math (often the rest frame, where $\vv p c = \vv 0$), and then use the invariant Equation \ref{eq:23} to ``convert" to other frames as needed.

You should be reminded of a similar tool:
\begin{equation*}
(\Delta s)^2 = (c \mkern.5mu \Delta t)^2 - (\Delta \vv r)^2,
\end{equation*}
where $\Delta s$ is the invariant spacetime interval between two events. Clearly, the familiar equation for the squared interval has something in common with our new Equation \ref{eq:23}. Inertial observers in relative motion will disagree on the distance and elapsed time between events, but they'll agree on the difference of their squares, $(c \mkern.5mu \Delta t)^2 - (\Delta \vv r)^2$. There's a pattern here, and this isn't the first time we've seen an intriguing connection between $\vv p c$ and $E$ on the one hand and $\vv r$ and $ct$ on the other (Equation \ref{eq:19}). To get to the bottom of this, we'll need the \emph{infinitesimal} spacetime interval, $\dd s$.

\subsection{The Spacetime Distance-Analogue}\label{ssec:ds}

It's important to understand that proper time $ct_0$ isn't restricted to inertial travelers. Proper time is the wristwatch time of \emph{any} traveler.\footnote{\label{fn:tr}In this discussion, we model an accelerating ``traveler" as a point particle.} To reckon the proper time interval $c \mkern.5mu \Delta t_0$ for a non-inertial journey, we'd use calculus. First we'd chop up the traveler's path through spacetime (the traveler's \textbf{world line}) into infinitely small segments, each constituting an instantaneous inertial journey with its own \emph{infinitesimal} timelike interval, as above but with d's instead of deltas:
\begin{equation}\label{eq:24}
\boxed{(\dd s)^2 =(c \, \dd t)^2 - (\dd \vv r)^2} \, .
\end{equation}
Corresponding to each instantaneous journey is an inertial frame of reference for which the traveler is momentarily at rest. We call this an \textbf{instantaneous rest frame}.\footnote{This is sometimes called a \textbf{momentarily comoving frame}. Note that the concept is just as applicable to classical mechanics as it is to special relativity.} In every such frame, $\dd \vv r = \vv 0$, so that $\dd s = c \,\dd t = c \, \dd t_0$, where we've introduced the infinitesimal proper time interval $c \, \dd t_0$. To find $c \mkern.5mu \Delta t_0$, then, we'd just add up all the $c \, \dd t_0$'s using integration: $c \mkern.5mu \Delta t_0 = \int c \, \dd t_0$.

But we don't need $c \mkern.5mu \Delta t_0$ right now. It's the infinitesimals we're interested in. This $\dd s$, the infinitesimal spacetime interval, is analogous to an infinitesimal distance in space $\Vert \dd \vv r \Vert$. Like a spatial distance, $\dd s$ doesn't have direction. It's an infinitesimal scalar quantity. Unlike $\Vert \dd \vv r \Vert$, however, $\dd s$ is invariant with respect to observer velocity (i.e., under a Lorentz boost). Different inertial observers will disagree on the traveler's $c \, \dd t$ and $\dd \vv r$, but they'll agree on the traveler's $(c \, \dd t)^2 - (\dd \vv r)^2$.

\subsection{The (Normalized) Spacetime Speed-Analogue}\label{ssec:b}

In light of this spacetime distance-analogue, a question arises naturally: can we assign the traveler a \emph{rate} of spacetime traversal, analogous to (normalized) speed? Mind, the name of the game is maintaining invariance---that's what makes the interval and the energy--momentum relation so useful---so any spacetime analogue we fashion must have a value that all inertial observers agree on. Dividing $\dd s$ by infinitesimal coordinate time $c \, \dd t$, for example, wouldn't work, since coordinate time is a relative measurement. Still, the answer is yes, we \emph{can} construct an invariant spacetime $\beta$-analogue. We can divide $\dd s$ by the invariant infinitesimal \emph{proper} time interval, $c \, \dd t_0$:\footnote{(that is, differentiate $s$ with respect to $ct_0$)}
\begin{equation*}
\left(\dfrac{\dd s}{\dd t_0} \, \dfrac{1}{c}\right)^2 = \left(c \, \dfrac{\dd t}{\dd t_0} \, \dfrac{1}{c}\right)^2 - \left(\dfrac{\dd \vv r}{\dd t_0} \, \dfrac{1}{c}\right)^2.
\end{equation*}

Before simplifying this, let's make sure we understand all the terms. We interpret the three derivatives as ratios. So $(\dd s/\dd t_0)/c$, our $\beta$-analogue, represents the infinitesimal ``distance" of spacetime a traveler covers per infinitesimal duration of the traveler's own wristwatch time, and all inertial observers agree on this rate. On the far right, $(\dd \vv r/\dd t_0)/c$ is the traveler's infinitesimal displacement as measured by an inertial observer---a frame-dependent vector quantity---divided by the infinitesimal passage of time according to the traveler's watch (the invariant proper time). And the middle term? It's the frame-dependent infinitesimal passage of time according to synchronized clocks in an inertial observer's rest frame (i.e., the observer's coordinate time), divided by the invariant infinitesimal passage of time according to the \emph{traveler} (again, proper time). In other words, it's the time-dilation ratio, which we know is just the Lorentz factor! And didn't we already establish that $\dd s = c \, \dd t_0$? We can work with this. First, by the chain rule we have:
\begin{equation*}
\left(\dfrac{\dd s}{\dd t_0} \, \dfrac{1}{c} \right)^2 = \left(c \, \dfrac{\dd t}{\dd t_0} \, \dfrac{1}{c}\right)^2 - \left(\dfrac{\dd \vv r}{\dd t} \, \dfrac{\dd t}{\dd t_0} \, \frac{1}{c} \right)^2,
\end{equation*}
and subbing in $1 = (\dd s / \dd t_0)/c$, $\gamma = \dd t / \dd t_0$, and $\vvbeta = (\dd \vv r / \dd t)/c$:\footnote{\label{fn:cr}Observe here that since $\gamma = \dd t / \dd t_0$, the chain rule lets us rewrite any $c t_0$-derivative as $\gamma$ times the corresponding $ct$-derivative. Now recall that we're using Newton's overdot notation for $ct$-derivatives and a little circle for $c t_0$-derivatives. Thus, e.g., $\mathring{\vv r} = \gamma \dot{\vv r}$.}
\begin{equation}\label{eq:25}
\boxed{1 = \gamma^2 - \left(\gamma \vvbeta \right)^2} \, .
\end{equation}

Now \emph{that's} rather interesting. Our spacetime $\beta$-analogue is, well, one! That's the invariant and \emph{constant} $c t_0$-rate of all ``motion" through spacetime. And from the perspective of any given inertial observer, the time and space ``contributions" to that rate are, respectively, $\dd t/ \dd t_0=\gamma$ and $(\dd \vv r / \dd t_0)/c = \gamma \vvbeta$. The difference of their squares equals the squared rate, $1^2 = 1$. When $\beta \ll 1$, inertial observers calculate a time contribution $\gamma$ of about one, and a space contribution $\gamma \vvbeta$ of about zero---exactly one and zero in the traveler's instantaneous rest frame. What happens as $\beta$ gets larger? Perhaps you've encountered the unfortunately common pop-sci factoid that the time contribution should then decrease, that ``speed through time" must diminish to accommodate a greater ``speed through space" so that they always sum to one (or to $c$ in different units). As Equation \ref{eq:25} shows, that's a white lie at best (and ``speed through time" is gibberish). Yes, the $ct_0$-rate of spacetime-traversal is always one, and yes, the magnitude of $\gamma \vvbeta$ obviously increases with $\beta$. But look at the time contribution. It doesn't decrease; it blows up to infinity as $\beta$ approaches one! The space contribution does, too, but it can never catch up to the time contribution, since $1>\beta$. So no, the contributions aren't inversely related, and they don't balance to one (or $c$ if unnormalized). The \emph{difference of their squares} balances to $1^2=1$.

Do note that Equation \ref{eq:25} doesn't permit setting $\beta=1$ (leads to division by zero in the Lorentz factors). That's consistent with the fact that proper time is only defined for timelike journeys. No spacelike or lightlike intervals allowed here. To speak of restless systems ``experiencing" time is meaningless.

\subsection{The Spacetime Magnitude-of-Momentum-Analogue}\label{ssec:pc}

So we have a spacetime distance-analogue $\dd s$ and a (normalized) spacetime speed-analogue $(\dd s/\dd t_0)/c=1$, both invariant and the latter constant. We could keep going. Taking a $c t_0$-derivative of the $\beta$-analogue, for instance, would lead us to a spacetime $\zeta$-analogue (magnitude-of-``normalized"-acceleration).\footnote{Recall that we've dubbed $\vvzeta = \dot{\vvbeta} = \vv a / c^2$ the ``normalized" acceleration.} Taking yet another would get us an analogue of ``normalized" jerk. The calculus gets more involved, but the principle is the same. We won't do either of those here, though. Instead, we'll find a spacetime magnitude-of-momentum-analogue to bring this section to a close.\footnote{``Spacetime magnitude-of-momentum-analogue"? Oy. Surely we can do better than this. Read on!}

How do we obtain our $p c$-analogue? Well, we already have a $\beta$-analogue, and we know from Equation \ref{eq:18} that momentum is $\vvbeta$ times $E$. But multiplying by $E$ won't do; it isn't invariant. How about rest energy $E_0$? That's the traveler's total energy $E$ as measured in the traveler's instantaneous rest frame, and it's definitely invariant. Yes, that will work, and so our spacetime $p c$-analogue is $(\dd s/\dd t_0)/c$ times $E_0$. By Equation \ref{eq:25}, we have:
\begin{equation*}
\begin{split}
\left[ \left(\dfrac{\dd s}{\dd t_0} \, \dfrac{1}{c} \right) E_0 \right]^2 &= \left[ \left(c \, \dfrac{\dd t}{\dd t_0} \, \dfrac{1}{c}\right) E_0\right]^2 - \left[ \left(\dfrac{\dd \vv r}{\dd t_0} \, \dfrac{1}{c} \right)E_0\right]^2 \\[5pt]
\bigl[ (1)E_0 \bigr]^2 &=  \bigl[ (\gamma) E_0 \bigr] ^2 - \bigl[ (\gamma \bm{\upbeta}) E_0 \bigr] ^2 ,
\end{split}
\end{equation*}
and by Equations \ref{eq:12} and \ref{eq:17}:
\begin{equation}\label{eq:26}
\boxed{E_0^2 = E^2 - (\vv p c)^2} \, .
\end{equation}

Remind you of anything? (Ahem \dots Equation \ref{eq:23}.) We've come full circle! Here, the space ``contribution" is the traveler's momentum, the time ``contribution" is the traveler's total energy, and the spacetime $p c$-analogue itself is the traveler's invariant rest energy. Now consider this: Earlier we left unanswered the question of whether total energy $E$---a function of $\beta$, really---qualifies as the ``measure of inertia" or ``resistance to change in velocity" for arbitrary $\vvbeta$ (see Section \ref{ssec:rm}). We know that it does when it reduces to the simple invariant scalar $E_0$ in the classical limit, but until we discuss force we'll have to withhold judgment for the general case. In our steps for Equation \ref{eq:26}, however, we constructed our spacetime $p c$-analogue by multiplying the $\beta$-analogue not by a function like $E$, but rather by the invariant $E_0$:
\begin{equation*}
p c \textrm{-analogue} = E_0 \times \beta \textrm{-analogue}.
\end{equation*}
This situation mirrors that of the low-$\beta$ Equation \ref{eq:16}, where $E_0$ is straightforwardly the resistance to change in motion through space. For constant $E_0$, a change in our spacetime $p c$-analogue would imply a strictly proportional change in our spacetime $\beta$-analogue, with $E_0$ as the constant of proportionality. Rest energy, it seems, is the resistance to change in ``motion" \emph{through spacetime}! But wait a minute. Our $\beta$-analogue \emph{can't} change. It's always one; that's why the $p c$-analogue ended up equaling $E_0$. If only there were some aspect of it that \emph{could} change, something that we're overlooking, then it would make sense to speak of $E_0$ as the measure of ``spacetime inertia." Hmm. We'll come back to this.


\subsection{Celerity (Not Proper Velocity)}\label{ssec:ce}

Hidden in our steps for Equation \ref{eq:26} is a pair of relations worth making explicit:
\begin{equation}\label{eq:27}
\dfrac{E}{E_0} = c \, \dfrac{\dd t}{\dd t_0} \, \dfrac{1}{c}
\end{equation}
and
\begin{equation}\label{eq:28}
\dfrac{\vv p c}{E_0} = \dfrac{\dd \vv r}{\dd t_0} \, \dfrac{1}{c}
\end{equation}
Constants aside, total energy and momentum are to invariant rest energy as coordinate time and displacement are to invariant proper time. A comparison with Equation \ref{eq:19} is instructive (it's Equation \ref{eq:28} divided by Equation \ref{eq:27}). Of course, Equation \ref{eq:27} becomes Equation \ref{eq:12} if we sub in $\gamma$ for $\dd t / \dd t_0$. Additionally, we can extract from Equation \ref{eq:28} a ``new" expression for momentum:
\begin{equation*}
\vv p c = E_0 \, \dfrac{\dd \vv r}{\dd t_0} \, \dfrac{1}{c} = E_0 \mkern.5mu \mathring{\vv r},
\end{equation*}
or introducing the symbol $\vvomega \equiv \mathring{\vv r} = \gamma \dot{\vv r} = \gamma \vvbeta$ (see Footnote \ref{fn:cr}):
\begin{equation}\label{eq:29}
\boxed{\vv p c = E_0 \vvomega} \quad \textrm{\footnotesize{ (for $E_0 \neq 0$)}}.
\end{equation}
Looks identical to our low-$\beta$ approximation for momentum but with a \emph{proper}-time derivative instead of a coordinate-time derivative (cf.\ Equation \ref{eq:16}). It goes without saying that Equation \ref{eq:29} is another way of writing Equation \ref{eq:17}; it's only a matter of where we put the Lorentz factor.

We'll call the vector $\vvomega = \gamma \vvbeta$ the (``normalized") \textbf{celerity}.\footnote{The ``unnormalized" celerity $\gamma \vv v$ is usually notated $\vv w$, so $\vvomega = \vv w / c$.} The scare quotes around \emph{normalized} are there because although $\vvomega$ is indeed dimensionless like $\vvbeta$, it has no upper limit (its magnitude $\omega = \gamma \beta$ can exceed one). Celerity often goes by the name ``proper velocity," but that's a confusing misnomer. We'll reserve the word \emph{proper} for rest-frame ``versions" of frame-dependent quantities.\footnote{\label{fn:pr}A competing convention prepends \emph{proper} to $t_0$-derivatives (hence ``proper velocity").} For instance, proper energy $E_0$ is a traveler's total energy $E$ as measured by an observer at rest relative to the traveler. Likewise, a proper time interval $c \mkern.5mu \Delta t_0$ (or $c \, \dd t_0$) is the coordinate time interval $c \mkern.5mu \Delta t$ (or $c \, \dd t$) in the traveler's (instantaneous) rest frame, where $\beta = 0$ and $\gamma = 1$. Proper energy and proper time intervals have \emph{invariant} values, as do all proper quantities; celerity does not. (For proper quantities that are vectors, it's the magnitude that's invariant.) Granted, proper velocity defined ``properly" would be a quantity of little interest---a zero vector---so the term is arguably up for grabs. Still, for the sake of consistency, we won't use it. Further, all our proper quantities will get the naught subscript, like $E_0$ and $c \mkern.5mu \Delta t_0$. The next one we'll encounter is the (``normalized") proper acceleration $\vvzeta_{\mkern1mu 0}$ (Section \ref{ssec:pa}).

Anyway, we've uncovered a definite pattern: we started with the equation for the (squared) invariant infinitesimal interval, with its (squared) scalar ``time contribution" and its (squared) vector ``space contribution," and we've multiplied (or differentiated) it by (squared) invariants to create new invariants, all (squared) spacetime analogues of familiar vector magnitudes. In the following sections, we'll look more closely at our analogy and find a much more convenient way to express it.



\section{Four-Vectors: The Elephant in the Room}
%
%\subsection{A Pythagorean Diversion}
%
%Our mission is to discover what Equations \ref{eq:24}, \ref{eq:25}, and \ref{eq:26} \emph{really} have in common. We've already put our finger on some of it, but there's more.
%
%We begin by noting that the Pythagorean relationship we previously identified with the energy--momentum relation holds for our $\Vert \dd \vv r \Vert$- and $\beta$-analogues, as well. This is illustrated in Figure \ref{f:2}. For a given angle $\theta$, we have $\sin \theta = (\Vert \dd \vv r \Vert / \dd t)/c = \beta = p c / E$, which bears a passing resemblance to Equation \ref{eq:19}. So $\theta = \sin^{-1} {\beta}$, a function of relative velocity. And if you've taken calculus recently, you'll know that the derivative of $\sin^{-1} {\beta}$ is $(1-\beta^2)^{-1/2}$. In other words, the rate of change of $\theta$ with respect to $\beta$ is $\gamma$.
%
%\begin{figure}[h]
%\caption{Equations \ref{eq:24}, \ref{eq:25}, and \ref{eq:26} as right triangles}
%\label{f:2}
%\vspace{10pt}
%\begin{subfigure}[b]{0.32\textwidth}
%\centering
%\resizebox{\linewidth}{!}{
%\begin{tikzpicture}[thick]
%\coordinate (O) at (0,0);
%\coordinate (A) at (4,0);
%\coordinate (B) at (0,2);
%\coordinate (K) at (.25, 1.9);
%\draw (O)--(B)--(A);
%\draw[thick,->] (.5,2.1) -- (.3,1.9);
%%\draw[help lines] (0,0) grid (4,3);
%\tkzMarkRightAngle[size=0.5,opacity=.4](A,O,B)% square angle here
%% Draw the arc which center is (2,1)
%\draw[thick,red] ([shift=(180:4cm)]4,0) arc (180:150:3.5cm);
%\tkzLabelSegment[below,color=red](O,A){\textit{$\dd s = c \, \dd t_0$}}
%\tkzLabelSegment[left](O,B){\textit{$\Vert \dd \vv r \Vert$}}
%\tkzLabelSegment[xshift =.2cm, yshift=0cm](A,B){\textit{$c \, \dd t$}}
%\tkzLabelPoint[xshift=.15cm, yshift=.62cm, color=black](K){\textit{\footnotesize $c \, \dd t_0(\gamma -1)$}}
%\draw[thick,red] (O)--(A);
%\tkzLabelAngle[pos = 1](B,A,O){$\theta$}
%\end{tikzpicture}
%}
%\subcaption{Equation \ref{eq:24}}
%\end{subfigure}
%\begin{subfigure}[b]{0.32\textwidth}
%\centering
%\resizebox{\linewidth}{!}{
%\begin{tikzpicture}[thick]
%\coordinate (O) at (0,0);
%\coordinate (A) at (4,0);
%\coordinate (B) at (0,2);
%\coordinate (K) at (.25, 1.9);
%\draw (O)--(B)--(A);
%\draw[thick,->] (.5,2.1) -- (.3,1.9);
%%\draw[help lines] (0,0) grid (4,3);
%\tkzMarkRightAngle[size=0.5,opacity=.4](A,O,B)% square angle here
%% Draw the arc which center is (2,1)
%\draw[thick,red] ([shift=(180:4cm)]4,0) arc (180:150:3.5cm);
%\tkzLabelSegment[below,color=red](O,A){\textit{$1_{\phantom{0}}$}}
%\tkzLabelSegment[left](O,B){\textit{$\gamma \beta$}}
%\tkzLabelSegment[xshift =.2cm, yshift=0cm](A,B){\textit{$\gamma$}}
%\tkzLabelPoint[xshift=.15cm, yshift=.62cm, color=black](K){\textit{\footnotesize $\gamma -1 \phantom{(E_0)}$}}
%\draw[thick,red] (O)--(A);
%\tkzLabelAngle[pos = 1](B,A,O){$\theta$}
%\end{tikzpicture}
%}
%\subcaption{Equation \ref{eq:25}}
%\end{subfigure}
%\begin{subfigure}[b]{0.32\textwidth}
%\centering
%\resizebox{\linewidth}{!}{
%\begin{tikzpicture}[thick]
%\coordinate (O) at (0,0);
%\coordinate (A) at (4,0);
%\coordinate (B) at (0,2);
%\coordinate (K) at (.25, 1.9);
%\draw (O)--(B)--(A);
%\draw[thick,->] (.5,2.1) -- (.3,1.9);
%%\draw[help lines] (0,0) grid (4,3);
%\tkzMarkRightAngle[size=0.5,opacity=.4](A,O,B)% square angle here
%% Draw the arc which center is (2,1)
%\draw[thick,red] ([shift=(180:4cm)]4,0) arc (180:150:3.5cm);
%\tkzLabelSegment[below,color=red](O,A){\textit{$E_0$}}
%\tkzLabelSegment[left](O,B){\textit{$p c$}}
%\tkzLabelSegment[xshift =.2cm, yshift=0cm](A,B){\textit{$E$}}
%\tkzLabelPoint[xshift=.15cm, yshift=.62cm, color=black](K){\text{\footnotesize $E_{\mkern.5mu \textrm{k}}=E_0(\gamma -1)$}}
%\draw[thick,red] (O)--(A);
%\tkzLabelAngle[pos = 1](B,A,O){$\theta$}
%\end{tikzpicture}
%}
%\subcaption{Equation \ref{eq:26}}
%\end{subfigure}
%\end{figure}
%
%The red legs are our invariants, and their length relative to the $\theta$-dependent hypotenuses is shown by the red circular arcs. The $(\gamma - 1)$ segments indicated by the arrows are the length of the hypotenuses minus the length of the invariant legs. In Figure \ref{f:2}(a), the $(\gamma - 1)$ segment has length $c \, \dd t_0(\gamma -1)$ and represents time dilation. In (c), its length is $E_0(\gamma-1)$ and it represents kinetic energy. In (b), it's the increase in the ``time contribution" of our spacetime $\beta$-analogue that comes with a greater ``space contribution" $\gamma \beta$ (remember that they both blow up to infinity as $\beta$ approaches one).
%
%When $\beta \ll 1$, $\theta$ approaches zero, the red arcs approximate straight lines, and the hypotenuses are about the same length as the invariant legs. In this ``non-relativistic" zone, the $(\gamma - 1)$ segments contribute negligibly to the length of the hypotenuses. As $\beta$ starts to get bigger, those segments remain negligible for a while. Eventually, though, the red arcs start to look arc-ish, and then the segments get appreciably larger. And if you imagine the triangles getting taller and taller and the arcs approaching (but never reaching) a quarter-circle, you'll see that $\gamma$ increases without bound and approximates $(\gamma - 1)$.
%
%All of this is fine, but in a sense we're barking up the wrong tree: the Pythagorean theorem treats the \emph{hypotenuse} as the special quantity---the Euclidean distance---whereas the quantities of most interest to us are represented by the red catheti in Figure \ref{f:2}. Next we'll bring these invariants to the center of our attention. Instead of studying how, say, $E_0$ and $\vv p c$ combine to make $E$ (informative in its own right), we'll focus on how $E$ and $\vv p c$ combine to make $E_0$.


\subsection{Invariant Magnitudes, Transformable Components}

What do Equations \ref{eq:24}, \ref{eq:25}, and \ref{eq:26} \emph{really} have in common? We've already put our finger on some of it, but there's more.

Our instinct might be to note that the Pythagorean relationship we previously identified with the energy--momentum relation holds for our $\Vert \dd \vv r \Vert$- and $\beta$-analogues, as well. We wouldn't be wrong, but in a sense we'd be barking up the wrong tree. The Pythagorean theorem treats the \emph{hypotenuse} as the special quantity. Given a unit circle centered on the origin, the hypotenuse of a right triangle is associated with the distance between the origin and a point on the circle, whereas the other legs (the catheti) are associated with the Cartesian coordinates of the point. Under a rotation of the coordinate axes, it's the \emph{hypotenuse}---the Euclidean distance---that remains unchanged. That's why it's special. But in the context of a Lorentz transformation, the quantities that \emph{look like} ``hypotenuses" in Equations \ref{eq:24}, \ref{eq:25}, and \ref{eq:26} ($c \, \dd t$, $\gamma$, and $E$) are not the ones that remain unchanged! If we embrace a geometric mindset, which we'll soon justify (later we'll even learn to regard a Lorentz boost as a ``rotation" of sorts), then our special quantities are invariants like spacetime intervals and rest energy. Let's bring these invariants to the center of our attention. Instead of studying how, say, $E_0$ and $\vv p c$ combine to make $E$, we'll focus on how $E$ and $\vv p c$ combine to make $E_0$. The payoff will be enormous.

So here again are our Equations \ref{eq:24}, \ref{eq:25}, and \ref{eq:26}, unsquared this time and with the vectors broken down into their Cartesian components:
\begin{align*}
\dd s&=\sqrt{(c \, \dd t)^2 - \Bigl[ (\dd x)^2 + (\dd y)^2 + (\dd z)^2 \Bigr]} \tag{$\Vert \dd \vv r \Vert$-analogue} \\[10pt]
\mathring{s} = \dfrac{\dd s}{\dd t_0} \, \dfrac{1}{c} =1&=\sqrt{\gamma^2 - \Bigl[ (\gamma \beta_x)^2 + (\gamma \beta_y)^2 + (\gamma \beta_z)^2 \Bigr]} \tag{$\beta$-analogue} \\[10pt]
E_0 \mkern1mu \mathring{s} = E_0 &=\sqrt{E^2 - \Bigl[ (cp_x)^2 + (cp_y)^2 + (cp_z)^2 \Bigr] } \tag{$p c$-analogue}
\end{align*}
And here's what we've already acknowledged about them:
\begin{itemize}
\item They're all analogous to familiar vector magnitudes (infinitesimal distance, normalized speed, and magnitude-of-momentum).
\item They're all \emph{invariant} scalars, thanks to our strict procedure for generating the $\beta$- and $p c$-analogues from the timelike $\dd s$.
\item Each has a scalar ``time contribution" with an embedded $c\, \dd t$ factor.
\item Each has a vector ``space contribution" with an embedded $\dd \vv r$ factor.
\item The square of the space contribution is always subtracted from the square of the time contribution.
\end{itemize}
Now let's go deeper.

Each equation takes four frame-dependent quantities in the same unit and fuses them into an invariant scalar. This is achieved by combining their squares in a particular way.

You have experience with something quite like this. In fact, you were looking at an example of it just a moment ago. Focus on the vector components in one of those equations---say, the $\vv p c$-components---and consider what we're doing to them. We're summing their squares, which is to say that we're taking the dot product $\vv p c \cdot \vv p c= (cp_x)^2 + (cp_y)^2 + (cp_z)^2 = (p c)^2$. If the energy term (and minus sign) weren't there, that dot product would be the entire radicand, and the square root would be $p c$, the magnitude of the momentum vector $\vv p c$. When we use this component-squaring method to find a self--dot product or a vector's magnitude in three-dimensional Euclidean space, we're combining three numbers that aren't invariant into a scalar with the same unit that \emph{is} invariant, though not in the sense we've been using the word in this paper (i.e., not under a boost; see Footnote \ref{fn:inv}). Vector components depend on your chosen origin and axes, but a vector's magnitude (and its square) stays the same if you arbitrarily rotate or translate (shift) your coordinate system.\footnote{\label{fn:pv}The position vector $\vv r$, which always has its tail at the origin, is an exception when it comes to translation. Its magnitude is invariant under \emph{rotation}, but if you shift your axes then you end up with a new origin, and consequently an entirely new position vector. Not so for $\Delta \vv r$ or its infinitesimal counterpart $\dd \vv r$; the magnitude of a displacement or separation vector is indeed invariant under translation.} Within the context of that three-dimensional space, the magnitude and squared magnitude of your run-of-the-mill vector are invariant scalars, while the vector's components transform under rotation or translation according to some linear formulas.

The invariant quantities \emph{we've} been dealing with (like $E_0$) are invariant under boosts in the context of the four-dimensional spacetime we inhabit. They're \textbf{\emph{Lorentz}-invariant}, and this brings us to one more thing our analogues have in common: the four (unsquared) quantities on the right side of Equations \ref{eq:24}, \ref{eq:25}, and \ref{eq:26} all obey the Lorentz transformation. Why? Because we started with the infinitesimals $c\, \dd t$, $\dd x$, $\dd y$, and $\dd z$ (Equation \ref{eq:24}), which we know transform Lorentzianly, and all we did to them was multiply by or differentiate with respect to invariant scalars ($ct_0$ and $E_0$), quantities whose values all observers agree on. Since the Lorentz transformation is linear, and since multiplication and differentiation are linear operations, the results must \emph{also} obey the Lorentz transformation! Under standard configuration, then, momentum and energy (Equation \ref{eq:26}) transform between frames like this:
\begin{equation}\label{eq:fm}
\begin{aligned}
E^\prime &= \gamma \left( E - \beta cp_x \right) & E &= \gamma \left( E^\prime + \beta cp_x^\prime \right) \\
cp_x^\prime &= \gamma \left( cp_x - \beta E \right) & \qquad cp_x &= \gamma \left( cp_x^\prime + \beta E^\prime \right) \\
cp_y^\prime &=cp_y & cp_y &= cp_y^\prime \\
cp_z^\prime &=cp_z & cp_z &= cp_z^\prime
\end{aligned}
\end{equation}
(we've just substituted energy and momentum-components for duration and separation-components in Equations \ref{eq:lt2}). For the transformation of $\gamma$ and $\gamma \vvbeta$ [$= \vvomega$] (Equation \ref{eq:25}), it helps to reintroduce the ``rel" subscript for the relative speed between the primed and unprimed frames, as well as for the corresponding Lorentz factor, so as not to confuse them with the primed and unprimed measurements of some third party's velocity and Lorentz factor: 
\begin{equation}\label{eq:fv}
\begin{aligned}
\gamma^\prime &= \gamma_{\textrm{rel}} \left( \gamma - \beta_{\textrm{rel}} \mkern1mu \gamma \beta_x \right) & \gamma &= \gamma_{\textrm{rel}} \left( \gamma^\prime + \beta_{\textrm{rel}} \mkern1mu \gamma^\prime \beta_x^\prime \right) \\
\gamma^\prime \beta_x^\prime &= \gamma_{\textrm{rel}} \left( \gamma \beta_x - \beta_{\textrm{rel}} \mkern1mu \gamma \right) & \qquad \gamma \beta_x &= \gamma_{\textrm{rel}} \left( \gamma^\prime \beta_x^\prime + \beta_{\textrm{rel}} \mkern1mu \gamma^\prime \right) \\
\gamma^\prime \beta_y^\prime &= \gamma \beta_y & \gamma \beta_y &= \gamma^\prime \beta_y^\prime \\
\gamma^\prime \beta_z^\prime &= \gamma \beta_z & \gamma \beta_z &= \gamma^\prime \beta_z^\prime.
\end{aligned}
\end{equation}
A bit of algebra leads directly back to Equations \ref{eq:vt}.

We're finally ready to put it all together. Our spacetime analogues really are just like vector magnitudes. They're invariant, and their squares are found by combining the squares of quantities that aren't invariant but which transform together according to a family of linear formulas. What's different is the \emph{method} of combination. Instead of adding all the squares like we're used to doing with vector components in Euclidean geometry (Pythagorean theorem), we subtract the squares of the space contributions from the square of the time contribution. This difference reflects the fact that although space and time are intimately connected, they're not the same thing.


\subsection{Four-Displacement: The Ur-Four-Vector}\label{ssec:fd}

We're missing one ingredient---direction! This is easily remedied. In what direction does a traveler journey through spacetime? The question answers itself: in the direction the traveler happens to be going. Now, direction is relative, even in three-dimensional Euclidean space, and so it must be specified in relation to something. Strictly speaking, a whole coordinate system isn't necessary; it's enough to have two events (two points in spacetime) to specify the direction from one to the other, from ``here-and-now" to ``there-and-then." Coordinate systems have their advantages, though. Once an inertial observer has adopted one, ``now" and ``then" can be expressed as coordinate times, and ``here" and ``there" can be expressed as spatial coordinates in reference to the observer's chosen spatial origin $(x,y,z)=(0,0,0)$. But the observer can also put ``from now to then" on an equal footing with ``from here$_{x, y, z}$ to there$_{x, y, z}$," and catalog all events a traveler reaches in reference to a chosen \emph{spacetime} origin, $(ct,x,y,z)=(0,0,0,0)$. That way, the \emph{spacetime} direction from one event on the traveler's world line to the ``next" (infinitesimally close to the first) can be thought of as an arrow pointing from the temporally earlier event ($ct$, $x$, $y$, $z$) to the temporally later event ($ct + c \, \dd t$, $x + \dd x$, $y + \dd y$, $z + \dd z$). In the traveler's instantaneous rest frame, $\dd \vv r = \vv 0$, and the spacetime direction is entirely temporal. In all other frames, however, observers say that the traveler's spacetime direction is both temporal and spatial. And they're all correct! Different inertial observers have different ideas of how far apart ``here" and ``there" are, and how much time passes between ``now" and ``then," but they agree on the spacetime interval between events, and they agree on the \emph{chronological order} of timelike- (and lightlike-) separated events.

One more step. We take this notion of a traveler's spacetime direction, and we pair it with the magnitude $\dd s$, the infinitesimal spacetime interval between ``adjacent" events on the traveler's world line. The resulting object is an infinitesimal displacement vector, analogous to our old friend $\dd \vv r$ that dwells in three-dimensional Euclidean space. Since this new vector dwells in four-dimensional spacetime, we say that it's a \textbf{four-vector}, and now we'll use the term \textbf{three-vector} to refer to $\dd \vv r$ and its associates. The \emph{components} of our new (infinitesimal) \textbf{four-displacement} are what we've hitherto called the time and space ``contributions" to $\dd s$: $c \, \dd t$, $\dd x$, $\dd y$, and $\dd z$. It bears repeating that they obey the Lorentz transformation, and (equivalently) that the vector's magnitude is Lorentz-invariant. These two properties are so important that we make them sine qua nons for any future four-vectors we define. \emph{By definition, four-vectors have Lorentz-invariant magnitudes and Lorentz-transformable components.} Fortunately, these conditions are satisfied automatically if we use the same procedure for generating new four-vectors that we used earlier to generate our spacetime analogues: take an existing four-vector, and either multiply it by or differentiate it with respect to an invariant.\footnote{More generally, \emph{any} four quantities that transform together Lorentzianly are the components of a four-vector.} In the case of our spacetime $\beta$- and $p c$-analogues, we've already done the work, and we'll see that they are the magnitudes of four-vectors, just as our spacetime $\Vert \dd \vv r \Vert$-analogue ($\dd s$) is the magnitude of four-displacement.

We'll need some notation. To distinguish the four-displacement from the three-displacement $\dd \vv r$, let's use uppercase for the former:
\begin{equation}\label{eq:30}
\boxed{\dd \vv R\equiv \langle c \, \dd t, \, \dd \vv r \rangle = \langle c\, \dd t, \, \dd x, \, \dd y, \, \dd z \rangle} \, .
\end{equation}
Its magnitude is
\begin{equation*}
\begin{split}
\Vert \dd \vv R \Vert &= \sqrt{\dd \vv R \cdot \dd \vv R}\\
&= \sqrt{(c \, \dd t)^2 - (\dd \vv r \cdot \dd \vv r)}\\
&= \sqrt{(c \, \dd t)^2 - (\dd x)^2 - (\dd y)^2 - (\dd z)^2}\\
&= \dd s,
\end{split}
\end{equation*}
the invariant interval (timelike, but let's allow lightlike too). We already knew that, but now we see explicitly that the dot product of two four-vectors differs from the dot product of two three-vectors by the signs of the products of the components: instead of ($+$$+$$+$), we have ($+$$-$$-$$-$), with the product of the time components in first position.\footnote{\label{fn:sc}It works just as well to use the convention ($-$$+$$+$$+$), or even to multiply the time component by the imaginary unit $\mathrm{i}$ and use ($+$$+$$+$$+$).} Why? Because of the equation for the spacetime interval (Equation \ref{eq:24}). That's just how it is in our \textbf{Minkowski spacetime}.\footnote{The mathematician Hermann Minkowski developed the geometric approach to special relativity we're exploring. He used the ($+$$+$$+$$+$) convention mentioned in the previous footnote, by the way, but it's fallen out of favor.} For constant velocity, we can of course use a finite four-displacement $\Delta \vv R = \langle c \mkern.5mu \Delta t, \Delta \vv r \rangle$.\footnote{\label{fn:sp}We can speak more abstractly of a \textbf{four-separation} $\Delta \vv R = \langle c \mkern.5mu \Delta t, \Delta \vv r \rangle$ that ``points" from one event to another. They needn't be on a traveler's world line, and the ``from" event needn't be chronologically earlier than the ``to" event. Indeed, the events may be spacelike-separated, in which case they don't even \emph{have} a fixed chronological order, and the vector's magnitude is then a \emph{spacelike} interval (times $\mathrm{i}$---see Section \ref{sssec:td}).}

You may be wondering if our new \textbf{Minkowski dot product} has a geometric definition like the Euclidean dot product does ($\vv q \cdot \vv w = qw \cos \theta$). The answer is a qualified yes: for \emph{qualifying} four-vectors $\vv Q$ and $\vv W$, the analogous relation is $\vv Q \cdot \vv W = QW \cosh \phi$. Here we have the hyperbolic cosine function and its argument $\phi$, which in this context can be thought of as the \emph{hyperbolic angle} between $\vv Q$ and $\vv W$ (itself a concept that needs qualifying). We'll forgo the details until Section \ref{sec:ra}. For now, it's enough to know that in Minkowski spacetime, \textbf{orthogonal vectors} remain those whose dot product is zero, and \textbf{parallel vectors} remain those that are scalar multiples of each other (\textbf{codirectional} if the scalar is positive, \textbf{contradirectional} if it's negative).\footnote{The words \emph{parallel} and \emph{antiparallel} are often used instead of \emph{codirectional} and \emph{contradirectional}, but then there's no word left to mean ``codirectional or contradirectional." For us, \emph{parallel} means ``codirectional or contradirectional" and gets the symbol $\parallel$. So if $\vv Q \parallel \vv W$, then $\vv Q$ and $\vv W$ are either codirectional or contradirectional. (Same for three-vectors.)}


\subsection{Four-Position?}

We could have started with a \emph{position} four-vector, but the \textbf{four-position} $\vv R = \langle ct, \vv r \rangle$ has the same limitation as the position three-vector mentioned in Footnote \ref{fn:pv}---namely, its magnitude isn't invariant under translation because its tail is always at the (spacetime) origin. Different origin? Different position vector. With standard configuration, this is no problem, since the frames share a spacetime origin and are merely ``boosted" relative to each other along their coincident $x$-axes. But all the other four-vectors we'll discuss have magnitudes that are invariant even under translation. In the jargon, these vector magnitudes are \textbf{Poincar\'e-invariant}. The magnitude of the four-position is \emph{Lorentz}-invariant like the others, but not Poincar\'e-invariant. For that reason, we take the \emph{displacement} four-vector as our prototype, rather than the four-position.


\subsection{(Normalized) Four-Velocity}

Next, we define the normalized \textbf{four-velocity} by replicating our steps for obtaining the spacetime $\beta$-analogue from the $\Vert \dd \vv r \Vert$-analogue (cf.\ Section \ref{ssec:b}). Again, we use uppercase (we'll always use uppercase for four-vectors and lowercase for three-vectors):
\begin{equation}\label{eq:31}
\boxed{
\begin{aligned}
\vv B&\equiv \mathring{\vv R} = \dfrac{1}{c} \, \dfrac{\dd \vv R}{\dd t_0} \\[3pt]
&= \langle \gamma, \gamma \vvbeta \rangle = \langle \gamma, \mkern1mu \vvomega \rangle \\[4pt]
&= \gamma \mkern1.5mu \big \langle 1, \, \beta_x, \, \beta_y, \, \beta_z \big \rangle
\end{aligned}
} \, .
\end{equation}
Its magnitude is $B = \sqrt{\vv B \cdot \vv B} = \gamma \sqrt{1 - (\vvbeta \cdot \vvbeta)} = 1$, a \emph{constant} invariant. As we could have guessed from Equation \ref{eq:25}, $\vv B$ is a unit vector. Note the similarity between Equation \ref{eq:31}'s $\vv B = \mathring{\vv R}$ and the relation $\vvbeta = \dot{\vv r}$.\footnote{Recall that we're using the circle for $c t_0$-derivatives and the dot for $ct$-derivatives. Also, the unnormalized four-velocity is $\vv V = \dd \vv R / \dd t_0 = \vv B c$, just as $\vv v = \vvbeta c$.}

Because proper time is undefined for lightlike intervals (or, if you prefer, because $\gamma$ is undefined for $\beta = 1$), $\vv B$ is undefined for light and anything else without a rest frame.


\subsection{Four-Momentum:\\ One Conservation Law to Rule Them All}

By the same logic, we can construct a \textbf{four-momentum} in terms of the normalized four-velocity, replicating our steps for obtaining the spacetime $p c$-analogue from the $\beta$-analogue (cf.\ Section \ref{ssec:pc}). We'll define it so that it has the dimension of energy:\footnote{Many authors use $\vv P \equiv \langle E/c , \, \vv p \rangle = m \vv V$ instead, giving it the dimension of momentum.}
\begin{equation}\label{eq:32}
\boxed{
\begin{aligned}
\vv P &= E_0 \vv B \\
\vv P &\equiv \langle E, \vv p c \rangle = \langle E, \, c p_x, \, c p_y, \, c p_z \rangle
\end{aligned}
} \, .
\end{equation}
Its magnitude is $P = \sqrt{\vv P \cdot \vv P} = \sqrt{E^2 - (\vv p c \cdot \vv p c)} = E_0$, the invariant rest energy. Note the similarity between Equation \ref{eq:32}'s $\vv P = E_0 \vv B$ and the relation $\vv p c = E \vvbeta$ (Equation \ref{eq:18}).

Unlike the four-velocity $\vv B$, the four-momentum $\vv P$ \emph{is} defined for a restless system, but only in component form $\langle E, \vv p c \rangle$. (That's why we reserved the ``$\equiv$" symbol for the second line in Equation \ref{eq:32}.) What's more, Equation \ref{eq:22} tells us that $E=p c$ in that case. The magnitude of the four-momentum is then $P = \sqrt{E^2 - (p c)^2} = 0$, as it must be when $E_0 = 0$. So the four-momentum of a restless system is a \textbf{null vector}, even though its components aren't zero. That runs counter to our Euclidean intuition. In Minkowski spacetime, a four-vector is null if the absolute value of its scalar time component equals the magnitude of its three-vector spatial component. The \textbf{zero four-vector} (we'll use the symbol \mbox{\boldmath$\emptyset$} to distinguish it from the null/zero three-vector $\vv 0$) is a null vector that indeed has only zeros for components; it has the special property of being orthogonal to all four-vectors.

Speaking of components, $E$ and $\vv p c$ are special because they're our additive-and-conserved quantities. And since the \emph{components} of four-momentum are additive and conserved, four-momentum \emph{itself} is additive and conserved.\footnote{Careful: this doesn't mean that the \emph{magnitude} of four-momentum is additive and conserved. The magnitude is $E_0$, which we know to be conserved but \emph{not} additive!} We've combined two additive conservation laws into one---the \textbf{additivity and conservation of four-momentum}. Fancy that!

Let's briefly see this law in action. Suppose a closed system consists only of particle $a$ and particle $b$. If they collide elastically, then $\vv P_{\mathrm{system}} = \vv P_a + \vv P_b$ before and after the collision,\footnote{In these types of problems, it's usually assumed that there's no potential energy or field momentum to worry about. If, however, field contributions aren't negligible, then we must use $\vv P_{\mathrm{system}} = \vv P_a + \vv P_b + \vv P_{\mathrm{field}}$. Naturally, conservation still holds.} and the system four-momentum before the collision (subscript i for \emph{initial}) equals the system four-momentum after the collision (subscript f for \emph{final}):
\begin{equation*}
\begin{aligned}
\vv P_{a, \mkern1mu \mathrm{i}} + \vv P_{b, \mkern1mu \mathrm{i}} &= \vv P_{a, \mkern1mu \mathrm{f}} + \vv P_{b, \mkern1mu \mathrm{f}}\\
\langle E, \vv p c \rangle_{a, \mkern1mu \mathrm{i}} + \langle E, \vv p c \rangle_{b, \mkern1mu \mathrm{i}} &= \langle E, \vv p c \rangle_{a, \mkern1mu \mathrm{f}} + \langle E, \vv p c \rangle_{b, \mkern1mu \mathrm{f}},
\end{aligned}
\end{equation*}
i.e.:
\begin{equation*}
\begin{split}
E_{a, \mkern1mu \mathrm{i}} + E_{b, \mkern1mu \mathrm{i}} &= E_{a, \mkern1mu \mathrm{f}} + E_{b, \mkern1mu \mathrm{f}}\\
cp_{x\,a, \mkern1mu \mathrm{i}} + cp_{x\,b, \mkern1mu \mathrm{i}} &= cp_{x\,a, \mkern1mu \mathrm{f}} + cp_{x\,b, \mkern1mu \mathrm{f}}\\
cp_{y\,a, \mkern1mu \mathrm{i}} + cp_{y\,b, \mkern1mu \mathrm{i}} &= cp_{y\,a, \mkern1mu \mathrm{f}} + cp_{y\,b, \mkern1mu \mathrm{f}}\\
cp_{z\,a, \mkern1mu \mathrm{i}} + cp_{z\,b, \mkern1mu \mathrm{i}} &= cp_{z\,a, \mkern1mu \mathrm{f}} + cp_{z\,b, \mkern1mu \mathrm{f}}.
\end{split}
\end{equation*}
Or say particle $d$ decays into particles $e$ and $g$. Then we have:
\begin{equation*}
\begin{split}
\vv P_d &= \vv P_e + \vv P_g\\
\langle E, \vv p c \rangle_{d} &= \langle E, \vv p c \rangle_{e} + \langle E, \vv p c \rangle_{g},
\end{split}
\end{equation*}
which works even if the decay products are restless (massless) particles, as when a pion decays into two photons.

Because the conservation of four-momentum holds for any inertial observer, many problems in relativistic dynamics are best solved by considering the rest frame of either the system or one of its constituents. There, the only non-zero component of $\vv P_{\mathrm{system}}$ (or of $\vv P_{\mathrm{constituent}}$) is the time component, $E_{\mathrm{system}}$ ($E_{\mathrm{constituent}}$), which equals the system's (constituent's) rest energy. Since rest energy is invariant, one can then use that result in conjunction with the conservation of four-momentum to find unknowns in other frames.


\subsection{Four-Force, Three-Force, and Power}\label{ssec:ff}

In Section \ref{ssec:pc}, we puzzled over the notion that rest energy is the measure of ``spacetime inertia." Now with Equation \ref{eq:32} we can understand what that means. Although $\vv B$'s magnitude is invariant and fixed at one, its \emph{direction} is certainly changeable. A system's rest energy must be its resistance to change in its direction of ``motion" through \emph{spacetime}.

With all this talk about resistance to change in velocity, it's about time we introduced a relativistic concept of force. But before we do, let's reiterate the stipulation we made in Footnote \ref{fn:tr}: for now, we model any accelerating system (or ``traveler") as a point particle. Otherwise we'd need to account for the fact that the whole system doesn't accelerate ``as one" under the influence of a (non-gravitational) force, and we'd have to allow for different parts of the system to have different rest frames. Let's also remind ourselves of a related issue we encountered back in Section \ref{ssec:re}: if the transfer of rest energy into or out of a composite system occurs at more than one location on the system's bounds, then the relativity of simultaneity makes it difficult if not impossible to consider the rest energy ``invariant." If the energy-transfer occurs at a single location on the system's bounds, or if the system is modeled as a point particle, then the relativity of simultaneity presents no such issue.

On to force. In Newtonian mechanics, force is $\vv f_{\mathrm{classical}} \equiv \dd \vv p_{\mathrm{classical}} / \dd t$.\footnote{\label{fn:cf}We regard $\vv f_{\mathrm{classical}} = m \vv a$ as incidental, valid only in the ``special case" that $\dot{m} = 0$. It's probably more common to \emph{define} classical force as $m \vv a$, so that its magnitude is \emph{always} invariant under a Galilean boost (even in a variable-mass situation). The definition we're using falls short in that regard (the chain rule produces a frame-dependent $(\dd m / \dd t) \vv v$ term), but it has two advantages for us: first, it emphasizes a conserved quantity (momentum); and second, it makes the transition to force in special relativity rather straightforward.} Let's take this as our cue and define a \textbf{four-force}---sometimes called the \textbf{Minkowski force}---in terms of the four-momentum (we'll use the chain rule with $\dd t / \dd t_0 = \gamma$):
\begin{equation}\label{eq:33}
\boxed{
\begin{aligned}
\vv F &\equiv \mathring{\vv P} \\[2pt]
&= \langle \mathring{E}, \mkern.5mu \mathring{\vv p} c \rangle \\[3pt]
&= \gamma \mkern1.5mu \big \langle \dot{E}, \mkern.5mu \dot{\vv p} c \big \rangle \\[3pt]
&=\gamma \mkern1.5mu \big \langle \mathcal{P}, \mkern.5mu \vv f \big \rangle
\end{aligned}
} \, ,
\end{equation}
where $\mathcal{P} \equiv \dot{E} = (\dd E / \dd t)/c$ is \textbf{relativistic power}, and $\vv f \equiv \dot{\vv p} c = (\dd \vv p c / \dd t)/c$ is \textbf{relativistic three-force}. Or if we wanted to be dimensionally consistent with classical physics, we could instead say $\mathcal{P} \equiv \dd E / \dd t$, and we'd have $\vv F = \gamma \langle \mathcal{P}/c, \mkern.5mu \vv f \rangle$. But we'll be bold and choose elegance over convention.\footnote{Recall that we chose convention over elegance when we opted \emph{not} to dimensionally ``redefine" momentum (or time!) back in Section \ref{ssec:rm}. As a result, we carry around more $c$'s than we need to---always $\vv p c$ and $ct$. With $\vvbeta$ and $\vvzeta$, by contrast, we avoid $\vv v / c$ and $\vv a / c^2$.} Like the four-velocity, the four-force is defined as a $ct_0$-derivative and is therefore inapplicable to restless systems.

So the greater a system's rest energy $E_0$, the greater its resistance to change in (direction of) four-velocity $\vv B$ under the influence of an acceleration-causing four-force $\vv F$. We specify ``acceleration-causing" because a system whose $E_0$ changes while its $\vvbeta$ (and hence $\vv B$) doesn't---like the light-emitting body in Einstein's thought experiment---is under the influence of a four-force, too.\footnote{\label{fn:fo}Unlike the classical force, whose magnitude isn't Galilean-invariant in a variable-mass scenario because of the frame-dependent $(\dd m / \dd t) \vv v$ term mentioned in Footnote \ref{fn:cf}, the four-force has a magnitude that's \emph{always} Lorentz-invariant (provided we model the system in question as a point particle). The chain rule does produce an analogous $\mathring{E}_0 \vv B$ term ($\vv F = \mathring{\vv P} = \mathring{E}_0 \vv B + E_0 \mathring{\vv B}$), but $\vv B$ is itself a four-vector with a Lorentz-invariant magnitude, whereas $\vv v$'s magnitude varies under a Galilean boost. \emph{The four-velocity is \emph{not} to the Lorentz transformation as the three-velocity is to the Galilean transformation.}}

In the common scenario that a system's $E_0$ doesn't change, the relativistic power $\mathcal{P} = \dot{E} = \dot{E}_{\mkern.5mu \textrm{k}}$, and we're inclined to ask ourselves whether the \textbf{work--kinetic energy principle} from Newtonian mechanics holds in special relativity: does $\dd E_{\mkern.5mu \textrm{k}} = \vv f \cdot \dd \vv r$ in this case? Or equivalently, does $\dot{E}_{\mkern.5mu \textrm{k}} = \vv f \cdot \vvbeta$? It does, as we discover if we take the $ct$-derivative of Equation \ref{eq:21}:
\begin{equation}\label{eq:rp}
\begin{split}
\frac{\dd}{\dd t} \Big[ E^2  &= E_0^2 + (\vv p c)^2 \Big] \frac{1}{c}\\[5pt]
2E \dot{E} &= 2 E_0 \dot{E}_0 + 2 \vv p c \cdot \dot{\vv p} c\\[5pt]
\dot{E} &= \dfrac{E_0}{E} \, \dot{E}_0 + \dot{\vv p} c \mkern1mu \cdot \dfrac{\vv p c}{E} \\[5pt]
\mathcal{P} &= \dfrac{\mathring{E}_0}{\gamma^2} + \vv f \cdot \vvbeta
\end{split}
\end{equation}
(by Equations \ref{eq:12} and \ref{eq:19}), which becomes $\mathcal{P} = \dot{E}_{\mkern.5mu \textrm{k}} = \vv f \cdot \vvbeta$ when $\dot{E}_0 = 0$.\footnote{The quantity $\mathring{E}_0 = \gamma \dot{E}_0$ that we introduced in Footnote \ref{fn:fo} and Equation \ref{eq:rp} is invariant (the proper-time derivative of the proper energy)---provided, of course, that no relativity-of-simultaneity issues throw a wrench in the works. Later we'll dub it the proper power.} Equation \ref{eq:33} then takes the sometimes useful form:
\begin{equation}\label{eq:fb}
\vv F = \gamma \mkern1.5mu \big \langle \vv f \cdot \vvbeta, \, \vv f \big \rangle \quad \textrm{\footnotesize{ (for $\mathring{E}_0 = 0$)}}.
\end{equation}

\subsection{Invariance of the Minkowski Dot Product}

Now, we've seen what happens when we take the dot product of a four-vector with itself: we get its (squared) magnitude, which is invariant. But what happens when we take the Minkowski dot product of one four-vector with another? A four-momentum and a four-displacement, say:
\begin{equation*}
\vv P \cdot \dd \vv R = (E)(c \, \dd t) - (\vv p c \cdot \dd \vv r) .
\end{equation*}
Is \emph{that} invariant? Well, how do things look in another frame?
\begin{equation*}
\vv P^\prime \cdot \dd \vv R^\prime = (E^\prime)(c \, \dd t^\prime) - (\vv p c ^\prime \cdot \dd \vv r ^\prime) .
\end{equation*}
We want to know whether $\vv P \cdot \dd \vv R = \vv P^\prime \cdot \dd \vv R^\prime$. We can put the frames in standard configuration without losing generality, and then the question becomes: does $(E^\prime)(c \, \dd t^\prime) - (cp_x^\prime)(\dd x^\prime)$ equal the unprimed $(E)(c \, \dd t) - (cp_x)(\dd x)$? To find out, we'll use the Lorentz transformation. For the components of four-momentum, that's Equations \ref{eq:fm}; for the components of four-displacement, it's Equations \ref{eq:lt2} (with $\dd$'s instead of $\Delta$'s). We'll also use Equation \ref{eq:20}:
\begin{equation*}
\begin{split}
(E^\prime)(c \, \dd t^\prime) - (cp_x^\prime)(\dd x^\prime)&=\Bigl[ \gamma(E-\beta cp_x) \Bigr] \Bigl[ \gamma(c \, \dd t - \beta \, \dd x) \Bigr]\\
&\quad \quad \quad - \Bigl[ \gamma(cp_x - \beta E) \Bigr] \Bigl[ \gamma(\dd x - \beta \mkern1mu c \, \dd t) \Bigr]\\[5pt]
&=\gamma^2 \bigl( E \mkern1mu c \, \dd t - \cancel{E \beta \, \dd x} - \bcancel{\beta c p_x \, c \, \dd t} + \beta^2 c p_x \, \dd x \bigr) \\
&\quad \quad \quad - \gamma^2 \bigl( cp_x \, \dd x - \bcancel{c p_x \beta \, c \, \dd t} - \cancel{\beta E \, \dd x} + \beta^2 E \mkern1mu c \, \dd t \bigr) \\[5pt]
&=\gamma^2 \Bigl[ (E \mkern1mu c \, \dd t - \beta^2 E \mkern1mu c \, \dd t) - (cp_x \, \dd x - \beta^2 cp_x \, \dd x) \Bigr]\\[5pt]
&=\gamma^2 \Bigl[ (E \mkern1mu c \, \dd t)(1-\beta^2) - (cp_x \, \dd x)(1-\beta^2) \Bigr]\\[5pt]
&=(E)(c \, \dd t) - (cp_x) (\dd x).
\end{split}
\end{equation*}
Wonderful! The dot product of \emph{any} two four-vectors is Lorentz-invariant\footnote{(Poincar\'e-invariant, even, as long as there are no position four-vectors involved)} as a direct consequence of the Lorentz-transformability of their components. Frankly, if that weren't the case, this four-vector business would be pointless.

Here is a quick example of the utility---nay, necessity---of this result. Imagine a perfectly inelastic collision between particles $h$ and $j$ with known rest energies $E_{0 \mkern.5mu h}$ and $E_{0j}$. Say that particle $h$ travels with velocity $\vvbeta$ (and Lorentz factor $\gamma$) in particle $j$'s rest frame before the collision. Given that information, how could we find the rest energy of the resulting composite ``blob"? By the additivity and conservation of four-momentum, we have:
\begin{equation*}
\vv P_h + \vv P_j = \vv P_{\textrm{blob}}.
\end{equation*}
Since $\vv P \cdot \vv P = P^2 = E_0^2$, we square the equation to get our desired unknown on the right side:
\begin{equation*}
\begin{split}
(\vv P_h + \vv P_j) \cdot (\vv P_h + \vv P_j) &= \vv P_{\textrm{blob}} \cdot \vv P_{\textrm{blob}} \\[2pt]
E_{0 \mkern.5mu h}^2 + 2 (\vv P_h \cdot \vv P_j) + E_{0j}^2 &= E_{0 \, \textrm{blob}}^2.
\end{split}
\end{equation*}
If the dot product of two four-vectors weren't invariant, we'd be stuck here! Fortunately, it is, and we can evaluate $\vv P_h \cdot \vv P_j$ in any inertial frame we choose. Let's make it easy on ourselves and choose the rest frame of particle $j$, where $\vv P_j = \langle E_{0j}, \vv 0 \rangle$ and $\vv P_h = \langle E, \, \vv p c \rangle_h =  \langle \gamma E_{0 \mkern.5mu h}, \, \gamma E_{0 \mkern.5mu h} \vvbeta \rangle$. Now we obtain $\vv P_h \cdot \vv P_j = (\gamma E_{0 \mkern.5mu h})(E_{0j}) - 0$. Plugging that in:
\begin{equation*}
E_{0 \, \textrm{blob}}^2 = E_{0 \mkern.5mu h}^2 + 2 (\gamma E_{0 \mkern.5mu h} E_{0j}) + E_{0j}^2,
\end{equation*}
and
\begin{equation*}
E_{0 \, \textrm{blob}} = \sqrt{E_{0 \mkern.5mu h}^2 + 2 \gamma E_{0 \mkern.5mu h} E_{0j} + E_{0j}^2}.
\end{equation*}
This solution is also a nice illustration of the non-additivity of rest energy (mass): $E_{0 \, \textrm{blob}} \neq E_{0 \mkern.5mu h} + E_{0j}$, except in the classical limit when the approximation $\gamma \approx 1$ suffices. Note that $E_{0 \, \textrm{blob}}$ was equal to the rest energy of the closed \emph{system} the whole time; it's just that before the collision, some of it came from the kinetic energy of particles $h$ and $j$.



\subsection[Orthogonality, ("Normalized") Four-Acceleration]{Orthogonality, (``Normalized") Four-Acceleration}

So we have the four-displacement $\dd \vv R$, the normalized four-velocity $\vv B$, the four-momentum $\vv P$, and the four-force $\vv F$. Question: in what direction do they point? We began by \emph{defining} $\dd \vv R$ as tangent to a traveler's world line, pointing in whatever spacetime direction the traveler happens to be going. Common sense says that the same is true of the traveler's $\vv B$ and $\vv P$, but let's take a moment to think about what that means mathematically. Two vectors are codirectional if one is obtained by scaling the other by a positive number. We know that $\vv B$ and $\vv P$ are codirectional, since the latter is simply the former scaled by the traveler's $E_0$. Can we make a similar argument for $\dd \vv R$ and $\vv B$, and say that the latter is simply the former ``scaled" by the infinitesimal $1/(c \, \dd t_0)$? We can, but we need to be careful---we're \emph{not} saying that $\vv B$ is codirectional with $\vv R$! We're ``scaling" $\dd \vv R$, but \emph{differentiating} $\vv R$.

With that in mind, consider what happens when we take the $c t_0$-derivative of the four-velocity $\vv B$ to find the (``normalized") \textbf{four-acceleration}, $\vv Z$, analogous to the ``normalized" three-acceleration $\vvzeta=(\dd \vvbeta / \dd t)/c$:\footnote{Like $\vvzeta$, $\vv Z$ isn't \emph{really} normalized. It inherits the label (with scare quotes) so that we can distinguish it from the ``unnormalized" four-acceleration $\vv A = \dd \vv V / \dd t_0 = \vv Z c^2$.}
\begin{equation}\label{eq:34}
\boxed{
\begin{aligned}
\vv Z &\equiv \mathring{\vv B} \\[2pt]
&= \big \langle \mathring{\gamma}, \mathring{\vvomega} \big \rangle \\[3pt]
&= \gamma \mkern1.5mu \big \langle \dot{\gamma}, \dot{\vvomega} \big \rangle = \gamma \mkern1.5mu \big \langle \dot{\gamma}, \, \gamma \vvzeta + \dot{\gamma} \vvbeta \big \rangle
\end{aligned}
} \, ,
\end{equation}
and it's apparent that $\vv Z$ and $\vv B$ are \emph{not} codirectional. In fact, they're orthogonal! It must be so: $\vv B$ is always a unit vector, so only its direction can change.\footnote{If you don't see why constant $B$ means that $\vv B$ and $\vv Z$ are orthogonal, then think of the velocity and acceleration three-vectors in the familiar case of uniform circular motion.} And we can easily confirm it, because if two vectors are orthogonal then their dot product is zero. To test that, let's start by evaluating the $ct$-derivative of $\gamma$ (remembering that $\beta^2$ is actually $\vvbeta \cdot \vvbeta$):
\begin{equation}\label{eq:gd}
\begin{split}
\dot{\gamma} &= \dfrac{\dd}{\dd t} \left[ \left(1-\beta^2 \right)^{-1/2} \right] \dfrac{1}{c} \\[5pt]
&= \dfrac{\dd}{\dd ( \beta^2 )} \left[ \left( 1 - \beta^2 \right)^{-1/2} \right] \dfrac{\dd (\beta^2)}{\dd t} \, \dfrac{1}{c}  \\[5pt]
&= \frac{1}{2} \left( 1 - \beta ^2 \right)^{-3/2} \, \dfrac{\dd}{\dd t} \bigl( \vvbeta \cdot \vvbeta \bigr) \dfrac{1}{c} \\[5pt]
&= \frac{\gamma^3}{2} \left[ ( \vvbeta \cdot \dot{\vvbeta} ) + ( \dot{\vvbeta} \cdot \vvbeta ) \right] \\[5pt]
\dot{\gamma} &= \gamma^3 (\vvbeta \cdot \vvzeta).
\end{split}
\end{equation}
Then we sub that result into Equation \ref{eq:34}:
\begin{equation}\label{eq:35}
\vv Z =\gamma \mkern1.5mu \big \langle \gamma^3 (\vvbeta \cdot \vvzeta), \, \gamma \vvzeta + \gamma^3 (\vvbeta \cdot \vvzeta) \vvbeta \big \rangle.
\end{equation}
And now we note that in a traveler's instantaneous rest frame, the three-velocity $\vvbeta = \vv 0$, which means that the spatial components of $\vv B$ are zero (Equation \ref{eq:31}) and the time component of $\vv Z$ is zero (Equation \ref{eq:35}). So the (invariant!) dot product $\vv B \cdot \vv Z$ is $(B^{ct})(0) - (0)(Z^x) - (0)(Z^y) - (0)(Z^z) = 0$. QED.\footnote{It's standard practice to use superscript for the components of a four-vector.}

Confession: we could have demonstrated that orthogonality more quickly without using the components of $\vv B$ and $\vv Z$ at all:
\begin{equation*}
\begin{split}
\dfrac{1}{c} \, \dfrac{\dd}{\dd t_0} \left( \vv B \cdot \vv B \right) &= \dfrac{1}{c} \, \dfrac{\dd}{\dd t_0} \, B^2 \\[2pt]
2 \, \big( \mathring{\vv B} \cdot \vv B \big) &= \mathring{1} \\[3pt]
\vv Z \cdot \vv B &= 0 .
\end{split}
\end{equation*}
But our circuitous route was a good excuse to derive some useful equations.


\subsection{Proper Acceleration}\label{ssec:pa}

Equation \ref{eq:35} isn't quite so messy when the three-acceleration is parallel to the three-velocity. The time component doesn't look much different: ${Z^{ct} = \gamma^4 (\vvbeta \cdot \vvzeta) = \pm \, \gamma^4 \beta \zeta}$ (positive if $\vvbeta$ and $\vvzeta$ are codirectional, negative if they're contradirectional). The spatial component, however, gets much cleaner. Recognizing that parallel unit vectors can differ only by sign, we use $\vvzeta = \pm \, \zeta \hatbeta$ (where $\hatbeta = \vvbeta / \beta$ is the unit vector corresponding to velocity):
\begin{equation*}
\begin{split}
\gamma^2 \vvzeta + \gamma^4 (\vvbeta \cdot \vvzeta)\vvbeta &= \pm \, \gamma^2 \zeta \hatbeta + ( \pm \, \gamma^4 \beta \zeta) \beta \hatbeta \\
&= \pm \, \gamma^4 \zeta \hatbeta \left(\gamma^{-2} + \beta^2 \right)\\
&= \gamma^4 \vvzeta
\end{split}
\end{equation*}
(by Equation \ref{eq:20}), and so:
\begin{equation}\label{eq:fa}
\vv Z = \gamma^4 \langle \pm \, \beta \zeta, \vvzeta \rangle \quad \textrm{\footnotesize{ (for $\vvbeta \parallel \vvzeta$)}}.
\end{equation}
Then the magnitude of the four-acceleration is:
\begin{equation}\label{eq:36}
\begin{split}
Z &= \sqrt{\gamma^8 \beta^2 \zeta^2 - \gamma^8 \zeta^2} \\[2pt]
&= \gamma^3 \zeta \, \sqrt{\gamma^2 (\beta^2 - 1)} \\[2pt]
&= \gamma^3 \zeta \, \sqrt{\frac{\beta^2 - 1}{1 - \beta^2}} \\[2pt]
Z &= \gamma^3 \zeta \mkern1mu \mathrm{i} \quad \textrm{\footnotesize{ (for $\vvbeta \parallel \vvzeta$)}}.
\end{split}
\end{equation}
Ignore the $\mathrm{i}$ for now. Since $Z$ is Lorentz-invariant (it's the magnitude of a four-vector) and reduces to $\zeta$ in each of the traveler's successive instantaneous rest frames (where $\gamma = 1$), it must be true that Equation \ref{eq:36} allows \emph{all} qualifying inertial observers (those for whom the traveler's $\vvbeta \parallel \vvzeta$) to calculate the magnitude of acceleration measured in the traveler's frame by using their \emph{own} coordinate measurements of the traveler's $\gamma^3 \zeta$. For this reason, we call $Z / \mathrm{i}$ the magnitude of the (``normalized") \textbf{proper acceleration}. Proper acceleration is the three-acceleration a traveler actually experiences and measures with an on-board accelerometer. It's the traveler's acceleration as measured in the traveler's instantaneous rest frame. In keeping with our practice with other ``proper" quantities, we'll use the naught subscript for this one: $\vvzeta_{\mkern1mu 0}$.\footnote{``Unnormalized" proper acceleration is $\vv a_0 = \vvzeta_{\mkern1mu 0} c^2$.} Equation \ref{eq:36} tells us that when $\vvbeta \parallel \vvzeta$, the (``normalized") proper acceleration is $\vvzeta_{\mkern1mu 0} = \gamma^3 \vvzeta$, and its invariant magnitude is $\zeta_0 = \gamma^3 \zeta$ (where, again, $\gamma$ and $\zeta$ are coordinate measurements made by a qualifying inertial observer).

For \emph{arbitrary} angle $\theta$ between $\vvbeta$ and $\vvzeta$, computing $Z$ (and thus $\zeta_0$, we'll see) is more tedious, but not difficult. From Equation \ref{eq:35} (and Equation \ref{eq:20}):
\begin{equation}\label{eq:pa2}
\begin{split}
Z &= \sqrt{\vv Z \cdot \vv Z} = \sqrt{ \left[ \gamma^4 (\vvbeta \cdot \vvzeta) \right]^2 - \left[ \gamma^2 \vvzeta + \gamma^4 (\vvbeta \cdot \vvzeta) \vvbeta \right]^2}\\[2pt]
&= \sqrt{\gamma^8(\vvbeta \cdot \vvzeta)^2 - \gamma^4 \zeta^2 - 2\gamma^6 (\vvbeta \cdot \vvzeta)^2 - \gamma^8 (\vvbeta \cdot \vvzeta)^2 \beta^2 }\\[2pt]
&= \sqrt{(\vvbeta \cdot \vvzeta)^2 \Big[ \gamma^8 (1 - \beta^2) - 2\gamma^6 \Big] - \gamma^4 \zeta^2} \\[2pt]
Z &= \mathrm{i} \, \sqrt{\gamma^6 (\vvbeta \cdot \vvzeta)^2 + \gamma^4 \zeta^2} .
\end{split}
\end{equation}
That's one way to write it. Let's find a more intuitive way, starting like this:\footnote{The derivation that follows is a variation of an answer on Stack Exchange contributed by the user Pulsar: \url{https://physics.stackexchange.com/questions/66839/relativistic-factor-between-coordinate-acceleration-and-proper-acceleration/66853\#66853}.}
\begin{equation*}
Z = \gamma^3 \zeta \mkern1mu \mathrm{i} \, \sqrt{\frac{(\vvbeta \cdot \vvzeta)^2}{\zeta^2} + \frac{1}{\gamma^2}}.
\end{equation*}
Looks awkward, but we can make good sense of that with a little trigonometry. By the geometric definition of the Euclidean dot product, we know that $\vvbeta \cdot \vvzeta = \beta \zeta \cos{\theta}$, so we can substitute $\beta^2 \cos^2{\theta}$ for $(\vvbeta \cdot \vvzeta)^2 / \zeta^2$, and we have:
\begin{equation*}
\begin{split}
Z &= \gamma^3 \zeta \mkern1mu \mathrm{i} \, \sqrt{\beta^2 \cos^2{\theta} + 1 - \beta^2}\\
&= \gamma^3 \zeta \mkern1mu \mathrm{i} \, \sqrt{\beta^2 (\cos^2{\theta} - 1) + 1}\\
&= \gamma^3 \zeta \mkern1mu \mathrm{i} \, \sqrt{1 - (\beta \sin{\theta})^2}.
\end{split}
\end{equation*}
Since $(\beta \sin{\theta})$ is just the component of $\vvbeta$ that's perpendicular to $\vvzeta$ ($\beta_{\perp \vvzeta}$):
\begin{equation*}
Z = \gamma^3 \zeta \mkern1mu \mathrm{i} \, \sqrt{1 - \beta_{\perp \vvzeta}^2}.
\end{equation*}
And if we define $\gamma_{\perp} \equiv (1 - \beta_{\perp \vvzeta}^2)^{-1/2}$:
\begin{equation}\label{eq:37}
Z = \frac{\gamma^3 \zeta}{\gamma_{\perp}} \, \mathrm{i}.
\end{equation}
That's $\zeta \mkern1mu \mathrm{i}$ in the traveler's instantaneous rest frame, so we can now say that the (``normalized") proper acceleration for arbitrary $\theta$ is:
\begin{equation}\label{eq:pa4}
\vvzeta_{\mkern1mu 0} = \dfrac{Z}{\mathrm{i}} \, \hatzeta ,
\end{equation}
with invariant magnitude $\zeta_0 = [ \gamma^6 (\vvbeta \cdot \vvzeta)^2 + \gamma^4 \zeta^2 ]^{-1/2} = \gamma^3 \zeta / \gamma_{\perp}$ (by Equations \ref{eq:pa2} and \ref{eq:37}). When $\vvbeta \parallel \vvzeta$, Equation \ref{eq:37} reduces to Equation \ref{eq:36} (because then $\gamma_{\perp} = 1$). Another important special case is when $\vvbeta \perp \vvzeta$, as in uniform circular motion. Then $\gamma_{\perp} = \gamma$, and we get $\vvzeta_{\mkern1mu 0} = \gamma^2 \vvzeta$.


\subsection{Classifying Four-Vectors}\label{ssec:cf}

We see now that the magnitude $Z$ of the four-acceleration \emph{must} contain the imaginary unit. Why $\mathrm{i}$? Because four-acceleration is orthogonal to four-velocity, and $\mathrm{i}$ \emph{never} appears in the magnitude of four-velocity.

That warrants some fleshing out.

The $\mathrm{i}$ is physically meaningless, an artifact of the ($+$$-$$-$$-$) sign convention we've adopted (see Footnote \ref{fn:sc}). What's physically meaningful is that four-acceleration has something in common with spacelike intervals---namely, the sign of its squared magnitude (negative for us). Likewise, the squared magnitude of the four-velocity (Equation \ref{eq:31}) is \emph{positive} under our sign convention, which is the distinguishing characteristic of \emph{timelike} intervals. And the squared magnitude of the four-displacement (Equation \ref{eq:30}) or the four-momentum (Equation \ref{eq:32}) can be either positive ($\beta < 1$) or zero ($\beta = 1$), like a timelike or lightlike interval.\footnote{The magnitude of a four-displacement \emph{is} a spacetime interval! More broadly, the magnitude of a four-\emph{separation} is a spacetime interval (times $\mathrm{i}$ if the interval is spacelike)---see Footnote \ref{fn:sp}.} A way of classifying four-vectors suggests itself:
\begin{itemize}
\item A four-vector is spacelike if the magnitude of its spatial three-vector component is greater than the absolute value of its time component.
\item A four-vector is timelike if the opposite is true.
\item A four-vector is lightlike (null) if the magnitude of its spatial three-vector component is equal to the absolute value of its time component.
\end{itemize}
Here are some rules about four-vector orthogonality:
\begin{itemize}
\item The zero four-vector \mbox{\boldmath $\emptyset$} is the lightlike vector with zeros for components, orthogonal to all four-vectors (dot product with it is always zero).
\item A non-zero four-vector that's orthogonal to a timelike vector is necessarily spacelike. This is easily seen by choosing the frame in which the timelike vector has zeros for its \emph{spatial} components (such a frame exists for every timelike vector); any second vector that could yield zero when dotted with the first must have a zero \emph{time} component in that frame. That's why the four-acceleration has an imaginary magnitude---it's spacelike because it's orthogonal to the timelike four-velocity, and spacelike vectors have imaginary magnitudes under our sign convention.
\item A non-zero four-vector that's orthogonal to a spacelike vector can be timelike (see above), but also spacelike or lightlike. For instance, the spacelike vectors $\langle 0, 1, 0, 0 \rangle$ and $\langle 0, 0, 1, 0 \rangle$ are orthogonal, and the lightlike vector $\langle 1, 0, 0, 1 \rangle$ is orthogonal to both of them.
\item A non-zero four-vector that's orthogonal to a non-zero lightlike vector must be either spacelike (see above) or lightlike (e.g., $\langle 1, 0, 0, 1 \rangle$ and $\langle 2, 0, 0, 2 \rangle$).
\end{itemize}

What about the four-force (Equation \ref{eq:33})? Spacelike, timelike, or lightlike? In the special case that $E_0$ changes but $\vv B$ doesn't (as with the light-emitting body in Einstein's thought experiment), $\vv F$ is timelike because it's parallel to the timelike $\vv B$:
\begin{equation}\label{eq:38}
\begin{split}
\vv F &= \mathring{\vv P}\\
&= \frac{\dd}{\dd t_0} \left( E_0 \mkern.5mu \vv B \right) \frac{1}{c}\\[5pt]
\vv F &= \mathring E_0 \mkern.5mu \vv B \quad \textrm{\footnotesize{ (for $\vv Z = \textrm{\mbox{\boldmath $\emptyset$}}$)}}.
\end{split}
\end{equation}
But in most applications it's $E_0$ that remains constant while $\vv B$ changes. In that case $\vv F$ is spacelike (codirectional with $\mathring{\vv B} = \vv Z$, orthogonal to $\vv B$):
\begin{equation}\label{eq:39}
\vv F = E_0 \mkern.5mu \vv Z \quad \textrm{\footnotesize{ (for $\mathring{E}_0 = 0$)}}.
\end{equation}
This says symbolically what we've already put into words: rest energy is the measure of ``spacetime inertia," the resistance to change in (direction of) four-velocity $\vv B$.\footnote{Equivalently, $\vv F = m \vv A$ when $\mathring{m} = 0$ (since $E_0 = mc^2$ and $\vv Z = \vv A / c^2$).}

If \emph{both} $E_0$ and $\vv B$ change (the general case), then the product rule gives us $\vv F = \mathring E_0 \mkern.5mu \vv B + E_0 \mkern.5mu \vv Z$. Such a four-force might be spacelike, timelike, or even lightlike, depending on whether the magnitude of the spatial component (three-force) is greater than, less than, or equal to the absolute value of the time component (power).

\subsection{Proper Power, Proper Force}\label{ssec:pf}

When we discussed the four-force in Section \ref{ssec:ff}, we didn't bother finding its magnitude $F$. We could have ``plug-and-chugged" with the temporal and spatial components (like we later did to calculate the four-acceleration's magnitude), but that would have been a chore. Now that we've covered the four-acceleration $\vv Z$ and the proper acceleration $\vvzeta_{\mkern1mu 0}$ (with invariant magnitude $Z/\mathrm{i}$), we have a better option:
\begin{equation}\label{eq:40}
\begin{split}
F &= \sqrt{\vv F \cdot \vv F} = \sqrt{\mathring{\vv P} \cdot \mathring{\vv P}} \\[2pt]
&= \sqrt{ \left( \mathring{E}_0 \mkern.5mu \vv B + E_0 \mkern.5mu \vv Z \right) \cdot \left( \mathring{E}_0 \mkern.5mu \vv B + E_0 \mkern.5mu \vv Z \right) }\\[2pt]
&= \sqrt{ \mathring{E}_0^2 \left( \vv B \cdot \vv B \right) + 2 E_0 \mathring{E}_0 \left( \vv B \cdot \vv Z \right) + E_0^2 \left( \vv Z \cdot \vv Z \right) } \\[2pt]
F &= \sqrt{ \mathring{E}_0^2 - \left( E_0 \vvzeta_{\mkern1mu 0} \right)^2 }
\end{split}
\end{equation}
because $\vv B$ and $\vv Z$ are orthogonal, $\vv B \cdot \vv B = B^2 = 1$, and $Z^2 = - (\vvzeta_{\mkern1mu 0} \cdot \vvzeta_{\mkern1mu 0})$.\footnote{Because the four-force concept is inapplicable to restless systems (see Section \ref{ssec:ff}), we don't lose generality by working with the rest energy. Whether the concepts of power and three-force are likewise inapplicable to restless systems is a thornier question. The simple answer is yes, they're inapplicable. A more complete answer would wrestle with optical phenomena like reflection and refraction. For instance, should we regard a reflected light wave as the same system before and after reflection? (Probably not.) And does it even make sense to label an electromagnetic wave ``restless" when it's traveling at $\beta < 1$ through a medium? (Probably not, and such a wave is better described as a \emph{polarization} wave.) There's also gravitational redshift/blueshift and gravitational lensing, but these are topics for general relativity, and it would be misleading to naively use the words \emph{power} and \emph{force} here.}

All right, but that four-vector magnitude is less straightforward than the others we've seen. What does it tell us? First, note that it accords with Equations \ref{eq:38} and \ref{eq:39}: when $\vv B$ is constant ($\vvzeta_{\mkern1mu 0} = \vv 0$), $\vv F = \mathring{E}_0 \vv B$ and $F = \mathring{E}_0 B = \mathring{E}_0$; when $E_0$ is constant, $\vv F = E_0 \vv Z$ and $F = E_0 Z = E_0 \mkern.5mu \zeta_0 \mkern1mu \mathrm{i}$. And what \emph{are} these quantities $\mathring{E}_0$ and $E_0 \mkern.5mu \zeta_0$? They're certainly both invariant---the first is the derivative of the invariant rest energy with respect to the invariant $c t_0$, and the second is the invariant rest energy times the invariant magnitude of the proper acceleration.\footnote{This is a good time to remind ourselves of the caveats we mentioned at the beginning of Section \ref{ssec:ff}. First, we're still modeling any accelerating system as a point particle. Second, the relativity of simultaneity means that for an open composite system, it's not always possible to speak of the rest energy (or its proper-time derivative!) as ``invariant."} But they also represent the power and three-force (magnitude) that the traveler ``feels," or that an observer in any of the traveler's successive instantaneous rest frames measures. Why? For power:
\begin{equation}\label{eq:po}
\begin{split}
\mathcal{P} &= \dot{E}\\
&= \dot{\gamma} E_0 + \gamma \dot{E}_0\\
\mathcal{P} &= \gamma^3 (\vvbeta \cdot \vvzeta) E_0 + \mathring{E}_0
\end{split}
\end{equation}
(by Equation \ref{eq:gd}), where only the second term remains in the traveler's instantaneous rest frame ($\vvbeta = \vv 0$).\footnote{Same argument could be made for Equation \ref{eq:rp}, which is equivalent to Equation \ref{eq:po}.} For three-force:
\begin{equation}\label{eq:41}
\begin{split}
\vv f &= \dot{\vv p} c\\
&= \dot{\gamma} E_0 \vvbeta + \gamma \dot{E}_0 \vvbeta + \gamma E_0 \dot{\vvbeta}\\
\vv f &=\gamma^3 (\vvbeta \cdot \vvzeta) E_0 \vvbeta + \mathring{E}_0 \vvbeta + E \vvzeta,
\end{split}
\end{equation}
where, again, only the last term remains in the traveler's instantaneous rest frame, but its magnitude there is $E_0 \mkern.5mu \zeta_0$, our desired invariant.

So inertial observers can use their own coordinate measurements of the traveler's $\mathring{E}_0 = \gamma \dot{E}_0$ and $E_0 \mkern.5mu \zeta_0 = E_0 \gamma^3 \zeta / \gamma_{\perp}$ (Equation \ref{eq:37}) to calculate the traveler's power and three-force (magnitude) as measured in the traveler's instantaneous rest frame. These are therefore ``proper" quantities: the \textbf{proper power} $\mathcal{P}_0 \equiv \mathring{E}_0$, and the \textbf{proper force} $\vv f_0 \equiv E_0 \vvzeta_{\mkern1mu 0}$ ($= [mc^2][\vv a_0 / c^2] = m \vv a_0$).\footnote{Beware that another convention defines proper power and proper force as something else entirely---$\dd E / \dd t_0$ and $\dd \vv p / \dd t_0$, in keeping with defining proper velocity ``improperly" as $\dd \vv r / \dd t_0$ (see Footnote \ref{fn:pr} in Section \ref{ssec:ce}).} Equation \ref{eq:40} can now be expressed more intelligibly:
\begin{equation}\label{eq:42}
F = \sqrt{\mathcal{P}_0^2 - (\vv f_0 \cdot \vv f_0)} .
\end{equation}

Now, about this proper force: we've got $\vv f_0 = E_0 \vvzeta_{\mkern1mu 0}$, which in the traveler's instantaneous rest frame is simply $\vv f = E_0 \vvzeta$ ($= m \vv a$). Is the point of all this merely to restate the mundane fact that the relativistic three-force reduces to the Newtonian three-force in the classical limit? No! The point is that the magnitude $f_0 = E_0 \mkern.5mu \zeta_0$ ($= m a_0$) is an invariant whose value is calculable by all inertial observers. In $\vv f_0 = E_0 \vvzeta_{\mkern1mu 0}$ $(= m \vv a_0)$, then, we have something like a relativistic version of Newton's $\vv f_{\mathrm{classical}} = m \vv a$, but valid even when $\dot{E}_0 \neq 0$ ($\dot{m} \neq 0$).\footnote{In principle, the concept of proper force is applicable to Newtonian mechanics, too, provided one has defined classical force as the time-derivative of momentum (rather than as $m \vv a$---see Footnote \ref{fn:cf}). But it could only be useful in a variable-mass situation. That's because acceleration-magnitude is invariant in the classical limit. Proper acceleration differs appreciably from ordinary coordinate acceleration only when $\beta \centernot{\ll} 1$.}


\section["Measure of Inertia" Revisited]{``Measure of Inertia" Revisited}\label{sec:in}

We see in Equation \ref{eq:41} that $\vv f \neq E_0 \vvzeta$ $( = m \vv a)$, even when $\dot{E}_0 = 0$ ($\dot{m} = 0$ being the condition under which $\vv f_\textrm{classical} = m \vv a$ is true). That's not surprising; back in Section \ref{ssec:rm} we ruled out the possibility that $E_0$ is the resistance to change in three-velocity for arbitrary $\vvbeta$. More interesting is that our \emph{candidate} for the relativistic measure of inertia, total energy $E$, likewise cannot be unambiguously described as the resistance to change in velocity under the influence of a three-force. Yes, the last term in Equation \ref{eq:41} is $E \vvzeta$, and the middle term disappears when $\dot{E}_0 = 0$, but there's still that pesky first term that doesn't vanish unless $\vvbeta = \vv 0$:
\begin{equation}\label{eq:43}
\vv f = \gamma^2 E \mkern1mu (\vvbeta \cdot \vvzeta) \vvbeta + E \vvzeta \quad \textrm{\footnotesize{ (for $\dot{E}_0 = 0$)}}.
\end{equation}

To gain a bit more insight into the relationship between force and acceleration, let's isolate $\vvzeta$ on one side of the equation so that we can compare the result with the Newtonian $\vvzeta = \vv f_{\mathrm{classical}} / E_0$. First we'll need to factor out that $(\vvbeta \cdot \vvzeta)$ on the right side, which we accomplish by taking the dot product of Equation \ref{eq:43} with the normalized three-velocity:
\begin{equation*}
\begin{split}
\vv f \cdot \vvbeta &= \left( \gamma^2 E \mkern1mu (\vvbeta \cdot \vvzeta) \vvbeta + E \vvzeta \right) \cdot \vvbeta \\
&= \gamma^2 E \mkern1mu (\vvbeta \cdot \vvzeta)(\beta^2 + \gamma^{-2}) \\
&= \gamma^2 E \mkern1mu (\vvbeta \cdot \vvzeta)
\end{split}
\end{equation*}
(by Equation \ref{eq:20}). Subbing that back into Equation \ref{eq:43}, we find:
\begin{equation}\label{eq:44}
\begin{split}
\vv f &= (\vv f \cdot \vvbeta) \vvbeta + E \vvzeta \\[5pt]
\vvzeta &= \frac{\vv f - (\vv f \cdot \vvbeta) \vvbeta}{E} \quad \textrm{\footnotesize{ (for $\dot{E}_0 = 0$)}}.
\end{split}
\end{equation}
Total energy $E$, we see, is the resistance to change in velocity not under the influence of ``a" three-force, but rather under the influence of \emph{a particular pairing of force and velocity}. Fix the $E$ and $\vv f$, and the acceleration $\vvzeta$ still depends on the magnitude and direction of the three-velocity $\vvbeta$.

Does this mean that there is no universal ``measure of inertia"? That depends on how we define ``measure of inertia." There are at least five reasonable possibilities:
\begin{enumerate}
\item If we define it as the scalar relating three-force $\vv f$ and three-acceleration $\vvzeta$ (when $\dot{E}_0 = 0$), then no, it doesn't exist beyond the classical limit.
\item If we define it as the scalar relating three-momentum $\vv p c$ and three-velocity $\vvbeta$, then the measure of inertia is total energy $E = \gamma E_0$, a function of speed (\emph{not} a constant of proportionality!).\footnote{\label{fn:rm}Some who take this view prefer in certain contexts to express $E$ in units of mass and call it the \textbf{relativistic mass}, $m_{\textrm{r}} \equiv E / c^2$ ($= \gamma m$). That way, $\vv p = m_{\textrm{r}} \vv v$, which jibes with the classical Equation \ref{eq:1}. Some people even reserve the term \emph{mass} and the symbol $m$ for this frame-dependent quantity, and then use the term \textbf{rest mass} (or \textbf{proper mass}, or \textbf{invariant mass}) and the symbol $m_0$ to refer to the invariant scalar that we know as $m$. Under that convention, $m = E / c^2$ ($ = \gamma m_0$), and $\vv p = m \vv v$ (jibes even better). It's become unfashionable to express total energy in mass units, but the practice still has its adherents.}
\item If we define it as the constant of proportionality relating proper force $\vv f_0$ and proper acceleration $\vvzeta_{\mkern1mu 0}$, then it's rest energy $E_0$ (or mass), as it is in Newtonian mechanics.
\item If we define it as the constant of proportionality relating $\vv f$ and $\dot{\vvomega}$ (the $ct$-derivative of \emph{celerity}), then again it's $E_0$. To see this, differentiate Equation \ref{eq:29} with respect to $ct$ (for $\dot{E}_0 = 0$).
\item If we define it as the constant of proportionality relating four-force $\vv F$ and four-acceleration $\vv Z$ (or indeed relating four-momentum $\vv P$ and four-velocity $\vv B$), then yet again it's $E_0$, as we've seen, though we've called this the measure of ``\emph{spacetime} inertia" (Equation \ref{eq:39}).
\end{enumerate}

But enough semantics!

Have another look at Equation \ref{eq:44}, and note that the force and acceleration three-vectors \emph{aren't even parallel}, unless the force happens to be either perpendicular or parallel to the three-velocity $\vvbeta$. If $\vv f \perp \vvbeta$, then $(\vv f \cdot \vvbeta) \vvbeta = \vv 0$:
\begin{equation}\label{eq:45}
\vvzeta = \frac{\vv f}{E} \quad \textrm{\footnotesize{ (for $\dot{E}_0 = 0$ and $\vv f \perp \vvbeta$)}}.
\end{equation}
If $\vv f \parallel \vvbeta$, then $\vv f \cdot \vvbeta = \pm \, f \beta$, and $\vvbeta = \pm \, \beta \mkern1mu \vv{\hat{f}}$ (where $\vv{\hat{f}}$ is the unit vector $\vv f / f$):
\begin{equation*}
\begin{split}
\vv f - (\vv f \cdot \vvbeta) \vvbeta &= f \mkern1mu \vv{\hat{f}} - (\pm \, f \beta) (\pm \, \beta \mkern1mu \vv{\hat{f}}) \\[2pt]
&= \vv f \mkern1mu (1 - \beta^2) \\[2pt]
&= \frac{\vv f}{\gamma^2},
\end{split}
\end{equation*}
such that
\begin{equation}\label{eq:46}
\vvzeta = \frac{\vv f}{\gamma^2 E} \quad \textrm{\footnotesize{ (for $\dot{E}_0 = 0$ and $\vv f \parallel \vvbeta$)}}.
\end{equation}
Recall from Equation \ref{eq:36} that the proper acceleration $\vvzeta_{\mkern1mu 0}$ is $\gamma^3 \vvzeta$ when $\vvzeta \parallel \vvbeta$, a condition that holds here. So Equation \ref{eq:46} gives:
\begin{equation*}
\vv f = \gamma^3 E_0 \vvzeta = E_0 \vvzeta_{\mkern1mu 0} = \vv f_0 ,
\end{equation*}
meaning that when $\dot{E}_0 = 0$, the magnitude of three-force is invariant among observers for whom $\vv f \parallel \vvbeta$ (not all observers!).

If $\vv f$ is neither perpendicular nor parallel to $\vvbeta$, then Equations \ref{eq:45} and \ref{eq:46} point us toward the \emph{component} vectors of $\vvzeta$ relative to $\vvbeta$, and with a minor notational adjustment we can express $\vvzeta$ as their sum:
\begin{equation}\label{eq:47}
\vvzeta = \frac{\vv f_{\perp \vvbeta}}{E} + \frac{\vv f_{\parallel \vvbeta}}{\gamma^2 E} \quad \textrm{\footnotesize{ (for $\dot{E}_0 = 0$)}},
\end{equation}
where $\vv f_{\perp \vvbeta}$ and $\vv f_{\parallel \vvbeta}$ are the component vectors of $\vv f$ perpendicular and parallel to $\vvbeta$. Equation \ref{eq:47} shows us exactly how inertia depends on direction. By a factor of $\gamma^2$, an object resists acceleration along its axis of motion more than it does against it.\footnote{By $\vvzeta = \vv a / c^2$ and $E = \gamma mc^2$, Equation \ref{eq:47} is equivalently $\vv a = \vv f_{\perp \vv v} / (\gamma m) + \vv f_{\parallel \vv v} / (\gamma^3 m)$. In this context, the quantities $\gamma m$ and $\gamma^3 m$ are sometimes respectively called the \textbf{transverse mass} and the \textbf{longitudinal mass}, to emphasize the dependence of inertia on direction. Like the relativistic mass (see Footnote \ref{fn:rm}), these concepts have generally fallen out of favor. Today most physicists speak only of one mass: Newton's invariant $m$, which Einstein showed is non-additive and equal to $E_0 / c^2$.}

Finally, we can take advantage of matrix notation to relate $\vv f$ and $\vvzeta$ in a more compact and quasi-familiar way. First we rewrite Equation \ref{eq:43}:
\begin{equation*}
\begin{split}
\vv f &= \gamma^2 E \vvbeta \mkern1mu (\vvbeta \cdot \vvzeta) + E \vvzeta \\
[\vv f] &= \gamma^2 E [\vvbeta] [\vvbeta]^{\mathrm{T}} [\vvzeta] + E [\vvzeta] ,
\end{split}
\end{equation*}
where square brackets around a vector indicate its column-matrix representation, and the superscript $\mathrm{T}$ indicates the transpose (so $[\vvbeta]^{\mathrm{T}}$ is a row matrix and $[\vvbeta]^{\mathrm{T}} [\vvzeta]$ represents the dot product $\vvbeta \cdot \vvzeta$). Now, because the matrix product is distributive over matrix addition, we can simply ``factor out" $[\vvzeta]$ (but only to the right---the matrix product is \emph{not} commutative):
\begin{equation*}
[\vv f] = (\gamma^2 E [\vvbeta] [\vvbeta]^{\mathrm{T}} + E I_3) [\vvzeta]
\end{equation*}
($I_3$ is the identity matrix). What remains in the parentheses is a 3-by-3 ($[\vvbeta] [\vvbeta]^{\mathrm{T}}$ being the matrix representation of the \textbf{dyadic product} $\vvbeta \otimes \vvbeta$). Label it $\mathcal{E} \equiv E(\gamma^2 [\vvbeta] [\vvbeta]^{\mathrm{T}} + I_3)$. Then Equations \ref{eq:43} and \ref{eq:44} can be notated:
\begin{equation}\label{eq:48}
[\vv f] = \mathcal{E} [\vvzeta] \quad \textrm{\footnotesize{ (for $\dot{E}_0 = 0$)}}
\end{equation}
and
\begin{equation}\label{eq:49}
[\vvzeta] = \mathcal{E}^{-1} [\vv f] \quad \textrm{\footnotesize{ (for $\dot{E}_0 = 0$)}},
\end{equation}
where $\mathcal{E}^{-1} = (I_3 - [\vvbeta] [\vvbeta]^{\mathrm{T}})/E$ is the matrix inverse of $\mathcal{E}$.\footnote{Those who prefer to work with $m$, $\vv v$, and $\vv a$ can instead use:
\begin{equation*}
[\vv f] = \mathcal{M} [\vv a] \qquad \qquad [\vv a] = \mathcal{M}^{-1} [\vv f],
\end{equation*}
where $\mathcal{M} \equiv \mathcal{E} / c^2 = \gamma m (\gamma^2 [\vv v] [\vv v]^{\mathrm{T}}/c^2 + I_3) =  m_{\textrm{r}}(\gamma^2 [\vv v] [\vv v]^{\mathrm{T}}/c^2 + I_3)$ (see Footnote \ref{fn:rm}).} In the classical limit, $\mathcal{E} \approx E_0 I_3$, giving $[\vv f] \approx E_0 I_3 [\vvzeta] = E_0 [\vvzeta]$ $(= m [\vv a])$, as expected.

Equations \ref{eq:48} and \ref{eq:49} are nice! Perhaps the matrices $\mathcal{E}$ and $\mathcal{E}^{-1}$ are a sixth candidate for ``measure of inertia."



\section{Lorentz Transformation Revisited}

We've spent a lot of time on invariant ``proper" quantities that allow observers to figure out what's happening in a traveler's (instantaneous) rest frame. That was time well spent, but what if an observer is less interested in a traveler's rest frame than in some other observer's measurements of the traveler's observables? Or what if there isn't a traveler at all, and an observer just wants to know what the components of some four-vector are in someone else's rest frame?\footnote{Most of the four-vectors we've considered pertain directly to something that's moving, but that's not a requirement. For instance, we're free to construct a four-separation that ``connects" a pair of spacelike-separated events and whose magnitude is the spacetime interval between them (times $\mathrm{i}$)---see Footnote \ref{fn:sp}.}

We already have the answer: four-vector components obey the Lorentz transformation. Let's now focus on this for a bit. We'll stick with standard-configuration Lorentz boosts. Matrix notation served us well at the end of the previous section, and as we apply the Lorentz transformation to four-vectors we'll find it convenient to use matrix notation once again.

Naturally, we begin with the transformation of the components of instantaneous four-displacement, which is just the infinitesimal version of Equations \ref{eq:lt2} (inverse transformation on the right side):
\begin{equation}\label{eq:fd}
\begin{aligned}
c \, \dd t^{\prime} &= \gamma \left( c \, \dd t - \beta \, \dd x \right) & c \, \dd t &= \gamma \left( c \, \dd t^\prime + \beta \, \dd x^\prime \right) \\
\dd x^{\prime} &= \gamma \left( \dd x - \beta \mkern1mu c \, \dd t \right) & \qquad  \dd x &= \gamma \left( \dd x^\prime + \beta \mkern1mu c \, \dd t^\prime \right) \\
\dd y^{\prime} &= \dd y& \dd y&= \dd y^\prime \\
\dd z^{\prime} &= \dd z& \dd z&= \dd z^\prime .
\end{aligned}
\end{equation}
Each of these systems of equations can be expressed as the matrix product of a 4-by-4 matrix and a four-displacement notated as a column:
\begin{equation*}
\begin{bmatrix}
c \, \dd t^\prime \\ \dd x^\prime \\ \dd y^\prime \\ \dd z^\prime
\end{bmatrix}
=
\begin{bmatrix}
\gamma & -\omega & 0 & 0 \\
- \omega & \gamma & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
c \, \dd t \\ \dd x \\ \dd y \\ \dd z
\end{bmatrix}
\qquad
\begin{bmatrix}
c \, \dd t \\ \dd x \\ \dd y \\ \dd z
\end{bmatrix}
=
\begin{bmatrix}
\gamma & \omega & 0 & 0 \\
\omega & \gamma & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
c \, \dd t^\prime \\ \dd x^\prime \\ \dd y^\prime \\ \dd z^\prime
\end{bmatrix}
\end{equation*}
(using the celerity $\omega = \gamma \beta$ just to keep it fresh in our minds). We call the 4-by-4 on the left the \textbf{Lorentz boost matrix} (for a boost along the $x$-axis), and we give it the symbol $\Lambda$:
\begin{equation}\label{eq:bm}
\Lambda =
\begin{bmatrix}
\gamma & -\gamma \beta & 0 & 0 \\
- \gamma \beta & \gamma & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix} .
\end{equation}
The other 4-by-4 is its inverse (Equation \ref{eq:20} is helpful for verifying this), the \textbf{inverse Lorentz boost matrix}:
\begin{equation}\label{eq:im}
\Lambda^{-1} =
\begin{bmatrix}
\gamma & \gamma \beta & 0 & 0 \\
\gamma \beta & \gamma & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix} .
\end{equation}
We can now write the Lorentz transformation of the components of a four-displacement $\dd \vv R$ (and its inverse) in a single line:
\begin{equation*}
[\dd \vv R^\prime] = \Lambda \, [\dd \vv R] \qquad \qquad [\dd \vv R] = \Lambda^{-1} \, [\dd \vv R^\prime] .
\end{equation*}
Indeed, we can do the same for an arbitrary four-vector $\vv Q$:
\begin{equation*}
[\vv Q^\prime] = \Lambda \mkern1mu [\vv Q] \qquad \qquad [\vv Q] = \Lambda^{-1} \mkern1mu [\vv Q^\prime] .
\end{equation*}
As we'll see, the details of a transformation can be an eyesore, so these one-line matrix equations are a fine thing.

Care is required when the four-vector's components contain speeds or Lorentz factors (e.g., four-velocity and four-acceleration). We mustn't confuse these $\beta$'s and $\gamma$'s with the $\beta$'s and $\gamma$'s in the boost matrices. Our solution, remember, is to add a ``rel" subscript to the $\beta$'s and $\gamma$'s that correspond to the relative motion of the primed and unprimed frames---that is, we notate the boost parameters $\beta_{\mathrm{rel}}$ and $\gamma_{\mathrm{rel}}$.

\subsection{Acceleration Transformed}\label{ssec:at}

We've already explicitly written out the Lorentz transformation for the components of four-displacement (Equations \ref{eq:lt2} and \ref{eq:fd}), four-velocity (Equations \ref{eq:fv}), and four-momentum (Equations \ref{eq:fm}). Let's go ahead and do the same for the components of four-acceleration (Equation \ref{eq:34}).
\begin{equation*}
[\vv Z^\prime] = \Lambda \mkern1mu [\vv Z] \qquad \qquad [\vv Z] = \Lambda^{-1} \mkern1mu [\vv Z^\prime] ,
\end{equation*}
i.e.,
\begin{equation*}
\begin{bmatrix}
Z^{\prime \mkern1.5mu ct} \\[.15 em]
Z^{\prime \mkern1.5mu x} \\[.15 em]
Z^{\prime \mkern1.5mu y} \\[.15 em]
Z^{\prime \mkern1.5mu z}
\end{bmatrix}
=
\begin{bmatrix}
\gamma_{\textrm{rel}} & -\gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & 0 & 0 \\[.15 em]
- \gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & \gamma_{\textrm{rel}} & 0 & 0 \\[.15 em]
0 & 0 & 1 & 0 \\[.15 em]
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
Z^{ct} \\[.15 em]
Z^{x} \\[.15 em]
Z^{y} \\[.15 em]
Z^{z}
\end{bmatrix}
\end{equation*}
and
\begin{equation*}
\begin{bmatrix}
Z^{ct} \\[.15 em]
Z^{x} \\[.15 em]
Z^{y} \\[.15 em]
Z^{z}
\end{bmatrix}
=
\begin{bmatrix}
\gamma_{\textrm{rel}} & \gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & 0 & 0 \\[.15 em]
\gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & \gamma_{\textrm{rel}} & 0 & 0 \\[.15 em]
0 & 0 & 1 & 0 \\[.15 em]
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
Z^{\prime \mkern1.5mu ct} \\[.15 em]
Z^{\prime \mkern1.5mu x} \\[.15 em]
Z^{\prime \mkern1.5mu y} \\[.15 em]
Z^{\prime \mkern1.5mu z}
\end{bmatrix}
\end{equation*}
(we use superscript for four-vector components, recall). Equivalently:
\begin{equation*}
\begin{bmatrix}
\gamma^\prime \dot{\gamma}^\prime \\[.25 em]
\gamma^{\prime} \dot{\omega}_x^{\prime} \\[.25 em]
\gamma^{\prime} \dot{\omega}_y^{\prime} \\[.25 em]
\gamma^{\prime} \dot{\omega}_z^{\prime} \\[.1 em]
\end{bmatrix}
=
\begin{bmatrix}
\gamma_{\textrm{rel}} & -\gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & 0 & 0 \\[.25 em]
- \gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & \gamma_{\textrm{rel}} & 0 & 0 \\[.25 em]
0 & 0 & 1 & 0 \\[.25 em]
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
\gamma \dot{\gamma} \\[.25 em]
\gamma \dot{\omega}_x \\[.25 em]
\gamma \dot{\omega}_y \\[.25 em]
\gamma \dot{\omega}_z \\[.1 em]
\end{bmatrix}
\end{equation*}
and
\begin{equation*}
\begin{bmatrix}
\gamma \dot{\gamma} \\[.25 em]
\gamma \dot{\omega}_x \\[.25 em]
\gamma \dot{\omega}_y \\[.25 em]
\gamma \dot{\omega}_z \\[.1 em]
\end{bmatrix}
=
\begin{bmatrix}
\gamma_{\textrm{rel}} & \gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & 0 & 0 \\[.25em]
\gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & \gamma_{\textrm{rel}} & 0 & 0 \\[.25 em]
0 & 0 & 1 & 0 \\[.25 em]
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
\gamma^\prime \dot{\gamma}^\prime \\[.25 em]
\gamma^{\prime} \dot{\omega}_x^{\prime} \\[.25 em]
\gamma^{\prime} \dot{\omega}_y^{\prime} \\[.25 em]
\gamma^{\prime} \dot{\omega}_z^{\prime} \\[.1 em]
\end{bmatrix}
\end{equation*}
(where a dot over anything primed signifies its $ct^\prime$-derivative). Dropping the matrix notation and ``breaking up" the celerity components into Lorentz factors and velocity/acceleration components (remember: $\vvzeta = \dot{\vvbeta}$), that's:
\begin{equation*}
\begin{aligned}
\gamma^\prime \dot{\gamma}^\prime &= \gamma_{\textrm{rel}} \mkern1mu \gamma \left[ \dot{\gamma} - \beta_{\textrm{rel}} \bigl( \gamma \zeta_x + \dot{\gamma} \beta_x \bigr) \right] \\
\gamma^\prime \left( \gamma^{\prime} \zeta^{\prime}_x + \dot{\gamma}^\prime \beta_x^\prime \right) &= \gamma_{\textrm{rel}} \mkern1mu \gamma \left( \gamma \zeta_x + \dot{\gamma} \beta_x - \beta_{\textrm{rel}} \mkern1.mu \dot{\gamma} \right) \\
\gamma^\prime \left( \gamma^{\prime} \zeta^{\prime}_y + \dot{\gamma}^\prime \beta_y^\prime \right) &= \gamma \left( \gamma \zeta_y + \dot{\gamma} \beta_y \right) \\
\gamma^\prime \left( \gamma^{\prime} \zeta^{\prime}_z + \dot{\gamma}^\prime \beta_z^\prime \right) &= \gamma \left( \gamma \zeta_z +  \dot{\gamma} \beta_z \right)
\end{aligned}
\end{equation*}
and
\begin{equation*}
\begin{aligned}
\gamma \dot{\gamma} &= \gamma_{\textrm{rel}} \mkern1mu \gamma^\prime \left[ \dot{\gamma}^\prime + \beta_{\textrm{rel}} \bigl( \gamma^{\prime} \zeta^\prime_x +  \dot{\gamma}^\prime \beta^\prime_x \bigr) \right] \\
\gamma \left( \gamma \zeta_x + \dot{\gamma} \beta_x \right) &= \gamma_{\textrm{rel}} \mkern1mu \gamma^\prime \left( \gamma^{\prime} \zeta^\prime_x + \dot{\gamma}^\prime \beta^\prime_x + \beta_{\textrm{rel}} \mkern1.mu \dot{\gamma}^\prime \right) \\
\gamma \left( \gamma \zeta_y + \dot{\gamma} \beta_y \right) &= \gamma^\prime \left( \gamma^{\prime} \zeta^{\prime}_y + \dot{\gamma}^\prime \beta_y^\prime \right) \\
\gamma \left( \gamma \zeta_z +  \dot{\gamma} \beta_z \right) &= \gamma^\prime \left( \gamma^{\prime} \zeta^{\prime}_z + \dot{\gamma}^\prime \beta_z^\prime \right) .
\end{aligned}
\end{equation*}
Not pretty! Some torturous algebra (using Equations \ref{eq:fv} for substitutions as necessary) leads to the transformation of the Lorentz factor's time-derivative and of the three-acceleration, which we could also obtain by taking the $ct$- ($ct^\prime$-) derivatives of Equations \ref{eq:vt}:
\begin{equation}\label{eq:at}
\begin{aligned}
\dot{\gamma}^\prime &= \dfrac{\dot{\gamma} - \dot{\omega}_x \mkern1mu \beta_{\textrm{rel}} }{1 - \beta_x \mkern1mu \beta_{\textrm{rel}}} \\[3pt]
\zeta_x^\prime &= \dfrac{\zeta_x}{\gamma_{\textrm{rel}}^3 \left( 1 - \beta_x \mkern1mu \beta_{\textrm{rel}} \right)^3 } \\[3pt]
\zeta_y^\prime &= \dfrac{\zeta_y}{\gamma_{\textrm{rel}}^2 \left( 1 - \beta_x \mkern1mu \beta_{\textrm{rel}} \right)^2 } + \dfrac{\beta_y \mkern1mu \beta_{\textrm{rel}} \mkern1mu \zeta_x}{\gamma_{\textrm{rel}}^2 \left( 1 - \beta_x \mkern1mu \beta_{\textrm{rel}} \right)^3} \\[4pt]
\zeta_z^\prime &= \dfrac{\zeta_z}{\gamma_{\textrm{rel}}^2 \left( 1 - \beta_x \mkern1mu \beta_{\textrm{rel}} \right)^2 } + \dfrac{\beta_z \mkern1mu \beta_{\textrm{rel}} \mkern1mu \zeta_x}{\gamma_{\textrm{rel}}^2 \left( 1 - \beta_x \mkern1mu \beta_{\textrm{rel}} \right)^3} \\[2 em]
\dot{\gamma} &= \dfrac{\dot{\gamma}^\prime + \dot{\omega}^\prime_x \mkern1mu \beta_{\textrm{rel}} }{1 + \beta^\prime_x \mkern1mu \beta_{\textrm{rel}}} \\[3pt]
\zeta_x &= \dfrac{\zeta_x^\prime}{\gamma_{\textrm{rel}}^3 \left( 1 + \beta^{\prime}_x \mkern1mu \beta_{\textrm{rel}} \right)^3 } \\[3pt]
\zeta_y &= \dfrac{\zeta_y^\prime}{\gamma_{\textrm{rel}}^2 \left( 1 + \beta_x^\prime \mkern1mu \beta_{\textrm{rel}} \right)^2 } - \dfrac{\beta_y^\prime \mkern1mu \beta_{\textrm{rel}} \mkern1mu \zeta_x^\prime}{\gamma_{\textrm{rel}}^2 \left( 1 + \beta^{\prime}_x \mkern1mu \beta_{\textrm{rel}} \right)^3} \\[5pt]
\zeta_z &= \dfrac{\zeta_z^\prime}{\gamma_{\textrm{rel}}^2 \left( 1 + \beta_x^\prime \mkern1mu \beta_{\textrm{rel}} \right)^2 } - \dfrac{\beta_z^\prime \mkern1mu \beta_{\textrm{rel}} \mkern1mu \zeta_x^\prime}{\gamma_{\textrm{rel}}^2 \left( 1 + \beta^{\prime}_x \mkern1mu \beta_{\textrm{rel}} \right)^3} .
\end{aligned}
\end{equation}
We can use these ugly equations if we must, but transforming to the \emph{traveler's} instantaneous rest frame using the proper acceleration is clearly preferable. In fact, if an observer's (unprimed) rest frame is in standard configuration with the traveler's (primed) instantaneous rest frame (which is just a matter of choosing the right coordinate system), then $\beta_x^\prime = \beta_y^\prime = \beta_z^\prime = 0$, and Equations \ref{eq:at} give the Cartesian components of the traveler's proper acceleration:
\begin{equation*}
\begin{aligned}
\qquad \qquad \qquad & \zeta_{0 \mkern1.5mu x} = \zeta_x^\prime  = \gamma_{\textrm{rel}}^3 \mkern1mu \zeta_x = \gamma^3 \zeta_x \\[2pt]
& \zeta_{0 \mkern1.5mu y} = \zeta_y^\prime  = \gamma^2  \zeta_y  \qquad \qquad \qquad \textrm{\footnotesize{ (for standard configuration)}} \\[2pt]
& \zeta_{0 \mkern1.5mu z} = \zeta_z^\prime  = \gamma^2 \zeta_z .
\end{aligned}
\end{equation*}
This accords with our results from Section \ref{ssec:pa}: if $\vvbeta \parallel \vvzeta$, then $\vvzeta_{\mkern1mu 0} = \gamma^3 \vvzeta$; and if $\vvbeta \perp \vvzeta$, then $\vvzeta_{\mkern1mu 0} = \gamma^2 \vvzeta$. In general, an observer can split the proper acceleration into a sum of two component vectors, one parallel and the other perpendicular to the traveler's three-velocity:
\begin{equation}\label{eq:pa}
\vvzeta_{\mkern1mu 0} = \gamma^3 \vvzeta_{\parallel \vvbeta} + \gamma^2 \vvzeta_{\perp \vvbeta}.
\end{equation}

\subsection{Force Transformed}\label{ssec:ft}

That wasn't pleasant. Transforming the components of four-force won't be pleasant either, but it won't be quite as bad. First:
\begin{equation*}
[\vv F^\prime] = \Lambda \mkern1mu [\vv F] \qquad \qquad [\vv F] = \Lambda^{-1} \mkern1mu [\vv F^\prime] ,
\end{equation*}
which is:
\begin{equation*}
\begin{bmatrix}
\gamma^\prime \mathcal{P}^\prime \\[.15 em]
\gamma^\prime f_x^\prime \\[.15 em]
\gamma^\prime f_y^\prime \\[.15 em]
\gamma^\prime f_z^\prime\\[.1 em]
\end{bmatrix}
=
\begin{bmatrix}
\gamma_{\textrm{rel}} & -\gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & 0 & 0 \\[.25 em]
- \gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & \gamma_{\textrm{rel}} & 0 & 0 \\[.25 em]
0 & 0 & 1 & 0 \\[.25 em]
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
\gamma \mathcal{P} \\[.15 em]
\gamma f_x \\[.15 em]
\gamma f_y \\[.15 em]
\gamma f_z \\[.1 em]
\end{bmatrix}
\end{equation*}
and
\begin{equation*}
\begin{bmatrix}
\gamma \mathcal{P} \\[.15 em]
\gamma f_x \\[.15 em]
\gamma f_y \\[.15 em]
\gamma f_z \\[.1 em]
\end{bmatrix}
=
\begin{bmatrix}
\gamma_{\textrm{rel}} & \gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & 0 & 0 \\[.25 em]
\gamma_{\textrm{rel}} \mkern1mu \beta_{\textrm{rel}} & \gamma_{\textrm{rel}} & 0 & 0 \\[.25 em]
0 & 0 & 1 & 0 \\[.25 em]
0 & 0 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
\gamma^\prime \mathcal{P}^\prime \\[.15 em]
\gamma^\prime f_x^\prime \\[.15 em]
\gamma^\prime f_y^\prime \\[.15 em]
\gamma^\prime f_z^\prime\\[.1 em]
\end{bmatrix} .
\end{equation*}
That's:
\begin{equation*}
\begin{aligned}
\gamma^\prime \mathcal{P}^\prime &= \gamma_{\textrm{rel}} \mkern1mu \gamma \left( \mathcal{P} - \beta_{\textrm{rel}} \mkern1mu f_x \right) \qquad & \gamma \mathcal{P} &= \gamma_{\textrm{rel}} \mkern1mu \gamma^\prime \left( \mathcal{P}^\prime + \beta_{\textrm{rel}} \mkern1mu f^\prime_x \right) \\[1pt]
\gamma^\prime f_x^\prime &= \gamma_{\textrm{rel}} \mkern1mu \gamma \left( f_x - \beta_{\textrm{rel}} \mkern1mu \mathcal{P} \right)  & \gamma f_x &= \gamma_{\textrm{rel}} \mkern1mu \gamma^\prime \left( f^\prime_x + \beta_{\textrm{rel}} \mkern1mu \mathcal{P}^\prime \right) \\[2pt]
\gamma^\prime f_y^\prime &= \gamma f_y & \gamma f_y &= \gamma^\prime f_y^\prime \\[2pt]
\gamma^\prime f_z^\prime &= \gamma f_z & \gamma f_z &= \gamma^\prime f_z^\prime.
\end{aligned}
\end{equation*}
After some algebra (again using Equations \ref{eq:fv} for substitutions as needed), we obtain the transformation of power and three-force:
\begin{equation}\label{eq:ft}
\begin{aligned}
\mathcal{P}^\prime &= \dfrac{\mathcal{P} - \beta_{\textrm{rel}} \mkern1mu f_x}{1 - \beta_{\textrm{rel}} \mkern1mu \beta_x} & \mathcal{P} &= \dfrac{\mathcal{P}^\prime + \beta_{\textrm{rel}} \mkern1mu f_x^\prime}{1 + \beta_{\textrm{rel}} \mkern1mu \beta_x^\prime} \\[6pt]
f_x^\prime &= \dfrac{f_x - \beta_{\textrm{rel}} \mkern1mu \mathcal{P} }{1 - \beta_{\textrm{rel}} \mkern1mu \beta_x} & f_x &= \dfrac{ f^\prime_x + \beta_{\textrm{rel}} \mkern1mu \mathcal{P}^\prime }{1 + \beta_{\textrm{rel}} \mkern1mu \beta^\prime_x} \\[5pt]
f_y^\prime &= \dfrac{f_y}{\gamma_{\textrm{rel}} \left( 1 - \beta_{\textrm{rel}} \mkern1mu \beta_x \right) } \qquad \quad & f_y &= \dfrac{f^\prime_y}{\gamma_{\textrm{rel}} \left( 1 + \beta_{\textrm{rel}} \mkern1mu \beta^\prime_x \right) } \\[5pt]
f_z^\prime &= \dfrac{f_z}{\gamma_{\textrm{rel}} \left( 1 - \beta_{\textrm{rel}} \mkern1mu \beta_x \right) } & f_z &= \dfrac{f^\prime_z}{\gamma_{\textrm{rel}} \left( 1 + \beta_{\textrm{rel}} \mkern1mu \beta^\prime_x \right) } .
\end{aligned}
\end{equation}
As with acceleration, we can use the power and force transformations if we must, but it's nicer to transform to the \emph{traveler's} instantaneous rest frame using proper power and proper force. And again, if an (unprimed) observer chooses a coordinate system that's in standard configuration with a (primed) traveler's instantaneous rest frame, then $\vvbeta^\prime = \vv 0$, $\mathcal{P}^\prime = \mathcal{P}_0$ (invariant proper power), and Equations \ref{eq:ft} give the Cartesian components of the proper force acting on the traveler:
\begin{equation}\label{eq:pf}
\begin{aligned}
\qquad \qquad \qquad & f_{0 \mkern1.5mu x} = f_x^\prime = f_x - \mathcal{P}_0 \mkern1mu \beta_x \\[2pt]
& f_{0 \mkern1.5mu y} = f_y^\prime = \gamma f_y \qquad \qquad \quad \textrm{\footnotesize{ (for standard configuration)}} \\[2pt]
& f_{0 \mkern1.5mu z} = f_z^\prime = \gamma f_z ,
\end{aligned}
\end{equation}
since now $\gamma_{\textrm{rel}} = \gamma$ and $\beta_{\textrm{rel}} = \beta = \beta_x$ (standard configuration means $\beta_x > 0$).

There are two notable special cases for Equations \ref{eq:ft} and \ref{eq:pf}. The first is when the four-force induces no acceleration but causes the traveler to lose or gain rest energy (as with the light-emitting body in Einstein's thought experiment). Then $\vv F = \mathcal{P}_0 \mkern.5mu \vv B$ (Equation \ref{eq:38}), $\mathcal{P} = \mathcal{P}^\prime = \mathcal{P}_0$ (Equation \ref{eq:po} with zero acceleration), and $\vv f^{[\prime]} = \mathcal{P}_0 \vvbeta^{[\prime]}$ (Equation \ref{eq:41} with zero acceleration), so that the three-force transformation is just the proper power times the three-velocity transformation (Equations \ref{eq:vt}):
\begin{equation*}
\begin{aligned}
f_x^{\prime} &= \mathcal{P}_0 \, \frac{\beta_x - \beta_{\textrm{rel}}}{1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}}} & f_x &= \mathcal{P}_0 \, \frac{\beta_x^{\prime} + \beta_{\textrm{rel}}}{1 + \beta_x^{\prime} \mkern1mu \beta_{\textrm{rel}}}\\[5pt]
f_y^{\prime} &= \mathcal{P}_0 \, \frac{\beta_y}{\gamma_{\textrm{rel}} \left( 1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}} \right)}& \qquad f_y &= \mathcal{P}_0 \, \frac{\beta_y^{\prime}}{\gamma_{\textrm{rel}} \left( 1 + \beta_x^{\prime} \mkern1mu \beta_{\textrm{rel}} \right) } \qquad \textrm{\footnotesize{ (for $\vv Z = \textrm{\mbox{\boldmath $\emptyset$}}$)}}\\[5pt]
f_z^{\prime} &= \mathcal{P}_0 \, \frac{\beta_z}{\gamma_{\textrm{rel}} \left( 1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}} \right)}& f_z &= \mathcal{P}_0 \, \frac{\beta_z^{\prime}}{\gamma_{\textrm{rel}} \left( 1 + \beta_x^{\prime} \mkern1mu \beta_{\textrm{rel}} \right)}.
\end{aligned}
\end{equation*}
If the primed observer \emph{is} the traveler (standard configuration), then $\vvbeta^\prime = \vv 0$, giving $f_y = f_z = 0$ and $f_x = \mathcal{P}_0 \mkern1mu \beta_{\mathrm{rel}} = \mathcal{P}_0 \mkern1mu \beta_x$, and Equations \ref{eq:pf} yield $\vv f_0 = \vv 0$, as they must (with no acceleration, the proper force is a zero vector).

The other special case---the usual scenario---is when the four-force induces acceleration without changing the traveler's rest energy. Then $\vv F = E_0 \mkern.5mu \vv Z$ (Equation \ref{eq:39}), $\vv f^{[\prime]} = E_0 \dot{\vvomega}^{[\prime]}$ (Equation \ref{eq:29}), and $\mathcal{P}^{[\prime]} = \vv f^{[\prime]} \cdot \vvbeta^{[\prime]}$ (Equation \ref{eq:fb}, or Equation \ref{eq:rp} with zero proper power). The three-force transformation from Equations \ref{eq:ft} is then:
\begin{equation*}
\begin{aligned}
f_x^\prime &= \dfrac{f_x - \beta_{\textrm{rel}} \mkern1mu ( \vv f \cdot \vvbeta ) }{1 - \beta_{\textrm{rel}} \mkern1mu \beta_x} & f_x &= \dfrac{ f^\prime_x + \beta_{\textrm{rel}} \mkern1mu ( \vv f^\prime \cdot \vvbeta^\prime ) }{1 + \beta_{\textrm{rel}} \mkern1mu \beta^\prime_x} \\[5pt]
f_y^\prime &= \dfrac{f_y}{\gamma_{\textrm{rel}} \left( 1 - \beta_{\textrm{rel}} \mkern1mu \beta_x \right) } \qquad \quad & f_y &= \dfrac{f^\prime_y}{\gamma_{\textrm{rel}} \left( 1 + \beta_{\textrm{rel}} \mkern1mu \beta^\prime_x \right) } \qquad \textrm{\footnotesize{ (for $\mathcal{P}_0 = 0$)}} \\[5pt]
f_z^\prime &= \dfrac{f_z}{\gamma_{\textrm{rel}} \left( 1 - \beta_{\textrm{rel}} \mkern1mu \beta_x \right) } & f_z &= \dfrac{f^\prime_z}{\gamma_{\textrm{rel}} \left( 1 + \beta_{\textrm{rel}} \mkern1mu \beta^\prime_x \right) } .
\end{aligned}
\end{equation*}
And if the primed observer \emph{is} the traveler (standard configuration), then Equations \ref{eq:pf} become:
\begin{equation*}
\begin{aligned}
\qquad \qquad \qquad & f_{0 \mkern1.5mu x} = f_x^\prime = f_x \\[2pt]
& f_{0 \mkern1.5mu y} = f_y^\prime = \gamma f_y \qquad \textrm{\footnotesize{ (for standard configuration and $\mathcal{P}_0 = 0$)}} \\[2pt]
& f_{0 \mkern1.5mu z} = f_z^\prime = \gamma f_z .
\end{aligned}
\end{equation*}
That first result means that the Cartesian component of three-force whose corresponding component vector is parallel to a traveler's three-velocity is always equal to the matching component of the traveler's \emph{proper} force (provided that the traveler's rest energy isn't changing); i.e., the component is invariant under a Lorentz boost along the axis of the traveler's motion (though that axis may change as the force acts!). This is a generalization of our earlier finding that $\vv f = \vv f_0$ when $\vv f \parallel \vvbeta$ (see the text after Equation \ref{eq:46}).


\section{Hyperbolic Angles}\label{sec:ra}

\subsection{What and Why?}

Back in Section \ref{ssec:fd}, we made passing reference to a special case in which the Minkowski dot product \emph{can} be expressed in a way that's analogous to the Euclidean relation $\vv q \cdot \vv w = qw \cos \theta$. The expression is $\vv Q \cdot \vv W = QW \cosh \phi$, where $\phi$ is the hyperbolic angle between the qualifying four-vectors $\vv Q$ and $\vv W$, but we noted that this equation---and indeed the very notion of an angle between four-vectors---is conditional.

Conditional on what? What's this special case?\footnote{Beyond the special case we'll discuss, there are actually a couple of others in which one might reasonably associate a hyperbolic argument with a pair of four-vectors. But they're unimportant for our purposes, and the particular relation $\vv Q \cdot \vv W = QW \cosh \phi$ doesn't hold for them.} What do hyperbolas have to do with anything, and where does that equation come from?

The special case is when the vectors $\vv Q$ and $\vv W$ are timelike and have time components with the same sign---either both positive (the vectors are \textbf{future-pointing}) or both negative (\textbf{past-pointing}).\footnote{\label{fn:nt}An example of a past-pointing vector is the four-force acting on the light-emitting body in Einstein's thought experiment. In the body's rest frame, the time component $\mathcal{P}_0$ is negative because the body loses rest energy, and the four-force is timelike because $\mathcal{P}_0^2 > f_0^2 = 0$. By the way, lightlike vectors are classified as future- or past-pointing, too.} We'll see why in a moment. As for hyperbolas, consider:
\begin{equation}\label{eq:50}
1 = \cosh^2 {\phi} - \sinh^2 {\phi},
\end{equation}
the hyperbolic equivalent of the better-known Pythagorean trigonometric identity $1 = \cos^2 {\theta} + \sin^2 {\theta}$. In case you're not familiar with the hyperbolic functions, the three ``main" ones and their inverses are:
\begin{equation}\label{eq:hf}
\begin{aligned}
\cosh{\phi} = \dfrac{\mathrm{e}^\phi + \mathrm{e}^{-\phi}}{2} \qquad \quad \sinh{\phi} &= \dfrac{\mathrm{e}^\phi - \mathrm{e}^{-\phi}}{2} \qquad \quad \tanh{\phi} = \dfrac{\mathrm{e}^\phi - \mathrm{e}^{-\phi}}{\mathrm{e}^\phi + \mathrm{e}^{-\phi}} \\[10pt]
\cosh^{-1}{x} = \ln{ \left( x + \sqrt{x^2 - 1} \right)} &\qquad \quad \sinh^{-1}{x} = \ln{ \left( x + \sqrt{x^2 + 1} \right)} \\[5pt]
\tanh^{-1}{x} &= \dfrac{1}{2} \mkern1mu \ln{ \left( \dfrac{1 + x}{1 - x} \right) } .
\end{aligned}
\end{equation}
Equation \ref{eq:50} immediately calls to mind Equation \ref{eq:25}:
\begin{equation*}
1 = \gamma^2 - \omega^2 ,
\end{equation*}
the squared magnitude of the normalized four-velocity $\vv B$. This is a striking similarity! They're both equations for the unit hyperbola. We take this as motivation to introduce the hyperbolic angle $\phi$ into our relativistic toolbox:
\begin{equation}\label{eq:51}
\boxed{
\begin{aligned}
\cosh{\phi} &= \gamma \\
\sinh{\phi} &= \omega = \gamma \beta \\
\tanh{\phi} &= \beta
\end{aligned}
} \, ,
\end{equation}
where we've used the identity $\tanh{\phi} = \sinh{\phi} \, / \cosh{\phi}$.

Now we can derive $\vv Q \cdot \vv W = QW \cosh \phi$. Start with the four-velocities $\vv B_1$ and $\vv B_2$ of two travelers, and calculate their dot product---say, in the frame where $\vv B_1 = \langle 1, \vv 0 \rangle$ and $\vv B_2 = \langle \gamma, \vvomega \rangle = \langle \cosh{\phi}, \, \hatbeta \sinh{\phi} \rangle$ (by Equations \ref{eq:51}):
\begin{equation*}
\vv B_1 \cdot \vv B_2 = \gamma = \cosh{\phi}.
\end{equation*}
That's an invariant result: the dot product of the four-velocities of a pair of travelers is always their relative Lorentz factor, or $\cosh{\phi}$, where we interpret $\phi$ as the hyperbolic angle between the four-velocities.\footnote{\label{fn:ha}At the risk of stating the obvious: the \emph{relative} $\beta$, $\omega$, and $\gamma$ of two travelers---namely, the $\beta$, $\omega$, and $\gamma$ of one traveler as measured by the other---are trivially invariant. This doesn't mean that either traveler's $\beta$, $\omega$, and $\gamma$ are invariant (they obviously aren't), nor that \emph{differences} in the travelers' velocities and celerities are invariant (again, we know they are not, even under boosts along the axis of the travelers' relative motion). The quantity $\phi$, however, is the hyperbolic \emph{angle} between the travelers' four-velocities, and if it's to live up to its name it should possess some of the nice \emph{properties} of angles. We should expect (and will soon verify), for example, that under appropriate circumstances, differences in hyperbolic angles \emph{are} invariant. Specifically, the (trivially invariant) hyperbolic angle between two four-velocities should always equal the difference of the hyperbolic angles they make with a third \emph{coplanar} four-velocity, though we'll need to allow for \emph{signed} hyperbolic angles to make this work.} Next, recall that normalized four-velocity is a dimensionless timelike unit vector (with a positive time component). Any other timelike vector can therefore be expressed as the product (or negative product, if it's past-pointing) of its own magnitude and a parallel four-velocity. So if $\vv Q = \pm \, Q \vv B_1$ and $\vv W = \pm \, W \vv B_2$, with $Q$ and $W$ both positive or both negative:
\begin{equation}\label{eq:52}
\begin{split}
\vv Q \cdot \vv W &= \pm \, Q \vv B_1 \cdot \pm \, W \vv B_2 \\
&= QW (\vv B_1 \cdot \vv B_2) \\
\vv Q \cdot \vv W &= QW \cosh{\phi}.
\end{split}
\end{equation}
For any timelike $\vv Q$ and $\vv W$ that are both future-pointing or both past-pointing, then, Equation \ref{eq:52} holds, and $\phi$ can be interpreted as the hyperbolic angle between them. Further, $\cosh{\phi}$ is the relative Lorentz factor of a pair of travelers whose four-velocities are parallel to $\vv Q$ and $\vv W$.

\subsection{Rapidity}

Of course, the four-velocities that are parallel to $\vv Q$ and $\vv W$ in Equation \ref{eq:52} need not correspond with \emph{actual} travelers in a given scenario. For instance, the four-force acting on the light-emitting body in Einstein's thought experiment isn't parallel to the four-velocity \emph{of} anything that we care about (see Footnote \ref{fn:nt}); rather, the ``four-velocity" there is just a unit vector with no physical significance. If that four-force were dotted with another past-pointing timelike vector, we could identify the hyperbolic angle $\phi$ between them, but we'd probably have little interest in the associated relative $\gamma = \cosh{\phi}$ or the rest of Equations \ref{eq:51}, because, well, $\gamma$ and $\beta$ of \emph{what}?

But sometimes the travelers are real, as they'd be if $\vv Q$ and $\vv W$ were their four-displacements or four-momenta (or indeed their four-velocities!). When they are, Equations \ref{eq:51} are physically meaningful, and we call the hyperbolic angle between the travelers' four-velocities their relative \textbf{rapidity}. As an observer, each traveler can speak of the other's rapidity $\phi = \tanh^{-1}{\beta}$, magnitude of the three-vector $\vvphi \equiv \phi \hatphi = \phi \hatbeta$.

\emph{Any expression with a (subluminal) speed,\footnote{Like celerity and the Lorentz factor, the rapidity is undefined for $\beta = 1$.} celerity, or Lorentz factor can be rewritten in terms of the rapidity.} Why might we want to do so?

For starters, rapidity is simply an elegant concept. Consider that we can now write the Lorentz boost matrix and its inverse (Equations \ref{eq:bm} and \ref{eq:im}) like this:
\begin{equation*}
\Lambda =
\begin{bmatrix}
\cosh{\phi} & -\sinh{\phi} & 0 & 0 \\
- \sinh{\phi} & \cosh{\phi} & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\qquad
\Lambda^{-1} =
\begin{bmatrix}
\cosh{\phi} & \sinh{\phi} & 0 & 0 \\
\sinh{\phi} & \cosh{\phi} & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix} .
\end{equation*}
This allows us to understand a Lorentz boost as a hyperbolic rotation of the $ct^{[\prime]}$- and $x^{[\prime]}$-axes through the observers' relative rapidity (the hyperbolic angle between their four-velocities). Isn't that something? It's a very close analogue of a spatial rotation. For example, here are the transformation matrices we'd use for a rotation of the $x^{[\prime]}$- and $y^{[\prime]}$-axes through a Euclidean angle $\theta$:
\begin{equation*}
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & \cos{\theta} & \sin{\theta} & 0 \\
0 & - \sin{\theta} & \cos{\theta} & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\qquad \qquad
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & \cos{\theta} & -\sin{\theta} & 0 \\
0 & \sin{\theta} & \cos{\theta} & 0 \\
0 & 0 & 0 & 1
\end{bmatrix} .
\end{equation*}

As a geometric quantity---the hyperbolic angle between a traveler's and observer's four-velocities---the rapidity is arguably more ``fundamental" than speed or celerity or the Lorentz factor. There's something satisfying about recasting relativistic mechanics in terms of four-vectors, invariants, and rapidities alone. Four-velocity (normalized):
\begin{equation*}
\vv B = \langle \cosh{\phi}, \, \hatphi \sinh{\phi} \rangle
\end{equation*}
($\hatphi = \hatbeta$); infinitesimal four-displacement:
\begin{equation*}
\dd \vv R = \vv B \, c \, \dd t_0 = c \, \dd t_0 \, \langle \cosh{\phi}, \, \hatphi \sinh{\phi} \rangle ;
\end{equation*}
four-momentum (for $E_0 \neq 0$):
\begin{equation*}
\vv P = E_0 \vv B = E_0 \, \langle \cosh{\phi}, \, \hatphi \sinh{\phi} \rangle ;
\end{equation*}
four-acceleration (``normalized"):\footnote{\label{fn:ph}By $\mathring{\phi}$ we mean the $ct_0$-derivative of the rapidity $\phi$, \emph{not} the magnitude of the vector $\mathring{\vvphi}$. They're equal only in the case of rectilinear motion (or when there's no acceleration):
\begin{equation*}
\left \Vert \mathring{\vvphi} \right \Vert = \left \Vert \dfrac{1}{c} \, \dfrac{\dd}{\dd t_0} \, ( \phi \hatphi ) \right \Vert = \left \Vert \mathring{\phi} \hatphi + \phi \mathring{\hatphi} \right \Vert,
\end{equation*}
which is just $\mathring{\phi}$ for rectilinear motion ($\mathring{\vvphi} \parallel \vvphi$), because then $\mathring{\hatphi} = \vv 0$.}
\begin{equation*}
\vv Z = \mathring{\vv B} = \langle  \mathring{\phi} \sinh{\phi} , \, \hatphi \mkern1.5mu \mathring{\phi} \cosh{\phi} + \mathring{\hatphi} \sinh{\phi} \rangle
\end{equation*}
(because the hyperbolic sine and hyperbolic cosine functions are derivatives of each other); and four-force:
\begin{equation*}
\vv F = \mathring{\vv P} = \mathcal{P}_0 \vv B + E_0 \vv Z
\end{equation*}
($\mathcal{P}_0 = \mathring{E}_0$ being the proper power), or
\begin{equation*}
\vv F = \langle \mathcal{P}_0 \cosh{\phi} + E_0 \mkern1.5mu \mathring{\phi} \sinh{\phi} , \, \hatphi ( \mathcal{P}_0 \sinh{\phi} + E_0 \mkern1.5mu \mathring{\phi} \cosh{\phi} ) + \mathring{\hatphi} E_0 \sinh{\phi} \rangle .
\end{equation*}
Admittedly, the components of four-acceleration and four-force are somewhat unwieldy when written out like this, but they're even worse in terms of $\vvbeta$, $\vvomega$, and $\gamma$, and the user-friendly nature of hyperbolic functions is a plus. Just look how easy it is to obtain an expression for the squared proper acceleration using the rapidity (and Equation \ref{eq:pa4}):
\begin{equation}\label{eq:pa3}
\begin{split}
- \, Z^2 &= - \, \vv Z \cdot \vv Z = \left( \hatphi \mkern1.5mu \mathring{\phi} \cosh{\phi} + \mathring{\hatphi} \sinh{\phi} \right)^2 - \left( \mathring{\phi} \sinh{\phi} \right)^2 \\
\zeta_0^2 &= \mathring{\phi}^2 + (\mathring{\hatphi} \sinh{\phi})^2 
\end{split}
\end{equation}
because $\hatphi \cdot \mathring{\hatphi} = 0$ (unit vector is orthogonal to its time-derivative), $\hatphi \cdot \hatphi = 1$, and $\cosh^{2}{\phi} - \sinh^{2}{\phi} = 1$. It requires some algebra to show that the right side of Equation \ref{eq:pa3} is equivalent to the negative square of the right sides of Equations \ref{eq:pa2} and \ref{eq:37}, but it is, and it was child's play to derive. So in addition to its conceptual elegance, the rapidity offers much \emph{mathematical} elegance.

Equation \ref{eq:pa3} also gives us the appealing relation $\zeta_0 = \mathring{\phi}$ for $\mathring{\vvphi} \parallel \vvphi$.\footnote{In this case, $\mathring{\phi} = \Vert \mathring{\vvphi} \Vert$ (see Footnote \ref{fn:ph}), and $\vvzeta_{\mkern1mu 0} = \mathring{\vvphi}$.} When the traveler's motion is rectilinear, then, $\mathring{\phi}$ is the magnitude of the proper acceleration and therefore invariant under a Lorentz boost in the $\pm \, \hatphi$ direction. Same goes for $\dd \phi = \mathring{\phi} \, c \, \dd t_0$: incremental \emph{changes} in the traveler's rapidity are invariant among observers for whom the traveler's $\mathring{\vvphi} \parallel \vvphi$. Now we're getting somewhere! Note that because we've defined rapidity as a magnitude, we cannot extend this argument to say that \emph{differences} in the rapidities of two rectilinear travelers with parallel velocities are necessarily invariant among the would-be qualifying observers. If one traveler is moving to your left at the same speed that the other is moving to your right, then you'll say the difference in their rapidities is zero, but you'd give a different answer after a boost to your left or right (imagine boosting all the way to one of the travelers' rest frames---is the other traveler now at rest? no). Partly to remedy this ``shortcoming," we'll introduce a \emph{signed} rapidity-like parameter in the next section. Naively we might guess that this signed parameter will be the Cartesian component of $\vvphi$ corresponding to the boost axis ($\phi_x$ if the boost is along the $x$-axis), but we'll see that that's not generally so.


\subsection{Longitudinal Rapidity}\label{ssec:lr}

Since the inverse hyperbolic tangent is an odd function with a domain of $(-1, 1)$, there's nothing stopping us from feeding it signed velocity \emph{components} and working with the identically signed rapidity-like quantities it outputs. The hyperbolic tangent function is likewise odd and also returns values that retain the sign of their inputs, so the process is ``reversible." Let's experiment with this. Say we've got two inertial observers whose rest frames are in standard configuration (boost along the $x^{[\prime]}$-axis), and say they're measuring the velocity $\vvbeta$ ($\vvbeta^\prime$) of some third party. By Equations \ref{eq:vt}, the velocity component corresponding to the boost axis transforms like this:
\begin{equation*}
\beta_x^{\prime} = \frac{\beta_x - \beta_{\textrm{rel}}}{1 - \beta_{x} \mkern1mu \beta_{\textrm{rel}}} \qquad \qquad \beta_x = \frac{\beta_x^{\prime} + \beta_{\textrm{rel}}}{1 + \beta_x^{\prime} \mkern1mu \beta_{\textrm{rel}}}.
\end{equation*}
Now let's use Equations \ref{eq:51} to rewrite the relative speed of the frames $\beta_{\textrm{rel}}$ in terms of its rapidity equivalent $\phi_{\textrm{rel}}$, and let's also rewrite each velocity \emph{component} as the hyperbolic tangent of a rapidity-like parameter that we'll label $\varphi^{[\prime]}$ ($\varphi$ is the ``curly" version of the Greek letter phi [$\phi$]):
\begin{equation}\label{eq:th}
\tanh{\varphi^{\prime}} = \frac{\tanh{\varphi} - \tanh{\phi_{\textrm{rel}}}}{1 - \tanh{\varphi} \,  \tanh{\phi_{\textrm{rel}}}} \qquad \quad \tanh{\varphi} = \frac{\tanh{\varphi^{\prime}} + \tanh{\phi_{\textrm{rel}}}}{1 + \tanh{\varphi^{\prime}} \, \tanh{\phi_{\textrm{rel}}}} .
\end{equation}
The pertinent characteristic of $\varphi$ is that it's the inverse hyperbolic tangent of the Cartesian velocity component \emph{corresponding to the axis of the given Lorentz boost} (the $x$-axis here). This new quantity is the \textbf{longitudinal rapidity}. Particle physicists usually label it $y$ and sometimes call it the ``particle rapidity" or, much to the confusion of outsiders, just the ``rapidity."\footnote{They also prefer to express it in terms of energy and momentum rather than velocity: $\tanh^{-1}{\beta_x} = \tanh^{-1}{(cp_x / E)} = \frac{1}{2} \ln{[(E + cp_x) / (E - cp_x) ]}$ (by Equations \ref{eq:19} and \ref{eq:hf}). We choose not to label it $y$ because that symbol is already doing Cartesian duties for us. Anyway, $\varphi$ kind of looks like $y$, and as a variant of the letter $\phi$ it seems fitting.}

So what does the longitudinal rapidity do for us? Quite a bit, if we happen to know the addition/subtraction rule for the hyperbolic tangent function:
\begin{equation*}
\tanh{(a \pm b)} = \dfrac{\tanh{a} \pm \tanh{b}}{1 \pm \tanh{a} \, \tanh{b}}.
\end{equation*}
That turns Equations \ref{eq:th} into:
\begin{equation*}
\tanh{\varphi^{\prime}} = \tanh{(\varphi - \phi_{\textrm{rel}})} \qquad \qquad \tanh{\varphi} = \tanh{(\varphi^\prime + \phi_{\textrm{rel}})},
\end{equation*}
meaning:
\begin{equation}\label{eq:lr}
\varphi^\prime = \varphi - \phi_{\textrm{rel}} \qquad \qquad \varphi = \varphi^\prime + \phi_{\textrm{rel}}.
\end{equation}
Under a boost along the relevant axis, the longitudinal rapidity transforms additively! Geometrically, this is to be expected: $\phi_{\textrm{rel}}$, $\varphi$, and $\varphi^\prime$ are all hyperbolic angles in the $x/ct$-plane ($\varphi$ and $\varphi^{\prime}$ being \emph{signed} angles), and coplanar angles \emph{should} be additive, as we mentioned in Footnote \ref{fn:ha}.

Another handy feature of longitudinal rapidity follows from the graceful way it transforms. Say that our primed and unprimed observers are measuring \emph{two} velocities instead of one (we'll use the subscripts 1 and 2). Maybe the velocities are those of a single traveler at two different proper-times, or maybe they're those of two different constant-velocity travelers. Either way, by Equations \ref{eq:lr}:
\begin{equation*}
\begin{aligned}
\varphi_{2}^\prime - \varphi_{1}^\prime &= \left( \varphi_{2} - \phi_{\textrm{rel}} \right) - \left( \varphi_{1} - \phi_{\textrm{rel}} \right) \\[4pt]
&= \varphi_{2} - \varphi_{1}.
\end{aligned}
\end{equation*}
In other words, \emph{changes} and \emph{differences} in longitudinal rapidity are invariant (under a boost along the axis in question). We know that this isn't true of \emph{velocity}: if one traveler recedes to your left at half the speed of light while another recedes to your right at the same speed, then you'll say their (normalized) ``separation speed" is ${1/2 - (-1/2) = 1}$, but neither traveler says the other recedes at the speed of light. Rather, by Equations \ref{eq:vt}, each says the other recedes at $\beta = 4/5$. Yet all three of you would agree on the travelers' ``separation rapidity": $\tanh^{-1}(1/2) - \tanh^{-1}(-1/2) = \tanh^{-1}(4/5)$.

Bear in mind that, in general, $\varphi \neq \phi_x$ ($x$ being the boost-axis). If $\theta$ is the angle between $\vvphi$ and the positive $x$-direction, then longitudinal rapidity is $\varphi = \tanh^{-1} \beta_x = \tanh^{-1}{(\beta \cos{\theta})}$, whereas the Cartesian component $\phi_x$ of $\vvphi$ is $\phi \cos{\theta} = (\tanh^{-1}{\beta}) (\cos{\theta})$. The latter isn't really a useful quantity, except in the special case that it's the \emph{only} non-zero component of $\vvphi$ and hence \emph{equal to} the former---i.e., if $\theta$ is $0$ or $\pi$, then $\cos{\theta} = \pm \, 1$, and consequently $\tanh^{-1} \beta_x = \phi_x$ (because inverse hyperbolic tangent is an odd function). Along the same lines, note that we cannot generally ``apply" Equations \ref{eq:51} to $\phi_x$ (or $\varphi$) to find $\gamma$ or the celerity component $\omega_x = \gamma \beta_x = \cosh{\phi} \tanh{\varphi}$. Again, the exception is when the velocity is directed along the $x$-axis, because then $[\varphi = \, ] \tanh^{-1} \beta_x = \phi_x = \pm \, \phi$ (and hyperbolic cosine is an even function):
\begin{equation}\label{eq:rx}
\begin{aligned}
\cosh{\phi _x} &= \gamma \\
\sinh{\phi_x} &= \omega_x \qquad \textrm{\footnotesize{ (for $\vvbeta = \langle \beta_x, 0, 0 \rangle $)}} .\\
\tanh{\phi_x} &= \beta_x
\end{aligned}
\end{equation}

Here's a neat application. Twice now---first after deriving Equation \ref{eq:46}, and then at the end of Section \ref{ssec:ft}---we've shown that among observers for whom a traveler's three-velocity and three-force are parallel, the magnitude of the three-force is equal to the magnitude of the proper force and thus invariant under a Lorentz boost in the $\pm \, \hatbeta$ direction (provided that the traveler's rest energy remains constant). More generally, we found that even if the three-force isn't parallel to the three-velocity, the Cartesian \emph{component} of three-force whose corresponding component vector is (momentarily) parallel to the three-velocity is invariant under such a boost. Another demonstration of this conditional invariance, more attractive perhaps, uses longitudinal rapidity and begins with Equation \ref{eq:29}:
\begin{equation*}
\begin{split}
\vv f &= \dot{\vv p} c = E_0 \dot{\vvomega} =  \dfrac{E_0}{\gamma} \, \mathring{\vvomega} \\[3pt]
f_x & = \dfrac{E_0}{\gamma} \, \mathring{\omega}_x = \dfrac{E_0}{\cosh{\phi}} \, \mathring{\omega}_x .
\end{split}
\end{equation*}
The $x$-axis will be our boost axis, so longitudinal rapidity is $\varphi = \tanh^{-1}{\beta_x}$, and $\omega_x = \gamma \beta_x = \cosh{\phi} \mkern1mu \tanh{\varphi}$. Now impose the condition that the three-velocity is in the positive or negative $x$-direction. This gives $\varphi = \phi_x = \pm \, \phi$ and puts Equations \ref{eq:rx} into play, such that $\omega_x = \cosh{\varphi} \mkern1mu \tanh{\varphi} = \sinh{\varphi}$:
\begin{equation*}
\begin{split}
f_x &= \dfrac{E_0}{\cosh{\varphi}} \, \dfrac{\dd}{\dd t_0} \left( \sinh{\varphi} \right) \dfrac{1}{c}  \\[3pt]
f_x &= E_0 \mkern1.5mu \mathring{\varphi} .
\end{split}
\end{equation*}
Since $E_0$ and $c \, \dd t_0$ are Lorentz-invariant, and since $\dd \varphi$ (infinitesimal change in longitudinal rapidity) is invariant under a boost along the $x$-axis, the component $f_x = E_0 (\dd \varphi / \dd t_0) / c$ must also be invariant under a boost along the $x$-axis---that is, under a boost in the $\pm \, \hatbeta$ direction.


\subsection{Pseudorapidity and the Ultrarelativistic Limit}

Particle physicists are keen on longitudinal rapidity (they often call it ``particle rapidity" or even just ``rapidity," and they label it $y$). That's because they deal with particles emitted from collisions in accelerators, and there's need to analyze this debris in the center-of-momentum frame of the colliding-particle system, which generally isn't the same as the lab frame. This puts a premium on quantities that are invariant under a Lorentz boost along the axis of the original beam (the $z$-axis, customarily), and differences in the corresponding longitudinal rapidities of ejecta fit the bill.

Unfortunately, longitudinal rapidities aren't always easy to measure directly in particle accelerators. But in the \textbf{ultrarelativistic limit} ($\beta \rightarrow 1$), there's another quantity called \textbf{pseudorapidity} that's an excellent approximation to longitudinal rapidity, and it's much easier to measure: all you need is the emitted particle's so-called polar angle $\theta$, defined as the angle between the beam axis (positive $z$-direction) and the particle's velocity vector.

Before getting into the pseudorapidity, let's take a moment to mull over the ultrarelativistic limit. Above, we characterized it as $\beta \rightarrow 1$, rather than $\beta \approx 1$. The reason is that we might care how close to the speed of light something is moving (i.e., $1 - \beta$).\footnote{Why $1 - \beta$ instead of just $\beta$? Because in the ultrarelativistic limit, $\beta$ will begin with too many $9$'s after the decimal point for practical use (too many significant digits for scientific notation). Much better to deal with a bunch of $0$'s.} If not, $\beta \approx 1$ may suffice, but if so, it plainly won't. This is similar to a situation we dealt with back in Section \ref{ssec:re}, when we couldn't use $\gamma \approx 1$ to derive Equation \ref{eq:14} (would have given us a value of zero for kinetic energy in the classical limit).\footnote{If one already has Equation \ref{eq:21} in hand (we didn't), then one \emph{can} derive Equation \ref{eq:14} using $\gamma \approx 1$ (and Equations \ref{eq:12} and \ref{eq:17}):
\begin{equation*}
\begin{split}
(pc)^2 &= E^2 - E_0^2 = (E + E_0) (E - E_0) = (E + E_0) E_{\mkern.5mu \textrm{k}} \\[2pt]
E_{\mkern.5mu \textrm{k}} &= \dfrac{(pc)^2}{E + E_0} = \dfrac{(\gamma E_0 \beta)^2}{\gamma E_0 + E_0} = E_0 \beta^2 \, \dfrac{\gamma^2}{\gamma + 1} \\[2pt]
&\approx \dfrac{1}{2} \, E_0 \beta^2 .
\end{split}
\end{equation*}
} Our solution here is similar, too: we'll use the binomial theorem to get extra terms as needed.

Start with the energy--momentum relation (Equation \ref{eq:21}), and isolate $E$:
\begin{equation*}
E = \sqrt{(p c)^2 + E_0^2} .
\end{equation*}
Now, since $p c \gg E_0$ as $\beta \rightarrow 1$, let's factor out $p c$. That way our binomial expansion will involve increasing powers of $E_0 / p c$, which is small:\footnote{If we wanted to approximate $E$ in the classical limit, we'd factor out $E_0$ instead.}
\begin{equation*}
E = p c \left[ 1 + \left( \dfrac{E_0}{p c} \right)^2 \right]^{1/2}.
\end{equation*}
Recall the binomial series:
\begin{equation*}
(1+x)^n = 1 + nx + \frac{n(n-1)}{2!} \, x^2 + \frac{n(n-1)(n-2)}{3!} \, x^3 + \dots ,
\end{equation*}
convergent if $|x| < 1$. We've got $x = (E_0 / p c)^2 = \omega^{-2}$ (Equation \ref{eq:29}), and some algebra shows that $|\omega^{-2}|<1$ when $\beta > \sqrt{2}/2 \approx .707$, in which case:
\begin{equation*}
E = p c \left[ 1 + \dfrac{1}{2} \left( \dfrac{E_0}{p c} \right)^2 - \dfrac{1}{8} \left( \dfrac{E_0}{p c} \right)^4 + \dots \right] .
\end{equation*}
As planned, $1 \gg (E_0 / p c)^2 \gg (E_0 / p c)^4$, etc. For many purposes we can stop at the first term and use $E \approx p c$, but here we'll want the second one, too:
\begin{equation}\label{eq:ue}
E \approx p c \left[ 1 + \dfrac{1}{2} \left( \dfrac{E_0}{p c} \right)^2 \right]  \qquad \textrm{\footnotesize{ (for $\beta \rightarrow 1$)}}.
\end{equation}
Then if we want to know $1 - \beta$, we can use Equations \ref{eq:19} and \ref{eq:ue}:
\begin{equation}\label{eq:ub}
\begin{split}
1 - \beta &= 1 - \dfrac{p c}{E} \\[5pt]
1 - \beta & \approx 1 - \left[ 1 + \dfrac{1}{2} \left( \dfrac{E_0}{p c} \right)^2 \right]^{-1} \qquad \textrm{\footnotesize{ (for $\beta \rightarrow 1$)}}.
\end{split}
\end{equation}
We can make that easier on the eyes. One option is another binomial expansion, which yields the \emph{slightly} worse but still-excellent approximation $1 - \beta \approx .5 (E_0 / p c)^2$. By Equations \ref{eq:29} and \ref{eq:51}, $E_0 / p c = \omega^{-1} = 1 / \sinh{\phi}$, so that this slightly worse approximation is equivalently $.5 \, \text{csch}^2 \, \phi$ (where $\text{csch} \, \phi = 1 / \sinh{\phi}$ is the hyperbolic cosecant function). But we can also plug the rapidity into Equation \ref{eq:ub} directly, rendering it elegant without sacrificing accuracy at all. First:
\begin{equation*}
1 - \beta \approx 1 - \left[ 1 + \dfrac{1}{2 \sinh^2{\phi}} \right]^{-1} .
\end{equation*}
Now we use the ``double-angle" hyperbolic identity $\cosh{(2 \phi)} = 2 \sinh^2{\phi} + 1$:
\begin{equation*}
\begin{split}
1 - \beta & \approx 1 - \left[ 1 + \dfrac{1}{\cosh{(2 \phi)} - 1} \right]^{-1} \\[5pt]
& = 1 - \left[ \dfrac{\cosh{(2 \phi)}}{\cosh{(2 \phi)} - 1} \right]^{-1} \\[7pt]
&= \dfrac{1}{\cosh{(2 \phi)}} ,
\end{split}
\end{equation*}
and if we introduce the hyperbolic secant ($\text{sech} \, \phi = 1 / \cosh{\phi}$):
\begin{equation}\label{eq:ur}
1 - \beta \approx \text{sech} \, (2 \phi) \qquad \textrm{\footnotesize{ (for $\beta \rightarrow 1$)}}.
\end{equation}
Not bad! A better-known approximation (virtually identical to Equation \ref{eq:ur} in the ultrarelativistic limit) comes from Equation \ref{eq:20}:
\begin{equation*}
\begin{split}
\gamma^{-2} &= 1 - \bigl( 1 - \left( 1 - \beta \right) \bigr)^2 \\
& = 1 - \left( 1 - 2 \left( 1 - \beta \right) + \left( 1 - \beta \right)^2 \right),
\end{split}
\end{equation*}
where we set $(1 - \beta)^2 \approx 0$, leaving:
\begin{equation}\label{eq:ug}
\begin{split}
\gamma^{-2} & \approx 2 \left( 1 - \beta \right) \\[5pt]
1 - \beta & \approx \dfrac{1}{2 \gamma^2} \left [= \dfrac{1}{2} \, \text{sech}^2 \, \phi \right] \qquad \textrm{\footnotesize{ (for $\beta \rightarrow 1$)}}.
\end{split}
\end{equation}
Equation \ref{eq:ug} is convenient if you know $\gamma$. Equation \ref{eq:ub} ($=$ Equation \ref{eq:ur}) is good if you know $E_0$ and $p c$. If you know the rapidity, take your pick.

That was a productive diversion, but we don't need any of it for the pseudorapidity. The pseudorapidity is what the longitudinal rapidity $\varphi$ becomes---er, what the particle physicist's ``rapidity" $y$ becomes---when we set $\beta \approx 1$:
\begin{equation*}
\begin{aligned}
y \equiv \varphi &= \tanh^{-1} \beta_z \\
&= \tanh^{-1} ( \beta \cos{\theta} ) \\
& \approx  \tanh^{-1} ( \cos{\theta} ) \equiv \eta ,
\end{aligned}
\end{equation*}
where $\theta$ is the ``polar" angle between $\vvbeta$ (velocity of emitted particle) and the particle beam (positive $z$-direction). This $\eta$ is the pseudorapidity. It's usually expressed as a natural logarithm (Equations \ref{eq:hf}):
\begin{equation*}
\begin{split}
\eta &= \dfrac{1}{2} \mkern1mu \ln{ \left( \dfrac{1 + \cos{\theta}}{1 - \cos{\theta}} \right) } \\[3pt]
&= \ln{ \left( \cot {\dfrac{\theta}{2}} \right) },
\end{split}
\end{equation*}
where we've used the ``half-angle" trig identities $\cos^2{(\theta / 2)} = (1 + \cos{\theta})/2$ and $\sin^2{(\theta / 2)} = (1 - \cos{\theta})/2$.

Because $\eta$ is such a good approximation for $y$ in the ultrarelativistic limit, particle physicists sometimes loosely refer to the pseudorapidity as just \emph{rapidity}. All this inconsistent usage can be confusing, so let's recap. In our lingo, rapidity is $\phi = \Vert \bm{\upphi} \Vert = \tanh^{-1}{\beta}$, and the longitudinal rapidity $\varphi$ is the inverse hyperbolic tangent of the velocity component corresponding to the axis of a given Lorentz boost ($\tanh^{-1}{\beta_x}$ if $x$ is the boost axis). In particle physics, longitudinal rapidity gets the symbol $y$ and is often called the ``particle rapidity" or even just the ``rapidity" for short, although the pseudorapidity $\eta$ is also sometimes called the rapidity.


\section{Advanced Rectilinear Kinematics:\\ Constant Proper Acceleration}\label{sec:ak}

With an important caveat, the Newtonian kinematic equations for constant acceleration are still applicable in special relativity, provided that only \emph{coordinate} quantities appear in them. This is merely a matter of defining velocity and acceleration as the first and second time-derivatives of position. In our notation, for example, and using the subscript i for \emph{initial} values ($ct = 0$):\footnote{It's common to use the naught subscript for initial values, but we're already using that for \emph{proper} quantities.}
\begin{equation*}
\begin{aligned}
\int_0^{ct} \dfrac{1}{c} \, \dfrac{\dd \vvbeta}{\dd t} \, c \, \dd t &= \int_0^{ct} \vvzeta \, c \, \dd t \\[5pt]
\vvbeta -  \vvbeta_{\textrm{i}} &= \vvzeta \mkern2mu ct,
\end{aligned}
\end{equation*}
and then also:
\begin{equation*}
\begin{aligned}
\int_0^{ct} \dfrac{1}{c} \, \dfrac{\dd \vv r}{\dd t} \, c \, \dd t &= \int_0^{ct} \vvbeta \, c \, \dd t \\[5pt]
\vv r - \vv r_{\textrm{i}} &= \int_0^{ct} \left( \vvbeta_{\textrm{i}} + \vvzeta \, ct \right) c \, \dd t \\[3pt]
\vv r &= \vv r_{\textrm{i}} + \vvbeta_{\textrm{i}} \mkern2mu ct + \dfrac{1}{2} \mkern2mu \vvzeta \, ( ct )^2 \\[4pt]
& \left[ = \vv r_{\textrm{i}} + \vv v_{\textrm{i}} \mkern2mu t + \dfrac{1}{2} \mkern2mu \vv a t^2 \right] ,
\end{aligned}
\end{equation*}
which is a parabola (\textbf{parabolic motion} through spacetime). The important caveat is that constant coordinate acceleration cannot be maintained forever, since our universe has a speed limit.

But these equations aren't very \emph{interesting} in special relativity, for the simple reason that coordinate acceleration can only be constant in one inertial frame. Much more interesting is the case of constant \emph{proper} acceleration $\vvzeta_{\mkern1mu 0}$, which can be maintained indefinitely (fuel considerations aside) and whose magnitude is invariant. In general, Equation \ref{eq:pa} gives:
\begin{equation*}
\int_0^{ct} \left( \gamma^3 \vvzeta_{\parallel \vvbeta} + \gamma^2 \vvzeta_{\perp \vvbeta} \right) c \, \dd t = \int_0^{ct} \vvzeta_{\mkern1mu 0} \, c \, \dd t ,
\end{equation*}
or in terms of the traveler's proper time:
\begin{equation*}
\int_0^{ct_0} \left( \gamma^3 \vvzeta_{\parallel \vvbeta} + \gamma^2 \vvzeta_{\perp \vvbeta} \right) c \, \dd t_0 = \int_0^{ct_0} \vvzeta_{\mkern1mu 0} \, c \, \dd t_0 .
\end{equation*}
The time-dependence of the angle between $\vvzeta$ and $\vvbeta$ complicates things, but we'll concern ourselves only with rectilinear motion ($\vvzeta \parallel \vvbeta$), where that's not a problem. First we'll derive kinematic functions of coordinate time, then we'll derive kinematic functions in terms of proper time, and finally we'll discuss the significance of this rectilinear constant-$\vvzeta_{\mkern1mu 0}$ scenario.


\subsection{Functions of Coordinate Time (Team Celerity)}

With $\vvzeta \parallel \vvbeta$, we have:
\begin{equation*}
\int_0^{ct} \gamma^3 \vvzeta \, c \, \dd t = \int_0^{ct} \vvzeta_{\mkern1mu 0} \, c \, \dd t .
\end{equation*}
A gander at Equations \ref{eq:34} and \ref{eq:fa} shows that in this rectilinear case, the quantity $\gamma^3 \vvzeta$ is the $ct$-derivative of the ``normalized" celerity ($\dot{\vvomega} = \gamma \dot{\vvbeta} + \dot{\gamma} \vvbeta$), so for constant proper acceleration:
\begin{equation*}
\begin{split}
\qquad \qquad \int_0^{ct} \dot{\vvomega} \; c \, \dd t &= \int_0^{ct} \vvzeta_{\mkern1mu 0} \, c \, \dd t \\[5pt]
\vvomega - \vvomega_{ \textrm{i}} &= \vvzeta_{\mkern1mu 0} \mkern1mu ct ,
\end{split}
\end{equation*}
similar in form to $\vvbeta =  \vvbeta_{\textrm{i}} + \vvzeta \mkern1mu ct$ for constant coordinate acceleration. Now, we'll need to manipulate vector \emph{components} (we'll point out why when we get there), so let's say that the motion here is in the positive or negative $x$-direction. That way the $y$- and $z$-components of the velocity, celerity, rapidity, and acceleration vectors are zero, and the traveler's $y$ and $z$ coordinates remain constant ($y_{\textrm{i}}$ and $z_{\textrm{i}}$). Then:
\begin{equation}\label{eq:ce}
\omega_x (ct) = \omega_{x \mkern2mu \textrm{i}} + \zeta_{0 \mkern2mu x} \mkern2mu ct \qquad \textrm{\footnotesize{ (for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern1mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$)}}.
\end{equation}
(The function notation serves in part to clarify that initial values like $\omega_{x \mkern2mu \textrm{i}}$ in this section correspond to when $ct = 0$. In the next section, initial values will have the same notation but will correspond to when $ct_0 = 0$.)

Equation \ref{eq:ce} is our anchor, and we need only plug it into Equations \ref{eq:rx} to find expressions for $\phi_x$, $\gamma$, and $\beta_x$ as functions of $ct$. First $\phi_x$:
\begin{equation*}
\phi_x (ct) = \sinh^{-1} \left( \omega_{x \mkern2mu \textrm{i}} + \zeta_{0 \mkern2mu x} \mkern2mu ct \right) .
\end{equation*}
Next, $\gamma$ and $\beta_x$ in terms of $\phi_x$:
\begin{equation*}
\gamma (ct) = \cosh \left( \sinh^{-1} \left( \omega_{x \mkern2mu \textrm{i}} + \zeta_{0 \mkern2mu x} \mkern2mu ct \right)  \right)
\end{equation*}
\begin{equation*}
\beta_x (ct) = \tanh \left( \sinh^{-1} \left( \omega_{x \mkern2mu \textrm{i}} + \zeta_{0 \mkern2mu x} \mkern2mu ct \right) \right).
\end{equation*}
Alternatively, we can use the identity $\cosh{(\sinh^{-1} q) = \sqrt{1 + q^2}}$ (or Equation \ref{eq:25}) to write the Lorentz factor like this:
\begin{equation*}
\gamma (ct) = \sqrt{1 + \omega^2_x (ct)} ,
\end{equation*}
and we can use $\beta_x = \omega_x / \gamma$ or the identity $\tanh{(\sinh^{-1} q)} = q / \sqrt{1 + q^2}$ to write $\beta_x$ like this:
\begin{equation*}
\beta_x (ct) = \dfrac{\omega_x (ct)}{\sqrt{1 + \omega^2_x (ct)}} .
\end{equation*}
As for position, $\vv r (ct) = \langle x (ct) , y_{\mkern1mu \mathrm{i}}, z_{\mkern1mu \mathrm{i}} \rangle$, and we integrate the above equation for $\beta_x = \dot{x}$ to find $x$:
\begin{equation*}
\int_0^{ct} \dfrac{1}{c} \, \dfrac{\dd x}{\dd t} \, c \, \dd t = \int_0^{ct} \dfrac{\omega_x}{\sqrt{1 + \omega^2_x}} \, c \, \dd t .
\end{equation*}
Use substitution, with $u = 1 + \omega^2_x$ and $\dd u = 2 \mkern1mu \omega_x \mkern2mu \zeta_{0 \mkern2mu x} \, c \, \dd t$ (by Equation \ref{eq:ce}):
\begin{equation*}
\begin{split}
x (ct) - x_{\mathrm{i}} &= \dfrac{1}{2 \zeta_{0 \mkern2mu x}} \int_{ct = 0}^{ct = ct} u^{-1/2} \, \dd u \\[5pt]
&= \dfrac{1}{ \zeta_{0 \mkern2mu x}} \, \sqrt{1 + \omega^2_x } \, \Big \vert^{ct = ct}_{ct = 0}
\end{split}
\end{equation*}
(division by a vector is illegal---this is why we switched to component notation). But $\gamma = \sqrt{1 + \omega^2_x}$, so that's:
\begin{equation}\label{eq:x}
x (ct) = x_{\mathrm{i}} + \dfrac{ \gamma (ct) - \gamma_{\mkern1mu \mathrm{i}} }{ \zeta_{0 \mkern2mu x}} \qquad \textrm{\footnotesize{ (for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern1mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$)}}.
\end{equation}

We can also express the accelerating traveler's \emph{proper time} as a function of the inertial observer's coordinate time. By time dilation and $\gamma = \sqrt{1 + \omega^2_x}$:
\begin{equation*}
\begin{aligned}
\int^{ct}_0 \dfrac{\dd t_0}{\dd t}  \, c \, \dd t &= \int^{ct}_0 \dfrac{1}{\gamma} \, c \, \dd t \\[5pt]
\int^{ct_0}_{ct_{0 \mkern1mu \mathrm{i}}} c \, \dd t_0 &= \int^{ct}_0 \dfrac{c \, \dd t}{\sqrt{1 + \omega^2_x}}
\end{aligned}
\end{equation*}
($ct_{0 \mkern2mu \mathrm{i}} = ct_0 (ct = 0)$ need not be zero!). Use $\dd \omega_x = \zeta_{0 \mkern2mu x} \, c \, \dd t$ (Equation \ref{eq:ce}):
\begin{equation*}
ct_0 (ct) - ct_{0 \mkern2mu \mathrm{i}} = \dfrac{1}{\zeta_{0 \mkern2mu x}} \int^{\omega_x}_{\omega_{x \mkern1mu \textrm{i} }} \dfrac{\dd \omega_x}{\sqrt{1 + \omega^2_x}},
\end{equation*}
and since $1/\sqrt{1 + q^2}$ is the derivative of $\sinh^{-1} q$:
\begin{equation*}
ct_0 (ct) = ct_{0 \mkern2mu \mathrm{i}} + \dfrac{\sinh^{-1} \omega_x (ct) - \sinh^{-1} \omega_{x \mkern2mu \textrm{i} }}{ \zeta_{0 \mkern2mu x}},
\end{equation*}
or:
\begin{equation}\label{eq:pt}
ct_0 (ct) = ct_{0 \mkern2mu \mathrm{i}} + \dfrac{ \phi_x (ct) - \phi_{x \mkern2mu \mathrm{i} }}{ \zeta_{0 \mkern2mu x}}  \qquad \textrm{\footnotesize{ (for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern1mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$)}}.
\end{equation}


To sum up, for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern2mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$, our ``master" function of coordinate time is Equation \ref{eq:ce}:\footnote{If $\zeta_{0 \mkern1mu x}$ isn't constant, then Equation \ref{eq:ce} becomes $\omega_x (ct) = \omega_{x \mkern1mu \textrm{i} } + \int_0^{ct} \zeta_{0 \mkern1mu x} \, c \, \dd t$, but all of the other equations we derived in this section remain the selfsame functions of $\omega_x (ct)$ (though they become different functions of $ct$).}
\begin{equation*}
\omega_x (ct) = \omega_{x \mkern2mu \textrm{i} } + \zeta_{0 \mkern2mu x} \mkern2mu ct ,
\end{equation*}
and the kinematic functions of $ct$ we built on it are:
\begin{equation*}
\begin{aligned}
\phi_x (ct) &= \sinh^{-1} \omega_x (ct) , \\[5pt]
\gamma (ct) &= \cosh \phi_x(ct) = \sqrt{1 + \omega^2_x (ct)} , \\[3pt]
\beta_x (ct) &= \tanh \phi_x(ct) = \dfrac{\omega_x (ct)}{\sqrt{1 + \omega^2_x (ct)}}
\end{aligned}
\end{equation*}
(from relations we already knew), and the new Equations \ref{eq:x} and \ref{eq:pt} (specific to the given conditions):
\begin{equation*}
\begin{aligned}
x (ct) &= x_{\mathrm{i}} + \dfrac{ \gamma (ct) - \gamma_{\mkern1mu \mathrm{i}} }{ \zeta_{0 \mkern2mu x}} \\[5pt]
ct_0 (ct) &= ct_{0 \mkern2mu \mathrm{i}} + \dfrac{ \phi_x (ct) - \phi_{x \mkern2mu \mathrm{i} }}{ \zeta_{0 \mkern2mu x}} .
\end{aligned}
\end{equation*}
``Ideal" initial conditions for Equations \ref{eq:x} and \ref{eq:pt} are of course $x_{\mathrm{i}} = \gamma_{\mkern1mu \mathrm{i}} / \zeta_{0 \mkern2mu x}$ and $ct_{0 \mkern2mu \mathrm{i}} = \phi_{x \mkern2mu \mathrm{i} } / \zeta_{0 \mkern2mu x}$, leaving $x = \gamma / \zeta_{0 \mkern2mu x}$ and $ct_0 = \phi_x / \zeta_{0 \mkern2mu x}$. For the whole set of equations, the ``ideal" initial condition is $\omega_{\mkern1mu \textrm{i}} = 0$, so that every $\omega_x (ct)$ can be replaced with $\zeta_{0 \mkern2mu x} \mkern2mu ct$.\footnote{Note that if all three conditions are met, then $ct_{0 \mkern2mu \mathrm{i}} = 0$ but $x_{\mathrm{i}} = 1 / \zeta_{0 \mkern1mu x}$ [$\neq 0$].} Also, if our $x$-components are all non-negative, then we can replace them with their corresponding vector magnitudes (drop the $x$ subscripts). The exception is $x(ct)$ itself---we can't replace it with $r(ct)$ because $\vv r$ has its tail at the origin, and we haven't required that the (constant) $y$- and $z$-coordinates be zero. Putting that all together, ``ideally":
\begin{equation*}
\begin{aligned}
\omega (ct) &= \zeta_0 \mkern2mu ct \\[3pt]
\phi (ct) &= \sinh^{-1} \big( \zeta_0 \mkern2mu ct \big) \\[5pt]
\gamma (ct) &= \sqrt{1 + \big( \zeta_0 \mkern2mu ct \big)^2} \\[5pt]
\beta (ct) &= \dfrac{\zeta_0 \mkern2mu ct}{\sqrt{1 + \big( \zeta_0 \mkern2mu ct \big)^2}} \\[5pt]
x (ct) &= \dfrac{ \gamma (ct) }{ \zeta_0} = \dfrac{\sqrt{1 + \big( \zeta_0 \mkern2mu ct \big)^2}}{ \zeta_0} \\[5pt]
ct_0 (ct) &= \dfrac{ \phi (ct) }{ \zeta_0} = \dfrac{\sinh^{-1} \big( \zeta_0 \mkern2mu ct \big)}{ \zeta_0}.
\end{aligned}
\end{equation*}


\subsection{Functions of Proper Time (Team Rapidity)}

For constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern2mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$ (the same rectilinear case we dealt with in the previous section), we can find kinematic functions of the accelerating traveler's \emph{proper} time by starting with Equation \ref{eq:pa3} (for $\mathring{\vvphi} \parallel \vvphi$):
\begin{equation*}
\mathring{\phi}_x = \zeta_{0 \mkern2mu x}
\end{equation*}
and integrating:
\begin{equation*}
\int_0^{ct_0} \mathring{\phi}_x \, c \, \dd t_0 = \int_0^{ct_0}  \zeta_{0 \mkern2mu x} \, c \, \dd t _0 ,
\end{equation*}
giving
\begin{equation}\label{eq:ra}
\phi_x (ct_0) = \phi_{x \mkern2mu \textrm{i} } + \zeta_{0 \mkern2mu x} \mkern2mu ct_0 \qquad \textrm{\footnotesize{ (for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern1mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$)}}.
\end{equation}

Before going further, a word of caution. The initial value $\phi_{x \mkern2mu \textrm{i} }$ in Equation \ref{eq:ra} is \emph{not} the $\phi_{x \mkern2mu \textrm{i} }$ that appeared in Equation \ref{eq:pt}. One is $\phi_x (ct_0 = 0)$, the other is $\phi_x (ct = 0)$, and they're only equivalent if we ``start the clock" on coordinate time and proper time simultaneously. Same goes for any such pair of identical-\emph{looking} initial values from this section and the last---though we use the same notation for them, they're different quantities in the general case. The function notation clarifies what's what.

Now apply Equations \ref{eq:rx} to Equation \ref{eq:ra}:
\begin{equation*}
\begin{aligned}
\gamma (ct_0) &= \cosh{\left( \phi_{x \mkern2mu \textrm{i} } + \zeta_{0 \mkern2mu x} \mkern2mu ct_0 \right)} \\[3pt]
\beta_x (ct_0) &= \tanh{\left( \phi_{x \mkern2mu \textrm{i} } + \zeta_{0 \mkern2mu x} \mkern2mu ct_0 \right)} \\[3pt]
\omega_x (ct_0) &= \sinh{\left( \phi_{x \mkern2mu \textrm{i} } + \zeta_{0 \mkern2mu x} \mkern2mu ct_0 \right)} .
\end{aligned}
\end{equation*}
Because $\mathring{x} = \omega_x$, we integrate the above equation for $\omega_x$ to find $x(ct_0)$:
\begin{equation*}
\begin{split}
\int^{ct_0}_0 \dfrac{1}{c} \, \dfrac{\dd x}{\dd t_0} \, c \, \dd t_0 &= \int^{ct_0}_0 \sinh{\phi_x} \, c \, \dd t_0 \\[6pt]
x(ct_0) - x_{\mathrm{i}} &= \dfrac{1}{\zeta_{0 \mkern2mu x}} \int^{\phi_x}_{\phi_{x \mkern1mu \textrm{i} }} \sinh{\phi_x} \, \dd \phi_x
\end{split}
\end{equation*}
(by Equation \ref{eq:ra}). Since $\sinh{q}$ is the derivative of $\cosh{q}$, that's:
\begin{equation}\label{eq:x2}
x(ct_0) = x_{\mathrm{i}} + \dfrac{\gamma(ct_0) - \gamma_{\mkern1mu \mathrm{i}}}{ \zeta_{0 \mkern2mu x}} \qquad \textrm{\footnotesize{ (for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern1mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$)}}
\end{equation}
($\gamma = \cosh{\phi_x}$). Last, we can express our inertial observer's coordinate time as a function of our accelerating traveler's proper time. By time dilation and Equation \ref{eq:ra}:
\begin{equation*}
\begin{split}
\int^{ct_0}_0 \dfrac{\dd t}{\dd t_0} \, c \, \dd t_0 &= \int^{ct_0}_0 \gamma \, c \, \dd t_0 \\[6pt]
\int^{ct}_{ct_{\mathrm{i}}} c \, \dd t &= \dfrac{1}{ \zeta_{0 \mkern2mu x}} \int^{\phi_x}_{\phi_{x \mkern1mu \textrm{i} }} \cosh{\phi_x} \, \dd \phi_x
\end{split}
\end{equation*}
($ct_{\mathrm{i}} = ct (ct_0 = 0)$ need not be zero). Since $\cosh q$ is the derivative of $\sinh q$:
\begin{equation}\label{eq:ct}
ct(ct_0) = ct_{\mathrm{i}} + \dfrac{\omega_x (ct_0) - \omega_{x \mkern2mu \textrm{i} }}{ \zeta_{0 \mkern2mu x}} \qquad \textrm{\footnotesize{ (for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern1mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$)}}
\end{equation}
($\omega_x = \sinh{\phi_x}$).

To recap, for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern2mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$, our ``master" function of proper time is Equation \ref{eq:ra}:\footnote{If $\zeta_{0 \mkern1mu x}$ isn't constant, then Equation \ref{eq:ra} becomes $\phi_x (ct_0) = \phi_{x \mkern1mu \textrm{i} } + \int_0^{ct_0} \zeta_{0 \mkern1mu x} \, c \, \dd t_0$, but all of the other equations we derived in this section remain the selfsame functions of $\phi_x (ct_0)$ (though they become different functions of $ct_0$).}
\begin{equation*}
\phi_x (ct_0) = \phi_{x \mkern2mu \textrm{i} } + \zeta_{0 \mkern2mu x} \mkern2mu ct_0 ,
\end{equation*}
and the kinematic functions of $ct_0$ we built on it are:
\begin{equation*}
\begin{aligned}
\gamma (ct_0) &= \cosh{\phi_x (ct_0)}  , \\[3pt]
\beta_x (ct_0) &= \tanh{\phi_x (ct_0)}  , \\[3pt]
\omega_x (ct_0) &= \sinh{\phi_x (ct_0)} 
\end{aligned}
\end{equation*}
(from relations we already knew), and the new Equations \ref{eq:x2} and \ref{eq:ct} (specific to the given conditions):
\begin{equation*}
\begin{aligned}
x(ct_0) &= x_{\mathrm{i}} + \dfrac{\gamma(ct_0) - \gamma_{\mkern1mu \mathrm{i}}}{ \zeta_{0 \mkern2mu x}} \\[5pt]
ct(ct_0) &= ct_{\mathrm{i}} + \dfrac{\omega_x (ct_0) - \omega_{x \mkern2mu \textrm{i} }}{ \zeta_{0 \mkern2mu x}} .
\end{aligned}
\end{equation*}
They're a tad sleeker than their coordinate-time counterparts, as there's no $\sinh^{-1}$ function playing middleman here. ``Ideal" initial conditions for Equations \ref{eq:x2} and \ref{eq:ct} are $x_{\mathrm{i}} = \gamma_{\mkern1mu \mathrm{i}} / \zeta_{0 \mkern2mu x}$ and $ct_{\mathrm{i}} = \omega_{x \mkern2mu \textrm{i} } / \zeta_{0 \mkern2mu x}$, leaving $x = \gamma / \zeta_{0 \mkern2mu x}$ and $ct = \omega_x / \zeta_{0 \mkern2mu x}$. If also the initial rapidity is zero, then $\phi_x (ct_0) = \zeta_{0 \mkern2mu x} \mkern2mu ct_0$, $ct_{\mathrm{i}} = 0$, and $x_{\mathrm{i}} = 1 / \zeta_{0 \mkern2mu x}$, and the initial values in this section (at $ct_0 = 0$) \emph{are} equivalent to the corresponding initial values in the previous section (at $ct = 0$). Finally, if all $x$-components are non-negative, then we can replace them with the appropriate vector magnitudes (except for $x (ct_0)$ itself, since the $y$- and $z$-coordinates---though constant---might not be zero, and $\vv r$ has its tail at the origin). So the ``ideal" versions of our proper-time functions are:
\begin{equation*}
\begin{aligned}
\phi (ct_0) &= \zeta_0 \mkern2mu ct_0 \\[3pt]
\gamma (ct_0) &= \cosh \big( \zeta_0 \mkern2mu ct_0 \big) \\[3pt]
\beta (ct_0) &= \tanh \big( \zeta_0 \mkern2mu ct_0 \big) \\[3pt]
\omega (ct_0) &= \sinh \big( \zeta_0 \mkern2mu ct_0 \big) \\[3pt]
x(ct_0) &= \dfrac{\gamma(ct_0)}{ \zeta_0 } = \dfrac{\cosh \big( \zeta_0 \mkern2mu ct_0 \big)}{ \zeta_0 } \\[5pt]
ct(ct_0) &= \dfrac{\omega (ct_0) }{ \zeta_0} = \dfrac{\sinh \big( \zeta_0 \mkern2mu ct_0 \big)}{ \zeta_0 }.
\end{aligned}
\end{equation*}



\subsection{Hyperbolic Motion}

The rectilinear motion with constant proper acceleration we've been studying goes by another name: \textbf{hyperbolic motion} (through spacetime). To see why, let's use our lovely functions of proper time (though we could obtain similar results with Equations \ref{eq:ce} and \ref{eq:x}).

Start with Equations \ref{eq:x2} and \ref{eq:ct}, and rearrange them:
\begin{equation*}
x(ct_0) - \left( x_{\mathrm{i}} - \dfrac{\gamma_{\mkern1mu \mathrm{i}}}{ \zeta_{0 \mkern2mu x}} \right) = \dfrac{\gamma(ct_0) }{ \zeta_{0 \mkern2mu x}} \qquad \qquad  ct(ct_0) - \left( ct_{\mathrm{i}} - \dfrac{\omega_{x \mkern2mu \textrm{i} }}{ \zeta_{0 \mkern2mu x}} \right) = \dfrac{\omega_x (ct_0) }{ \zeta_{0 \mkern2mu x}} .
\end{equation*}
The right sides of these expressions stand out: by Equation \ref{eq:25}, we know that the square of the first minus the square of the second is just $1 / \zeta^2_{0 \mkern2mu x}$, so we can eliminate two variables by squaring and subtracting our equations. Here:
\begin{equation}\label{eq:hm}
\begin{aligned}
\left[ x(ct_0) - \left( x_{\mathrm{i}} - \dfrac{\gamma_{\mkern1mu \mathrm{i}}}{\zeta_{0 \mkern2mu x}} \right) \right] ^2 &- \left[ ct(ct_0) - \left( ct_{\mathrm{i}} - \dfrac{\omega_{x \mkern2mu \textrm{i} }}{ \zeta_{0 \mkern2mu x}} \right) \right] ^2 = \dfrac{1}{ \zeta^2_{0 \mkern2mu x}} \\[8pt]
& \textrm{\footnotesize{ (for constant $\vvzeta_{\mkern1mu 0} = \langle \zeta_{0 \mkern1mu x} , 0, 0 \rangle$ and $\vvzeta \parallel \vvbeta$)}}.
\end{aligned}
\end{equation}
Since the only variables left are the spacetime coordinates $x$ and $ct$ (parameterized by $ct_0$), Equation \ref{eq:hm} is a hyperbola in the $ct$/$x$-plane. One of its two branches traces the accelerating traveler's world line: if $\zeta_{0 \mkern2mu x} > 0$, then it's the branch that opens in the positive $x$-direction; if $\zeta_{0 \mkern2mu x} < 0$, then it's the branch that opens in the negative $x$-direction.\footnote{To recover the ``parabolic motion" we know from Newtonian mechanics, we can apply the Taylor series $\cosh{q} = 1 + q^2/2! + q^4/4! + q^6/6! + \dots$ to Equation \ref{eq:x2} ($\gamma = \cosh{\phi_x}$). For sufficiently small $| \phi_x | = | \phi_{x \mkern1mu \textrm{i} } + \zeta_{0 \mkern1mu x} \mkern2mu ct_0 |$ and $| \phi_{x \mkern1mu \textrm{i} } |$:
\begin{equation*}
\begin{split}
x(ct_0) &\approx x_{\mathrm{i}} + \dfrac{ \left[ 1 + \frac{1}{2} \big( \phi_{x \mkern1mu \textrm{i} } + \zeta_{0 \mkern1mu x} \mkern2mu ct_0 \big)^2 \right] \vphantom{\big|_{\mathstrut_{\mathstrut_{A}}}}  - \Bigl[ 1 + \frac{1}{2} \phi^2_{x \mkern1mu \textrm{i} } \Bigr] }{ \zeta_{0 \mkern1mu x}}\\
&=  x_{\mathrm{i}} \,  + \, \phi_{x \mkern1mu \textrm{i} } \mkern2mu ct_0 \, + \, \dfrac{1}{2} \mkern1mu \zeta_{0 \mkern1mu x} (ct_0)^2 .
\end{split}
\end{equation*}
With $\zeta_{0 \mkern1mu x} \approx \zeta_{x}$ and $ct_0 \approx ct$ (set $ct_{\mathrm{i}} = 0$), as well as $\phi_{x \mkern1mu \textrm{i} } \approx \beta_{x \mkern1mu \mathrm{i} }$ (by Equations \ref{eq:rx} and the Taylor series $\tanh^{-1}{q} = q + q^3/3 + q^5/5 + \dots$), the parabola reduces to the familiar
\begin{equation*}
x(ct_0) \approx x_{\mathrm{i}} \, + \, \beta_{x \mkern1mu \mathrm{i} } \mkern2mu ct \, + \, \dfrac{1}{2} \mkern1mu \zeta_{x} (ct)^2 \left[ = x_{\mathrm{i}} \, + \, v_{x \mkern1mu \mathrm{i} } \mkern2mu t \, + \, \dfrac{1}{2} \mkern1mu a_{x} t^2 \right] .
\end{equation*}
}

Equation \ref{eq:hm} is an \emph{equilateral} (or \emph{rectangular}) hyperbola, meaning that its asymptotes have slopes $c \mkern.5mu \Delta t / \Delta x = \pm \, 1$ and intersect at right angles at the hyperbola's center. Specifically, the asymptotes are the lines
\begin{equation*}
ct = \left( ct_{\mathrm{i}} - \dfrac{\omega_{x \mkern2mu \textrm{i} }}{ \zeta_{0 \mkern2mu x}} \right) \pm \left[  x - \left( x_{\mathrm{i}} - \dfrac{\gamma_{\mkern1mu \mathrm{i}}}{ \zeta_{0 \mkern2mu x}} \right) \right],
\end{equation*}
and the hyperbola's center is the event in spacetime whose coordinates in this frame are $ct = ct_{\mathrm{i}} - \omega_{x \mkern2mu \textrm{i} } / \zeta_{0 \mkern2mu x}$ and $x = x_{\mathrm{i}} - \gamma_{\mkern1mu \mathrm{i}} / \zeta_{0 \mkern2mu x}$ (and whatever the accelerating traveler's $y$ and $z$ coordinates are---they're constant, remember). 

There are some intriguing consequences to all this. First, notice that Equation \ref{eq:hm} tells us the spacetime interval $\Delta s$ between a given event on the accelerating traveler's world line and the event that is the hyperbola's center (henceforth the ``center event"):
\begin{equation*}
(\Delta s)^2 = (c \mkern.5mu \Delta t)^2 - (\Delta x)^2 = - \dfrac{1}{ \zeta^2_{0 \mkern2mu x}},
\end{equation*}
where $c \mkern.5mu \Delta t = ct(ct_0) - (ct_{\mathrm{i}} - \omega_{x \mkern2mu \textrm{i} } / \zeta_{0 \mkern2mu x})$ and $\Delta x = x(ct_0) - (x_{\mathrm{i}} - \gamma_{\mkern1mu \mathrm{i}} / \zeta_{0 \mkern2mu x})$ are the differences in the time and space coordinates of the events (again, as measured in any of the infinitely many inertial frames for which the traveler's $y$ and $z$ coordinates don't change). This interval is spacelike, since its square is negative, and it's also constant. The invariant $\Delta s / \mathrm{i} = |1 / \zeta_{0 \mkern2mu x}|$ is therefore the proper distance between the center event and \emph{every event on the traveler's world line}! What's more, $ct(ct_0) = ct_{\mathrm{i}} - \omega_{x \mkern2mu \textrm{i} } / \zeta_{0 \mkern2mu x}$ when the traveler is momentarily at rest in any of these frames (Equation \ref{eq:ct} with $\omega_x = 0$), giving $c \mkern.5mu \Delta t = 0$ and $(\Delta s)^2 = - (\Delta x)^2$ at that instant. This means that the center event is simultaneous with the traveler's ``turnaround event" (graphically this is obvious: the turnaround event is one of the hyperbola's vertices, and the vertices and the center lie on the major axis $ct = ct_{\mathrm{i}} - \omega_{x \mkern2mu \textrm{i} } / \zeta_{0 \mkern2mu x}$). But there's an instantaneous rest frame corresponding to \emph{every} event on the traveler's world line; they \emph{all} get their ``turn" as the turnaround event. So what we're really saying is that for each of the traveler's successive instantaneous rest frames, the center event is simultaneous with the traveler's coming to momentary rest. As far as the traveler is concerned, the center event is always happening ``right now" at a distance $|1 / \zeta_{0 \mkern2mu x}|$ in the direction opposite to the acceleration.\footnote{When we speak of \emph{right now} and \emph{at a distance} here, we're assuming that the accelerating traveler always adopts the $(ct, x, y, z)$ coordinates of the current instantaneous rest frame. We'll soon find, however, that instead of continuously adopting new coordinate systems corresponding to successive inertial frames, the traveler with constant proper acceleration may choose to adopt a single coordinate system that's naturally suited to the accelerating frame itself.}

If the center event's perpetual ``now-and-there-ness" for the accelerating traveler is difficult to take in, it may help to confirm it with a Lorentz transformation of $c \mkern.5mu \Delta t $ and $\Delta x$ from the preceding paragraph (i.e., the differences in the $ct$ and $x$ coordinates of the center event and an arbitrary event on the traveler's world line, as reckoned in one of our qualifying frames). First, by Equations \ref{eq:ct} and \ref{eq:x2}:
\begin{equation*}
c \mkern.5mu \Delta t = ct(ct_0) - \left( ct_{\mathrm{i}} - \dfrac{\omega_{x \mkern2mu \textrm{i} }}{ \zeta_{0 \mkern2mu x}} \right) = \dfrac{\omega_x}{ \zeta_{0 \mkern2mu x}}
\end{equation*}
and
\begin{equation*}
\Delta x = x(ct_0) - \left( x_{\mathrm{i}} - \dfrac{\gamma_{\mkern1mu \textrm{i} }}{ \zeta_{0 \mkern2mu x}} \right) = \dfrac{\gamma}{ \zeta_{0 \mkern2mu x}} .
\end{equation*}
Now use Equations \ref{eq:lt2} to boost to the traveler's (primed) instantaneous rest frame, noting that the boost parameters are $\gamma_{\mathrm{rel}} = \gamma$ and $\omega_{\mathrm{rel}} = \pm \, \omega_x$ (negative if $\omega_x < 0$, in which case this is an ``inverse" transformation and the $\omega_{\textrm{rel}}$ terms below take the $+$ sign):
\begin{equation*}
c \mkern.5mu \Delta t^\prime = \gamma_{\textrm{rel}} \, c \mkern.5mu \Delta t \, \pm \, \omega_{\textrm{rel}} \, \Delta x = \gamma \, \dfrac{\omega_x}{ \zeta_{0 \mkern2mu x}} - \omega_x \, \dfrac{\gamma}{ \zeta_{0 \mkern2mu x}} = 0
\end{equation*}
and
\begin{equation*}
\Delta x^\prime = \gamma_{\textrm{rel}} \, \Delta x \, \pm \, \omega_{\textrm{rel}} \, c \mkern.5mu \Delta t = \gamma \, \dfrac{\gamma}{ \zeta_{0 \mkern2mu x}} - \omega_x \, \dfrac{\omega_x}{ \zeta_{0 \mkern2mu x}} = \dfrac{1}{ \zeta_{0 \mkern2mu x}}
\end{equation*}
(by Equation \ref{eq:25}). So we see again that in each instantaneous rest frame, the center event is simultaneous with the traveler's coming to momentary rest and occurs a distance $|1 / \zeta_{0 \mkern2mu x}|$ away from the traveler.

Next, consider the asymptotes. With their slopes of $\pm \, 1$, they correspond to \emph{lightlike} world lines. In fact, let's go ahead and imagine that they \emph{are} the world lines of two pulses of light that were emitted infinitely long ago---one from far off in the negative $x$-direction and aimed in the positive $x$-direction, the other vice versa, and both with constant $y$ and $z$ coordinates identical to the accelerating traveler's. The pulses then intersect at the center event $(ct_{\mathrm{i}} - \omega_{x \mkern2mu \textrm{i} } / \zeta_{0 \mkern2mu x}, \, x_{\mathrm{i}} - \gamma_{\mkern1mu \mathrm{i}} / \zeta_{0 \mkern2mu x}, \, y_{\mathrm{i}}, \, z_{\mathrm{i}})$. Asymptotes being lines that the hyperbola approaches but never reaches, our qualifying inertial observers would say that one pulse approaches but never reaches the traveler, while the other was arbitrarily \emph{close} to the traveler in the distant past but always a smidge ahead. And as far as the accelerating traveler is concerned, the ``eternal" center event a fixed distance $|1 / \zeta_{0 \mkern2mu x}|$ away constitutes the pulses' entire existence!\footnote{Again, we're assuming that the traveler is using the time and space coordinates of the current instantaneous rest frame. There are other options.} They always were and always will be intersecting. We arrive at the inescapable conclusion that \emph{a traveler with constant proper acceleration will ``outrun" a pulse of light that's approaching from the direction opposite to the acceleration}, provided that the light is at least a proper distance $|1 / \zeta_{0 \mkern2mu x}|$ away.

Since information can travel no faster than $\beta = 1$, it also follows that information about any event \emph{beyond} the ``approaching" asymptote (the one that comes arbitrarily close to the relevant hyperbola branch in the \emph{future}) will never reach the ever-accelerating traveler. That asymptote then represents an unphysical but nonetheless meaningful ``boundary" in spacetime---called the \textbf{future Rindler horizon}---that ``splits" the $ct$/$x$-plane into two regions: a ``knowable" region populated by events whose existence the traveler may learn of, and an ``unknowable" region populated by events whose existence the traveler must remain forever ignorant of.\footnote{The future Rindler horizon (named after Wolfgang Rindler) is quite like the \textbf{event horizon} of a \textbf{black hole}, though the former's existence depends on the traveler's ``choice" to continue accelerating, whereas the latter is a gravitational phenomenon.} The other asymptote (the one that was arbitrarily close to the accelerating traveler in the \emph{past}) is the \textbf{past Rindler horizon}, and it splits the $ct$/$x$-plane into two regions as well: a ``pingable" region that can receive light signals from the traveler, and an ``unpingable" region that cannot. Altogether the asymptotes quarter the $ct$/$x$-plane into four \textbf{Rindler wedges}: one contains the traveler's world line and is both knowable and pingable; a second is knowable but unpingable (it's behind the past horizon but not the future horizon); a third is pingable but unknowable (behind the future horizon but not the past horizon); and the last is both unknowable and unpingable (behind both Rindler horizons).


\subsection{Born Rigidity}



\subsection{Rindler Coordinates}

%So far in this paper, we've \emph{only} used ($ct$ + Cartesian) coordinate to describe events and four-vector components. And after this little section, we'll continue that practice. But there are other possibilities! For example, we can use ($ct$ + spherical) or ($ct$ + cylindrical) coordinates, and we can figure out how those spatial coordinates transform under a Lorentz boost by combining our knowledge of how Cartesian coordinates transform with our knowledge of how Cartesian coordinates are related to spherical or cylindrical coordinates.
%
%Now take it one step further: if we can ``keep" the $ct$ coordinate but ``mix" the Cartesian coordinates as we've just described, why can't we instead \emph{include} the $ct$ coordinate in the mixing? The answer is, we can! And for our traveler with constant proper acceleration, this is actually the natural choice. Let's assume ``ideal" conditions, so that a qualifying inertial observer writes the traveler's world line in ($ct$ + Cartesian) coordinates like this:
%\begin{equation*}
%x(ct_0) = \dfrac{\cosh \big( \zeta_0 \mkern2mu ct_0 \big)}{ \zeta_0 } \qquad \qquad ct(ct_0) = \dfrac{\sinh \big( \zeta_0 \mkern2mu ct_0 \big)}{ \zeta_0 }.
%\end{equation*}
%What we'd like to do is mix the ``old" $ct$ and $x$ coordinates into a pair of ``new" ones in such a way that one of the new coordinates \emph{remains constant} along the traveler's world line (the traveler will regard this as a kind of ``position" coordinate) while the other \emph{changes proportionally with the invariant $ct_0$} (a kind of ``time" coordinate). The idea is that in the new coordinate system, the traveler will be ``at rest," and ``time" will be what the traveler's wristwatch logs---that's why it's the ``natural choice." There's more than one way to accomplish this. Here is maybe the simplest, called \textbf{Rindler coordinates}:
%\begin{equation*}
%X = \sqrt{x^2 - (ct)^2} \qquad \qquad cT = \dfrac{1}{\zeta_0} \, \tanh^{-1} \dfrac{ct}{x} ,
%\end{equation*}
%with the ``inverse" transformation:
%\begin{equation*}
%x = X \cosh \big( \zeta_0 \mkern2mu cT \big) \qquad \qquad ct = X \sinh \big( \zeta_0 \mkern2mu cT \big),
%\end{equation*}
%($Y = y$ and $Z = z$). Note that the Rindler ``time" coordinate $cT$ is just the traveler's proper time $ct_0$, and that along the traveler's world line the ``position" coordinate $X$ is constant (equals $1 / \zeta_0$).

%The Rindler wedge (the knowable and pingable part of the $ct$/$x$-plane) is of particular interest. Let's now stipulate that our ``ideal" initial conditions hold ($x_{\mathrm{i}} = \gamma_{\mkern1mu \mathrm{i}} / \zeta_{0 \mkern2mu x}$, $ct_{\mathrm{i}} = \omega_{x \mkern2mu \textrm{i} } / \zeta_{0 \mkern2mu x}$, and $\phi_{x \mkern2mu \textrm{i} } = 0$, with the traveler's acceleration in the positive $x$-direction), and also that $y_{\mathrm{i}} = z_{\mathrm{i}} = 0$. Then the center event has coordinates $ct = ct_{\mathrm{i}} - \omega_{x \mkern2mu \textrm{i} } / \zeta_{0 \mkern2mu x} = 0$ and $x = x_{\mathrm{i}} - \gamma_{\mkern1mu \mathrm{i}} / \zeta_{0 \mkern2mu x} = 0$, and the accelerating traveler's four-position in component form is: 
%\begin{equation*}
%\vv R (ct_0) = \left \langle \dfrac{\sinh \big( \zeta_0 \mkern2mu ct_0 \big)}{ \zeta_0 }, \, \dfrac{\cosh \big( \zeta_0 \mkern2mu ct_0 \big)}{ \zeta_0 } , \, 0, \, 0 \right \rangle .
%\end{equation*}
%Actually, if all this is true for one inertial frame, then it's true for \emph{every} frame that's in standard configuration with it, since all such frames share a spacetime origin that is the center event. [...]

\section{Electromagnetism}\label{sec:rem}

Near the beginning of this document, we took a quantum shortcut: instead of using classical electromagnetism to show that the energy of light transforms between frames in the same manner as its frequency, we used the Planck--Einstein relation. This wasn't quite ahistorical, but it wasn't entirely satisfactory, either, and we pledged that we'd come back and redo it the ``right" way, without invoking the photon model. The time has come to settle that unfinished business. We'll start by formulating electrodynamics from scratch using four-vectors---no explicit tensor analysis, no index gymnastics, no differential forms.\footnote{At the end there's a supplemental foray into dyadics, which is kind of like ``tensors lite" (see Section \ref{ssec:dy}).} Then we'll derive the transformation law for the energy of a light wave the way that Einstein did it (more or less). But first we'll need to go over some spacetime vector calculus. Familiarity with Euclidean vector calculus is assumed, though we'll keep our steps transparent and point out the identities we use.


\subsection{The Math (Spacetime Vector Calculus)}


\subsubsection{The Four-Del}\label{sssec:fd}

We'll need a four-vector equivalent of the ``del" (``nabla") symbol:
\begin{equation*}
\del = \left \langle \dfrac{\partial}{\partial x}, \, \dfrac{\partial}{\partial y}, \, \dfrac{\partial}{\partial z} \right \rangle .
\end{equation*}
Let's notate it $\partialup$ (``vectorized" $\partial$ symbol). A natural guess is:
\begin{equation*}
\partialup \stackrel{?}{=} \left \langle \dfrac{1}{c} \, \dfrac{\partial}{\partial t} , \, \dfrac{\partial}{\partial x}, \, \dfrac{\partial}{\partial y}, \, \dfrac{\partial}{\partial z} \right \rangle = \left \langle \dfrac{1}{c} \, \dfrac{\partial}{\partial t} , \, \del \right \rangle ,
\end{equation*}
but do these components transform Lorentzianly under a boost? If not, then $\partialup$ isn't a four-vector, and (say) dotting it with a four-vector field won't have the intended effect of producing a scalar field whose value at each event in spacetime is invariant (dotting with it wouldn't even be defined!). To find out, use the multivariable chain rule and Equations \ref{eq:lt} (standard configuration, so $ct^\prime$ and $x^\prime$ are functions of $ct$ and $x$, and vice versa):
\begin{equation*}
\begin{split}
\dfrac{\partial}{\partial (ct^\prime)} &= \dfrac{\partial (ct)}{\partial (ct^\prime)} \, \dfrac{\partial}{\partial (ct)} \, + \, \dfrac{\partial x}{\partial (ct^\prime)} \, \dfrac{\partial}{\partial x} = \gamma \left( \dfrac{\partial}{\partial (ct)} + \beta \, \dfrac{\partial}{\partial x} \right) \\[3pt]
\dfrac{\partial}{\partial x^\prime} &= \dfrac{\partial x}{\partial x^\prime} \, \dfrac{\partial}{\partial x} \, + \, \dfrac{\partial (ct)}{\partial x^\prime} \, \dfrac{\partial}{\partial (ct)}  = \gamma \left( \dfrac{\partial}{\partial x} + \beta \, \dfrac{\partial}{\partial (ct)}  \right) \\[3pt]
\dfrac{\partial}{\partial y^\prime} &= \dfrac{\partial}{\partial y} \\[3pt]
\dfrac{\partial}{\partial z^\prime} &= \dfrac{\partial}{\partial z} ,
\end{split}
\end{equation*}
That's not the Lorentz transformation; it's the \emph{inverse} Lorentz transformation! The \emph{Lorentz} transformation would then get us the unprimed components from the primed ones. This is precisely the opposite of how a four-vector's components transform.\footnote{\label{fn:cv}An object in Minkowski spacetime whose components transform in this ``backward" way is called a \textbf{four-\emph{co}vector}. Covectors are also called one-forms, dual vectors, or covariant vectors (if the latter, then vectors are called ``contravariant vectors").} If we simply change the sign of $\del$, then the components transform like those of a four-vector:
\begin{equation*}
\begin{split}
\dfrac{\partial}{\partial (ct^\prime)} &= \gamma \left[ \dfrac{\partial}{\partial (ct)} - \beta \left(- \dfrac{\partial}{\partial x} \right) \right] \\[3pt]
- \dfrac{\partial}{\partial x^\prime} &= \gamma \left[ \left( - \dfrac{\partial}{\partial x} \right) - \beta \, \dfrac{\partial}{\partial (ct)}  \right] \\[3pt]
- \dfrac{\partial}{\partial y^\prime} &= - \dfrac{\partial}{\partial y} \\[3pt]
- \dfrac{\partial}{\partial z^\prime} &= - \dfrac{\partial}{\partial z} .
\end{split}
\end{equation*}
There we go. Our \textbf{four-del} symbol is:
\begin{equation}\label{eq:fn}
\boxed{
\begin{aligned}
\partialup &\equiv \left \langle \dfrac{1}{c} \, \dfrac{\partial}{\partial t} , \, - \del \right \rangle \\[3pt]
&= \left \langle \dfrac{1}{c} \, \dfrac{\partial}{\partial t} , \, - \dfrac{\partial}{\partial x}, \, - \dfrac{\partial}{\partial y}, \, - \dfrac{\partial}{\partial z} \right \rangle \\[3pt]
&=  \left \langle \partial^{ct}, \, \partial^x , \, \partial^y , \, \partial^z \right \rangle
\end{aligned}
}
\end{equation}
(recall that we use superscript for four-vector components).\footnote{The reason we use superscript for four-vector components is that subscripts are used for the components of \emph{co}vector components in more advanced treatments, and it's important to distinguish them (see Footnote \ref{fn:cv}). We won't use covectors explicitly, but we abide by this notational convention. In \emph{Euclidean} space, vectors and covectors are indistinguishable, so it doesn't matter whether we use superscripts or subscripts for our three-vector components (we'll continue using subscripts).} We must be careful with signs when we use $\partial^x$, $\partial^y$, and $\partial^z$. These spatial components of $\partialup$ are \emph{negative} partial-derivative operators, but $\partial^{ct}$ is a \emph{positive} partial.


\subsubsection{Four-Gradient, Four-Divergence, and the d'Alembertian}

Like $\del$ in three-dimensional Euclidean space, the symbol $\partialup$ in Minkowski spacetime has several uses. Here are a few of them:
\begin{itemize}
\item{
When it operates on a spacetime scalar field $\psi = \psi (\vv R)$, it's the \textbf{four-gradient} function, and it outputs a four-vector field:
\begin{equation*}
\partialup \psi = \left \langle \partial^{ct} \psi, \, -\del \psi \right \rangle.
\end{equation*}
}
\item{
When it's dotted with a four-vector field $\vv Q = \vv Q (\vv R) = \langle Q^{ct}, \vv q \rangle$, we get the \textbf{four-divergence} function, and it outputs a spacetime scalar field:
\begin{equation*}
\begin{split}
\partialup \cdot \vv Q &= \partial^{ct} Q^{ct} - \left( - \del \cdot \vv q \right) \\
&= \partial^{ct} Q^{ct} + \del \cdot \vv q ,
\end{split}
\end{equation*}
whose value at every event is Lorentz-invariant.
}
\item{
When $\partialup$ is dotted with \emph{itself}, the resulting function is called the \textbf{wave operator}, the \textbf{four-Laplacian}, or the \textbf{d'Alembertian}, labeled $\Box$:\footnote{Another convention---preferable, perhaps, but less common these days---uses $\Box$ as the four-del symbol and $\Box^2$ as the d'Alembertian, mirroring the use of $\del$ for the three-del and $\del^2 = \del \cdot \del$ for the three-Laplacian.}
\begin{equation*}
\begin{split}
\Box &\equiv \partialup \cdot \partialup = \dfrac{1}{c^2} \, \dfrac{\partial^2}{\partial t^2} - \left[ \left( - \del \right) \cdot \left( - \del \right) \right] \\
&= \dfrac{1}{c^2} \, \dfrac{\partial^2}{\partial t^2} - \del^2 \\[4pt]
&= \dfrac{1}{c^2} \, \dfrac{\partial^2}{\partial t^2} - \dfrac{\partial^2}{\partial x^2} - \dfrac{\partial^2}{\partial y^2} - \dfrac{\partial^2}{\partial z^2} .
\end{split}
\end{equation*}
}
\end{itemize}
The d'Alembertian is the four-divergence of the four-gradient. Like the Laplacian, it can operate on a scalar field \emph{or} a vector field, always outputting a field of the same kind as its input.\footnote{\label{fn:gd}Does this mean that a vector field can have a gradient? Doesn't that contradict what we said above about the four-gradient acting on scalar fields? It depends on how we define the word \emph{gradient}, but basically the answer is yes! The details involve tensors (briefly---the gradient of a tensor field of rank $n$ is a tensor field of rank $n + 1$, and the divergence of a tensor field of rank $n$ is a tensor field of rank $n - 1$), but for now it's enough to know that the Laplacian and d'Alembertian can operate on both scalar and vector fields.} Think of the d'Alembertian as the four-del's invariant ``squared magnitude." It acts like a \emph{scalar} when combined with other operators, in the sense that the order of the operations is reversible: $\Box (\partialup \psi) = \partialup (\Box \psi)$ and $\Box (\partialup \cdot \vv Q) = \partialup \cdot (\Box \vv Q)$.\footnote{It's a good exercise to verify these relations. Use the analogous Laplacian identities $\del^2 (\del f) = \del (\del^2 f)$ and $\del^2 (\del \cdot \vv c) = \del \cdot (\del^2 \vv c)$, and also the fact that the order of mixed partials doesn't matter (e.g., $\partial^{ct} \del \cdot \vv c = \del \cdot \partial^{ct} \vv c$, and $\partial^{ct} \del f = \del \partial^{ct} f$).} By contrast, the four-gradient of the four-divergence is not equivalent to the four-divergence of the four-gradient (the d'Alembertian): $\partialup (\partialup \cdot \vv Q) \neq \Box \vv Q$.


\subsubsection{Differential Triple Products; Gauge Transformations}\label{sssec:tp}

In three-dimensional Euclidean space, we have the ``$bac - cab$" mnemonic to help us remember how the vector triple product expands into two terms:
\begin{equation*}
\vv a \times (\vv b \times \vv c) = \vv b (\vv a \cdot \vv c) - \vv c (\vv a \cdot \vv b).
\end{equation*}
If $\vv b$ and $\vv c$ are given, we can regard the vector triple product as a function that takes a vector as input and returns a new vector as output:
\begin{equation*}
\text{\underline{\hspace{.6em}}} \times (\vv b \times \vv c) = \vv b ( \mkern1.5mu \text{\underline{\hspace{.6em}}} \cdot \vv c ) - \vv c ( \mkern1.5mu \text{\underline{\hspace{.6em}}} \cdot \vv b ).
\end{equation*}
A special case is when $\vv b = \del$ and $\vv c$ is some three-vector field $\vv c = \vv c (\vv r)$:
\begin{equation}\label{eq:cc}
\begin{split}
\text{\underline{\hspace{.6em}}} \times (\del \times \vv c) &= \del_{\vv c} ( \mkern1.5mu \text{\underline{\hspace{.6em}}} \cdot \vv c ) - \vv c ( \mkern1.5mu \text{\underline{\hspace{.6em}}} \cdot \del_{\vv c} ) \\[2pt]
&= \del_{\vv c} ( \mkern1.5mu \text{\underline{\hspace{.6em}}} \cdot \vv c ) - ( \mkern1.5mu \text{\underline{\hspace{.6em}}} \cdot \del ) \vv c ,
\end{split}
\end{equation}
where the subscript clarifies that $\del$ operates on $\vv c$ (and $\vv c$ alone) regardless of its placement in a term.\footnote{Richard Feynman tried to popularize this handy notation, though it's still somewhat rare. See: \url{http://www.feynmanlectures.caltech.edu/II_27.html\#Ch27-S3}.} This function is the cross-product-to-be of an input-vector and the curl of the field $\vv c$. If we feed it an $\vv a$, we have:
\begin{equation*}
\vv a \times (\del \times \vv c) = \del_{\vv c} ( \vv a \cdot \vv c ) - ( \vv a \cdot \del ) \vv c ,
\end{equation*}
which we'll call the \textbf{differential three-vector triple product}.\footnote{Of course, $\del \times (\vv b \times \vv c)$ is also a ``differential" vector triple product, but we need a name for $\vv a \times (\del \times \vv c)$.} Explicitly, the right side is:
\begin{equation*}
\begin{split}
&\del_{\vv c} \left( a_x c_x + a_y c_y + a_z c_z \right) - \left( a_x \, \dfrac{\partial}{\partial x} + a_y \, \dfrac{\partial}{\partial y} + a_z \, \dfrac{\partial}{\partial z} \right) \vv c \\[3pt]
&= \left( a_x \del c_x + a_y \del c_y + a_z \del c_z \right) - \left( a_x \, \dfrac{\partial \vv c}{\partial x} + a_y \, \dfrac{\partial \vv c}{\partial y} + a_z \, \dfrac{\partial \vv c}{\partial z} \right) ,
\end{split}
\end{equation*}
which gives the Cartesian component form:
\begin{equation*}
\begin{split}
& \Bigg \langle a_y \left( \dfrac{\partial c_y}{\partial x} - \dfrac{\partial c_x}{\partial y} \right) + a_z \left( \dfrac{\partial c_z}{\partial x} - \dfrac{\partial c_x}{\partial z} \right) , \\
&\qquad  a_z \left( \dfrac{\partial c_z}{\partial y} - \dfrac{\partial c_y}{\partial z} \right) + a_x \left( \dfrac{\partial c_x}{\partial y} - \dfrac{\partial c_y}{\partial x} \right) , \\
&\qquad \qquad  a_x \left( \dfrac{\partial c_x}{\partial z} - \dfrac{\partial c_z}{\partial x} \right) + a_y \left( \dfrac{\partial c_y}{\partial z} - \dfrac{\partial c_z}{\partial y} \right) \Bigg \rangle .
\end{split}
\end{equation*}
This is an interesting vector. All three components of $\del \times \vv c$ make an appearance, as do their additive inverses (the components of what $\del \times \vv c$ would be under a left-hand rule). As the cross product $\vv a \times (\del \times \vv c)$, it's necessarily orthogonal to the input-vector $\vv a$, but that's evident from the vector's component form, too---if we multiply each component with the corresponding component of $\vv a$ and sum the resulting products (i.e., take the Euclidean dot product), we get zero.

In \emph{four} dimensions, an axis that's orthogonal to a given pair of vectors isn't unique, so there's no such thing as a Minkowski cross product or curl, and the expressions above containing the $\times$ symbol have no direct four-vector counterparts.\footnote{Actually, some people \emph{do} speak of a ``four-curl" in relation to what we're about to do, but this is a misnomer unless one tweaks the definition of \emph{curl} (and we won't).} We do, however, have a Minkowski dot product and a four-del, and that's enough to make four-vector triple products fashioned after the Euclidean ``$bac - cab$" expansions. In our study of electromagnetism, we'll find it particularly useful to have a spacetime analogue of the ``cross product of a curl" function. Mimicking the right side of Equation \ref{eq:cc} with the four-del and a four-vector field $\vv Q = \vv Q (\vv R)$ (and the \emph{Minkowski} dot product), that's:
\begin{equation*}
\partialup_{\vv Q} ( \mkern1.5mu \text{\underline{\hspace{.6em}}} \cdot \vv Q) - ( \mkern1.5mu \text{\underline{\hspace{.6em}}} \cdot \partialup) \vv Q .
\end{equation*}
Input a four-vector $\vv W$, and we have:
\begin{equation*}
\partialup_{\vv Q} ( \vv W \cdot \vv Q) - ( \vv W \cdot \partialup) \vv Q ,
\end{equation*}
a new four-vector. This is our \textbf{differential four-vector triple product}. For convenience, we'll use a shorter notation for it, modeled after its Euclidean three-vector counterpart:
\begin{equation}\label{eq:dtp}
\vv W \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv Q) \equiv \partialup_{\vv Q} ( \vv W \cdot \vv Q) - ( \vv W \cdot \partialup) \vv Q .
\end{equation}
The scare quotes remind us that we have \emph{not} defined the cross product or curl as four-vector operations.\footnote{Warning: neither this notation nor the very concept of a ``differential four-vector triple product" is standard. But with the differential four-vector triple product in our toolkit, we can cover the basics of relativistic electrodynamics without getting mired in tensor analysis and index gymnastics.}
 
With $\vv W = \langle W^{ct}, \, \vv w \rangle$ and $\vv Q = \langle Q^{ct} , \, \vv q \rangle$, we can put the right side of Equation \ref{eq:dtp} into component form. Here is the first step:
\begin{equation*}
\begin{split}
&\partialup_{\vv Q} \left[ W^{ct} Q^{ct} - \left( \vv w \cdot \vv q \right) \right] - \left[ W^{ct} \partial^{ct} - \big(  \vv w \cdot [ - \del ] \big) \right] \vv Q \\[8pt]
&= W^{ct} \partialup Q^{ct} - \partialup_{\vv q} ( \vv w \cdot \vv q ) - W^{ct} \partial^{ct} \vv Q - ( \vv w \cdot \del ) \vv Q
\end{split}
\end{equation*}
(the $-\del$ comes from Equation \ref{eq:fn}). Now in component form:
\begin{equation*}
\begin{split}
& \left \langle W^{ct} \partial^{ct} Q^{ct} - \vv w \cdot \partial^{ct} \vv q - W^{ct} \partial^{ct} Q^{ct} - \vv w \cdot \del Q^{ct} , \right. \\[3pt]
& \qquad \left. W^{ct} (- \del Q^{ct}) - [- \del_{\vv q} (\vv w \cdot \vv q)]  -  W^{ct} \partial^{ct} \vv q - (\vv w \cdot \del) \vv q \right \rangle \\[10pt]
&= \left \langle \vv w \cdot \left( - \del Q^{ct} - \partial^{ct} \vv q  \right) , \, W^{ct} \left( - \del Q^{ct} - \partial^{ct} \vv q \right) + \big[ \vv w \times (\del \times \vv q) \big] \right \rangle ,
\end{split}
\end{equation*}
where we've again used the identity $\del_{\vv q} (\vv w \cdot \vv q) - (\vv w \cdot \del) \vv q = \vv w \times (\del \times \vv q)$ (differential \emph{three}-vector triple product). Not much to look at, but there's a logic to it, and if we define the three-vectors ${\vv m \equiv - \del Q^{ct} - \partial^{ct} \vv q}$ and ${\vv n \equiv \del \times \vv q}$ (time-dependent three-vector \emph{fields}, really, since $\vv Q$ is a four-vector field), then:
\begin{equation}\label{eq:dt}
\vv W \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv Q) = \left \langle \vv w \cdot \vv m , \, W^{ct} \vv m + \vv w \times \vv n \right \rangle .
\end{equation}
This form makes plain that the output-vector is orthogonal to the input-vector $\vv W$ (dot product is zero), as $(\vv w \times \vv n) \cdot \vv w = 0$. So even though we have no Minkowski cross product or curl, Equation \ref{eq:dt} is indeed closely analogous to the cross product of a curl in three-dimensional Euclidean space.

Now, we know that $\vv a \times [ \del \times (\vv c + \del f) ] = \vv a \times (\del \times \vv c)$ for any scalar field $f$ (because the curl of a gradient is always the zero vector). This is apparent from the ``$bac - cab$" expansion, as well:
\begin{equation*}
\begin{split}
\vv a \times [ \del \times (\vv c + \del f) ] &= \del_{\vv c, \, f} \, [ \vv a \cdot ( \vv c + \del f ) ] - (\vv a \cdot \del) ( \vv c + \del f ) \\
&= \del_{\vv c} ( \vv a \cdot \vv c ) + \del_{f} ( \vv a \cdot \del f ) - (\vv a \cdot \del) \vv c - (\vv a \cdot \del) \del f \\
&= \del_{\vv c} ( \vv a \cdot \vv c ) - (\vv a \cdot \del) \vv c \\
&= \vv a \times (\del \times \vv c) ,
\end{split}
\end{equation*}
where we've used $\del_{f} ( \vv a \cdot \del f ) = \del_{f} ( \vv a \cdot \del ) f = ( \vv a \cdot \del ) \del f$ (write it out in terms of components if you need convincing). By precisely the same logic, we find that our differential \emph{four-vector} triple product is invariant under a transformation that takes the vector field $\vv Q$ to $(\vv Q + \partialup \psi)$ for any spacetime scalar field $\psi$:
\begin{equation*}
\begin{split}
\vv W \mkern1mu `` \mkern-4mu \times [ \partialup \times \mkern-4mu " \mkern1mu ( \vv Q + \partialup \psi ) ] &= \partialup_{\vv Q, \, \psi} [ \vv W \cdot ( \vv Q + \partialup \psi ) ] - (\vv W \cdot \partialup) ( \vv Q + \partialup \psi ) \\
&= \partialup_{\vv Q} ( \vv W \cdot \vv Q ) + ( \vv W \cdot \partialup) \partialup \psi - (\vv W \cdot \partialup) \vv Q - (\vv W \cdot \partialup) \partialup \psi \\
&= \vv W \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv Q) .
\end{split}
\end{equation*}
This is an important identity! Its value is perhaps most salient when the ``input vector" $\vv W$ is the four-del:
\begin{equation*}
\begin{split}
\partialup \mkern1mu `` \mkern-4mu \times \partialup \times \mkern-4mu " \mkern1mu \vv Q &= \partialup ( \partialup \cdot \vv Q) - ( \partialup \cdot \partialup) \vv Q \\
&= \partialup ( \partialup \cdot \vv Q) - \Box \vv Q ,
\end{split}
\end{equation*}
analogous to the curl of the curl:
\begin{equation*}
\begin{split}
\del \times \del \times \vv c &= \del (\del \cdot \vv c) - (\del \cdot \del) \vv c \\
&=  \del (\del \cdot \vv c) - \del^2 \vv c .
\end{split}
\end{equation*}
In this context our new identity looks like:
\begin{equation*}
\begin{split}
\partialup \mkern1mu `` \mkern-4mu \times \partialup \times \mkern-4mu " \mkern1mu ( \vv Q + \partialup \psi ) &= \partialup [ \partialup \cdot ( \vv Q + \partialup \psi ) ] - \Box ( \vv Q + \partialup \psi ) \\
&= \partialup ( \partialup \cdot \vv Q ) + \partialup ( \Box \psi ) - \Box \vv Q - \Box ( \partialup \psi ) \\
&=  \partialup (\partialup \cdot \vv Q) - \Box \vv Q \\
&= \partialup \mkern1mu `` \mkern-4mu \times \partialup \times \mkern-4mu " \mkern1mu \vv Q .
\end{split}
\end{equation*}
This means that if we ever encounter a physical quantity equal to the ``double curl" of a four-vector field $\vv Q$ (we will!), it's also equal to the ``double curl" of $(\vv Q + \partialup \psi)$ for any $\psi$ we choose. This is an example of \textbf{gauge invariance}, and adding a $\partialup \psi$ to $\vv Q$ is called making a \textbf{gauge transformation}. Often it's convenient to choose a $\psi$ that satisfies $\Box \psi = - \partialup \cdot \vv Q$, because then the ``$bac$" term vanishes:
\begin{equation*}
\begin{split}
\partialup \mkern1mu `` \mkern-4mu \times \partialup \times \mkern-4mu " \mkern1mu ( \vv Q + \partialup \psi ) &= \partialup [ \partialup \cdot ( \vv Q + \partialup \psi ) ] - \Box ( \vv Q + \partialup \psi ) \\
&= \partialup ( \partialup \cdot \vv Q + \Box \psi ) - \Box ( \vv Q + \partialup \psi ) \\
&=  - \Box ( \vv Q + \partialup \psi ) ,
\end{split}
\end{equation*}
leaving just a wave equation for the divergenceless(!) $\vv Q^\prime = \vv Q + \partialup \psi$. Performing this transformation imposes the \textbf{Lorenz gauge} (named for Ludvig Lorenz, not Hendrik Lorentz). We still have limited \textbf{gauge freedom} within the Lorenz gauge---for any spacetime scalar field $\chi$ whose d'Alembertian is zero ($\Box \chi = 0$), we get $\Box ( \vv Q^\prime + \partialup \chi ) = \Box \vv Q^\prime + \partialup \Box \chi = \Box \vv Q^\prime$.

That's all the math we'll need. Back to the physics.



\subsection{The Physics (Relativistic Electrodynamics)}

\subsubsection{Four-Current Density}

Experiment shows that electric charge $q$ is Lorentz-invariant. This allows for the construction of new four-vectors by scaling existing ones by $q$. But right now we'll find it more useful to scale by a \emph{related} invariant called the \textbf{proper charge density}.

What is the proper charge density? It's the \textbf{charge density} $\rho$ of an (infinitesimally small) charge distribution as measured in the distribution's (instantaneous) rest frame. And what is charge density? It's the charge per unit volume $V$ of the distribution. Modeling charge distributions as continuous,\footnote{If this approximation is insufficient, one may use the Dirac delta function to account for charge discreteness. Regardless, the proper charge density is invariant.} the charge density of a volume element is:
\begin{equation*}
\rho = \lim\limits_{\substack{\\ \Delta V \to 0}} \, \dfrac{\Delta q}{\Delta V} = \dfrac{\dd q}{\dd V} .
\end{equation*}
The \emph{proper} charge density is then:
\begin{equation*}
\rho_0 = \lim\limits_{\substack{\\ \Delta V_0 \to 0}} \, \dfrac{\Delta q}{\Delta V_0} = \dfrac{\dd q}{\dd V_0} = \dfrac{1}{\gamma} \, \dfrac{\dd q}{\dd V} = \dfrac{\rho}{\gamma},
\end{equation*}
where $V_0$ is the \textbf{proper volume} of the charge distribution---that is, its volume as measured in its rest frame, an invariant. (The Lorentz factor comes from length contraction along the boost axis: $V_0 = \gamma V$. See Section \ref{sssec:td}.) Since both charge and proper volume are invariant, the proper charge density is, too.

Now, scale an infinitesimal charge distribution's normalized four-velocity $\vv B$ by its proper charge density $\rho_0$, and obtain a new four-vector:
\begin{equation}\label{eq:fc}
\boxed{ \vv J \equiv \rho_0 \vv B = \rho_0 \langle \gamma , \gamma \vvbeta \rangle = \langle \rho, \, \vv j \rangle } \, ,
\end{equation}
called the \textbf{four-current density} (the three-vector $\vv j = \rho \vvbeta$ is the ``normalized" \textbf{three-current density} of the infinitesimal distribution). If we associate a (proper) charge density with every event in spacetime, we can speak of $\vv J = \vv J (\vv R)$ as a four-vector \emph{field}.\footnote{We've defined $\vv J$ in such a way that it's necessarily timelike (or the zero vector). This would seem to rule out situations in which $\rho = 0$ and $\vv j \neq \vv 0$, but then what of an electrically neutral current-carrying wire? Isn't that a counterexample? Not exactly. The wire with its ``overlapping" positive and negative charges doesn't fit neatly into our ``continuous charge distribution" model. To accommodate it, we'd handle the positive and negative charges \emph{separately}, treating each collection as a continuous distribution with its own $\vv J^+$ or $\vv J^-$ field. For the positive-charge distribution, $\rho_0^+$ is everywhere non-negative and $\vv J^+ = \rho_0^+ \vv B^+$ is consequently either zero or future-pointing timelike at a given event; for the negative-charge distribution, $\vv J^- = \rho_0^- \vv B^-$ must be either zero or \emph{past}-pointing timelike. When we sum up $\vv J^+$ and $\vv J^-$ to get the ``total" $\vv J$ at an event, we'll find that it can be timelike, spacelike, or lightlike. Note that when we deal with this kind of composite scenario, we can't use $\vv J = \rho_0 \vv B$, but we can still use $\vv J = \langle \rho, \, \vv j \rangle = \langle \rho^+ + \rho^- , \, \vv j^+ + \vv j^- \rangle $.} An important property of this field is that its four-divergence vanishes:
\begin{equation}\label{eq:con}
{\partialup \cdot \vv J = 0} ,
\end{equation}
or $\partial^{ct} \rho + \del \cdot \vv j = 0$. This \textbf{continuity equation} says simply that charge is locally conserved---if the charge density in a region is changing, it must be because charge is crossing the region's boundaries. As a quantity that's both invariant and (additive-and-)conserved, charge is rather special.

The four-current density is the electromagnetic \emph{source} field. To complete the picture, we need to know how ``information" about $\vv J$ spreads and what physical effects this information has.


\subsubsection{Four-Potential}

Without motivation, we define the \textbf{four-potential} as any four-vector field $\vv A = \vv A (\vv R)$ whose negative ``double curl" is the four-current density field:
\begin{equation}\label{eq:gfp}
\boxed{
\begin{split}
\vv J &= - \partialup \mkern1mu `` \mkern-4mu \times \partialup \times \mkern-4mu " \mkern1mu \vv A  \\
&= \Box \vv A - \partialup ( \partialup \cdot \vv A) .
\end{split}
}
\end{equation}
We say ``any" because, as we discussed in Section \ref{sssec:tp}, gauge transformations taking $\vv A$ to $(\vv A + \partialup \psi)$ for any spacetime scalar field $\psi$ leave the ``double curl" of $\vv A$ unchanged. This non-uniqueness of the four-potential doesn't bother us; \emph{differences} in $\vv A$ (and \emph{derivatives} of $\vv A$) are what matter physically, and they're uniquely defined and measurable. Note that Equation \ref{eq:gfp} is inherently compatible with the continuity equation (Equation \ref{eq:con}) because the four-divergence of the right side is necessarily zero: $\partialup \cdot (\Box \vv A) - \partialup \cdot \partialup (\partialup \cdot \vv A) = \Box (\partialup \cdot \vv A) - \Box (\partialup \cdot \vv A)$. 

Something beautiful happens when we impose the Lorenz gauge:
\begin{equation}\label{eq:fp}
\boxed{ \Box \vv A = \vv J } \quad \textrm{\footnotesize{ (in the Lorenz gauge)}} ,
\end{equation}
\begin{equation}\label{eq:lg}
\boxed{ \partialup \cdot \vv A = 0 } \quad \textrm{\footnotesize{ (Lorenz gauge condition)}}
\end{equation}
(again, see Section \ref{sssec:tp}). Equation \ref{eq:fp} is \emph{the} electromagnetic field equation. It's an inhomogeneous wave equation, and it says that a non-zero value of the $\vv J$ field at an event in spacetime generates a commensurate disturbance in the  (Lorenz-gauge) $\vv A$ field that propagates outward in space at $\beta = 1$:
\begin{equation*}
\dfrac{1}{c^2} \, \dfrac{\partial^2 \vv A}{\partial t^2} - \del^2 \vv A = \vv J .
\end{equation*}
The disturbance propagates forever, though it can undergo interference with other disturbances. This is how information about $\vv J$ spreads. The sum of all such disturbances passing through a point in space at a given time determines the ``value" of $\vv A$ at that event (scare quotes because the value isn't uniquely defined---we have ``gauge freedom" even within the Lorenz gauge). A disturbance passing through a region where $\vv J = \textrm{\mbox{\boldmath $\emptyset$}}$ (\textbf{free space}) is called an \textbf{electromagnetic wave}.


\subsubsection{The Lorentz Four-Force; Electric and Magnetic Fields}\label{sssec:lff}

We've introduced the four-potential $\vv A = \vv A (\vv R)$ as any vector field whose negative ``double curl" equals the four-current density, but we haven't yet established why we should care about it. We care about it because of the following empirical result:\footnote{In the next section, we'll actually ``derive" it with a well-chosen Lagrangian.}
\begin{equation}\label{eq:ff}
\boxed{\vv F_{\textrm{L}} = q \, \bigl[ \vv B \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A) \bigr]} \, .
\end{equation}
This is the physical effect that information about the four-current density's value elsewhere has on a test charge upon reaching it.\footnote{By ``test charge" we mean that we neglect the influence of the particle's charge on the four-potential field, which, in turn, affects the particle! Accounting for it is beyond our scope and not entirely possible in classical electrodynamics. We do note that accelerating charges are what generate electromagnetic waves, and that the ``recoil" four-force that a radiating particle experiences (called the \textbf{Abraham--Lorentz--Dirac four-force}) is, \emph{to first approximation}, $\vv F = (2q^2 / 3)[\mathring{\vv Z} - (\vv Z \cdot \vv Z) \vv B]$, where $q$ is the particle's charge, $\vv B$ its normalized four-velocity, $\vv Z$ its ``normalized" four-acceleration, and $\mathring{\vv Z}$ its ``normalized" four-jerk.} We call $\vv F_{\textrm{L}}$ the \textbf{Lorentz four-force}. The quantities on the right side are the particle's charge $q$, the particle's normalized four-velocity $\vv B$, and the ``value" of the four-potential field $\vv A$ at the particle's spacetime position (we're differentiating the four-potential, so its non-uniqueness doesn't matter). As we learned in Section \ref{sssec:tp}, this differential four-vector triple product is gauge-invariant and can also be written:
\begin{equation}\label{eq:lff}
\begin{split}
\vv F_{\textrm{L}} &= q \, \bigl[ \partialup_{\vv A} (\vv B \cdot \vv A) - (\vv B \cdot \partialup) \vv A \bigr] \\
\vv F_{\textrm{L}} &= q \, \bigl[ \partialup (\vv B \cdot \vv A) - (\vv B \cdot \partialup) \vv A \bigr] 
\end{split}
\end{equation}
(the subscript is superfluous since the particle's four-velocity has no $\vv R$-dependence anyway). By Equations \ref{eq:dt} and \ref{eq:31}, the component form of Equation \ref{eq:ff} is:
\begin{equation}\label{eq:lfc}
\boxed{ \vv F_{\textrm{L}} = q \, \big \langle \vv \gamma \vvbeta \cdot \vv e , \, \gamma \vv e + \gamma \vvbeta \times \vv b \big \rangle } \, ,
\end{equation}
where we've defined $\vv e$ and $\vv b$ in terms of the four-potential $\vv A = \langle A^{ct}, \, \vv a \rangle$:
\begin{equation}\label{eq:eb}
\boxed{ \vv e \equiv - \del A^{ct} - \partial^{ct} \vv a \qquad \qquad \vv b \equiv \del \times \vv a } \, .
\end{equation}
These time-dependent three-vector fields are called the \textbf{electric field} and the \textbf{magnetic field}, respectively. Unlike $\vv A$, they are unique and measurable.

We know from our study of the differential four-vector triple product that $\vv F_{\mathrm{L}}$ must be orthogonal to the input-vector $\vv B$. This means that the Lorentz four-force is codirectional with the particle's resulting ``normalized" four-acceleration ($\vv Z =  \vv F / E_0$, Equation \ref{eq:39}) and causes no change in the particle's rest energy $E_0$. Then by Equation \ref{eq:fb}, the spatial three-vector component of $\vv F_{\textrm{L}}$ given in Equation \ref{eq:lfc} is the particle's $\gamma$ times the \textbf{Lorentz three-force} $\vv f_{\textrm{L}}$ exerted on the particle:
\begin{equation}\label{eq:ltf}
\begin{split}
\gamma \vv f_{\textrm{L}} &= q ( \gamma \vv e + \gamma \vvbeta \times \vv b ) \\
\vv f_{\textrm{L}} &= q ( \vv e + \vvbeta \times \vv b ),
\end{split}
\end{equation}
and the temporal component is the particle's $\gamma$ times the \textbf{Lorentz power} $\mathcal{P}_{\textrm{L}}$ (the particle's $\dot{E} = \dot{E}_{\mkern.5mu \textrm{k}} = \vv f \cdot \vvbeta$):
\begin{equation}\label{eq:lp}
\begin{split}
\gamma \mathcal{P}_{\textrm{L}} &= q ( \gamma \vvbeta \cdot \vv e ) \\
\mathcal{P}_{\textrm{L}} &= q ( \vvbeta \cdot \vv e ) \, \left[ \, = \vv f_{\textrm{L}} \cdot \vvbeta \right].
\end{split}
\end{equation}
The magnetic field $\vv b$ plays no role in Equation \ref{eq:lp} because the \textbf{magnetic force} $q \vvbeta \times \vv b$ in Equation \ref{eq:ltf} is perpendicular to $\vvbeta$ and therefore only changes the \emph{direction} of the particle's velocity (and momentum), not its magnitude. In other words, the magnetic field does no work. The \textbf{electric force} $q \vv e$, on the other hand, is parallel to $\vv e$ regardless of the direction of $\vvbeta$, and generally changes both the direction and magnitude of the particle's velocity.

As four-vectors, $\vv J$, $\vv A$, and $\vv F_{\mathrm{L}}$ have components that obey the Lorentz transformation. As three-vectors, $\vv e$ and $\vv b$ do not, but their components are \emph{built} from the components of four-vectors ($\partialup$ and $\vv A$) and transform accordingly. Here are the Cartesian components of $\vv e$ and $\vv b$ in terms of those four-vector components (using Equations \ref{eq:fn} and \ref{eq:eb}):
\begin{equation}\label{eq:ebc}
\begin{aligned}
e_x = \partial^x A^{ct} - \partial^{ct} A^x \qquad \qquad b_x = \partial^z A^y - \partial^y A^z \\
e_y = \partial^y A^{ct} - \partial^{ct} A^y \qquad \qquad b_y = \partial^x A^z - \partial^z A^x \\
e_z = \partial^z A^{ct} - \partial^{ct} A^z \qquad \qquad b_z = \partial^y A^x - \partial^x A^y 
\end{aligned}
\end{equation}
(remember, $\partial^{ct}$ is the positive $ct$-partial, but the other components of $\partialup$ are \emph{negative} partials). Under a standard-configuration boost with relative rapidity $\phi$ between the frames, $e^\prime_x$ is:
\begin{equation*}
\begin{split}
e^\prime_x &= \partial^{\mkern.5mu \prime \mkern1mu x} A^{\prime \mkern1mu ct} - \partial^{\mkern.5mu \prime \mkern1mu ct} A^{\prime \mkern1mu x} \\[2pt]
&= (\cosh{\phi} \; \partial^x  - \sinh{\phi} \; \partial^{ct} )(A^{ct} \cosh{\phi}  - A^x \sinh{\phi} ) \\
& \quad - (\cosh{\phi} \; \partial^{ct} -  \sinh{\phi} \; \partial^x )(A^x \cosh{\phi} - A^{ct} \sinh{\phi} ) \\[2pt]
&= \partial^x A^{ct} - \partial^{ct} A^x = e_x
\end{split}
\end{equation*}
(by Equation \ref{eq:50}). Let's do one more component:
\begin{equation*}
\begin{split}
b^\prime_y &= \partial^{\mkern.5mu \prime \mkern1mu x} A^{\prime \mkern1mu z} - \partial^{\mkern.5mu \prime \mkern1mu z} A^{\prime \mkern1mu x} \\[2pt]
&= (\cosh{\phi} \; \partial^x  - \sinh{\phi} \; \partial^{ct} ) A^z - \partial^z (A^x \cosh{\phi} - A^{ct} \sinh{\phi} ) \\[2pt]
&= \cosh{\phi} \, (\partial^x A^z - \partial^z A^x) + \sinh{\phi} \, (\partial^z A^{ct} - \partial^{ct} A^z) \\[2pt]
&= b_y \cosh{\phi} + e_z \sinh{\phi} .
\end{split}
\end{equation*}
Similar calculations give:
\begin{equation}\label{eq:ebt}
\begin{aligned}
&e^\prime_x = e_x  \qquad \qquad& &b^\prime_x = b_x  \\
&e^\prime_y = e_y \cosh{\phi} - b_z \sinh{\phi}  \qquad& &b^\prime_y = b_y \cosh{\phi} + e_z \sinh{\phi} \\
&e^\prime_z = e_z \cosh{\phi} + b_y \sinh{\phi}  \qquad& &b^\prime_z = b_z \cosh{\phi} - e_y \sinh{\phi} .
\end{aligned}
\end{equation}
For the inverse transformation, swap primed and unprimed components and switch the sign of each $\sinh{\phi}$. From Equations \ref{eq:ebt} and \ref{eq:50}, it follows (after some algebra) that  ${e^{\prime \, 2} - b^{\prime \, 2} = e^2 - b^2}$ and that $\vv e^\prime \cdot \vv b^\prime = \vv e \cdot \vv b$.\footnote{\label{fn:ps}As the curl of an ordinary vector, $\vv b$ is a \emph{pseudovector}, which means that it switches sign under some types of coordinate transformations (e.g., reflection across a plane). The Lorentz-invariant $\vv e \cdot \vv b$ inherits this property (it's a \emph{pseudoscalar}). For this reason, some people regard its square $(\vv e \cdot \vv b)^2$ as the quantity of interest.} These are the fundamental invariants we can construct from $\vv e$ and $\vv b$. An immediate consequence is the invariance of the special cases $\vv e \perp \vv b$ and $e = b$; i.e., if the electric and magnetic fields are perpendicular in one frame ($\vv e \cdot \vv b = 0$), then they're perpendicular in all frames, and if their magnitudes are equal in one frame ($e^2 - b^2 = 0$), then their magnitudes are equal in all frames.

Another way of writing Equations \ref{eq:ebt} is in terms of component vectors parallel and perpendicular to the boost direction $\hatphi$. Using $\vv e_\perp = \langle 0, e_y, e_z \rangle$ and $\hatphi \times \vv b = \langle 0, -b_z, b_y \rangle$, and also $\vv b_\perp = \langle 0, b_y, b_z \rangle$ and $\hatphi \times \vv e = \langle 0, -e_z, e_y \rangle$:
\begin{equation*}
\begin{aligned}
& \vv e^\prime_\parallel = \vv e_\parallel  \qquad \qquad& & \vv b^\prime_\parallel = \vv b_\parallel  \\
& \vv e^\prime_\perp =  \cosh{\phi} \; \vv e_\perp  +  \sinh{\phi} \; ( \hatphi \times \vv b )   \qquad& & \vv b^\prime_\perp = \cosh{\phi} \; \vv b_\perp  - \sinh{\phi} \; ( \hatphi \times \vv e ) ,
\end{aligned}
\end{equation*}
which holds for a boost in an \emph{arbitrary} direction. With $\vv e_\parallel = (\hatphi \cdot \vv e) \hatphi$ and $\vv e_\perp = \vv e - \vv e_\parallel$, we can condense the transformation of $\vv e$ to a single equation that doesn't mention components or component vectors at all:
\begin{equation*}
\begin{split}
\vv e^\prime &= \vv e^\prime_\perp  + \vv e^\prime_\parallel \\
&= \cosh{\phi} \; \vv e_\perp  +  \sinh{\phi} \; ( \hatphi \times \vv b ) + \vv e_\parallel \\
&= \cosh{\phi} \; \big[ \vv e - (\hatphi \cdot \vv e) \hatphi \big] + \sinh{\phi} \; (\hatphi \times \vv b) + (\hatphi \cdot \vv e) \hatphi .
\end{split}
\end{equation*}
Do the same for $\vv b$, and simplify both expressions using the ``half-angle" hyperbolic identity $\cosh{\phi} - 1 = 2 \sinh^2{(\phi/2)}$:
\begin{equation}\label{eq:ebr}
\boxed{
\begin{aligned}
\vv e^\prime &= \cosh{\phi} \; \vv e + \sinh{\phi} \; (\hatphi \times \vv b) - 2 \sinh^2 \frac{\phi}{2} \; (\hatphi \cdot \vv e) \hatphi \\[4pt]
\vv b^\prime &= \cosh{\phi} \; \vv b - \sinh{\phi} \; (\hatphi \times \vv e) - 2 \sinh^2 \frac{\phi}{2} \; (\hatphi \cdot \vv b) \hatphi 
\end{aligned}
} \, .
\end{equation}
Equivalently, though maybe less elegantly:
\begin{equation*}
\vv e^\prime = \gamma \left( \vv e + \vvbeta \times \vv b \right) - \dfrac{\gamma^2 (\vvbeta \cdot \vv e) \vvbeta}{\gamma + 1} \qquad \quad \vv b^\prime = \gamma \left( \vv b - \vvbeta \times \vv e \right) - \dfrac{\gamma^2 (\vvbeta \cdot \vv b) \vvbeta}{\gamma + 1},
\end{equation*}
where $\gamma = \cosh{\phi}$ and $\beta = \tanh{\phi}$ are boost parameters (and $\hatbeta = \vvbeta / \beta = \hatphi$).

Finally, it's useful to derive a relationship between $\vv J$ and the $\vv e$ and $\vv b$ fields (Equations \ref{eq:eb}). Let's use the Lorenz gauge, with $\vv J= \Box \vv A = \langle \Box A^{ct}, \Box \vv a \rangle$. By the Lorenz condition ${\partialup \cdot \vv A = 0}$ (that is, $\partial^{ct} A^{ct} = - \del \cdot \vv a$):
\begin{equation}\label{eq:ace}
\begin{split}
\rho &= \Box A^{ct} \\
&= \partial^{ct} (\partial^{ct} A^{ct}) - \del^2 A^{ct} \\
&= \partial^{ct} (- \del \cdot \vv a) - \del \cdot \del A^{ct} \\
&= \del \cdot (- \partial^{ct} \vv a - \del A^{ct}) \\
\rho &= \del \cdot \vv e
\end{split}
\end{equation}
(we can swap $\partial^{ct}$ and $\del \cdot$ because the order of mixed partials is reversible). Then using the identity ${\del^2 \vv a = \del(\del \cdot \vv a) - \del \times (\del \times \vv a)}$ for the vector Laplacian (and Equations \ref{eq:eb}), we also have:
\begin{equation}\label{eq:abe}
\begin{split}
\vv j &= \Box \vv a \\
&= \partial^{ct} (\partial^{ct} \vv a) - \del^2 \vv a \\
&= \del \times (\del \times \vv a) + \del(\partial^{ct} A^{ct}) + \partial^{ct} (\partial^{ct} \vv a) \\
\vv j &=  \del \times \vv b - \partial^{ct} \vv e ,
\end{split}
\end{equation}
where we've again invoked the Lorenz condition $\partial^{ct} A^{ct} = - \del \cdot \vv a$ and reversed the order of mixed partials. We're now in a position to write \textbf{Maxwell's equations}, the three-vector equivalent of the more compact Equation \ref{eq:fp}:
\begin{equation}\label{eq:me}
\boxed{
\begin{aligned}
\del \cdot \vv e &= \rho \\
\del \times \vv b - \partial^{ct} \vv e &= \vv j \\ 
\del \cdot \vv b &= 0 \\
\del \times \vv e + \partial^{ct} \vv b &= \vv 0
\end{aligned}
} \, .
\end{equation}
We've used Equations \ref{eq:eb}, \ref{eq:ace}, and \ref{eq:abe}, and a pair of vector calculus identities (the divergence of a curl is $0$, and the curl of a gradient is $\vv 0$).\footnote{If we weren't working in the Lorenz gauge, we'd instead want to find a relationship between $\vv J = \Box \vv A - \partialup (\partialup \cdot \vv A)$ and the $\vv e$ and $\vv b$ fields (see Equation \ref{eq:gfp}), and of course we couldn't invoke the Lorenz condition. But we'd still get Equations \ref{eq:me}! For example: $\rho = \Box A^{ct} - \partial^{ct}(\partialup \cdot \vv A) = \partial^{ct} (\partial^{ct} A^{ct}) - \del \cdot \del A^{ct} - \partial^{ct} (\partial^{ct} A^{ct} + \del \cdot \vv a) = \del \cdot \vv e$.} Note that the first two of Maxwell's equations together constitute a four-vector (equal to the four-current density $\vv J = \langle \rho, \vv j \rangle$), and that the last two do the same (equal to the zero four-vector $\textrm{\mbox{\boldmath $\emptyset$}} = \langle 0, \vv 0 \rangle$). The left sides are suggestive; each four-vector looks like it might be expressible with the four-del $\partialup$ somehow. We'll come back to this in Section \ref{sssec:md}.


\subsubsection{For Fun: Lorentz Force via the Euler--Lagrange Equation}

Earlier we called the Lorentz four-force (Equation \ref{eq:ff}) an ``empirical result." Alternatively, we can start with a suitable Lorentz-invariant Lagrangian and use the Minkowski-spacetime version of the Euler--Lagrange equation to \emph{derive} the Lorentz four-force. This is typically done with index gymnastics and covectors, which we haven't covered, but our four-vector notation will suffice if we define:
\begin{equation*}
\partialup^\ddagger \equiv \left \langle \dfrac{\partial}{\partial B^{ct}} , \,  - \dfrac{\partial}{\partial B^x} , \, - \dfrac{\partial}{\partial B^y} , \, - \dfrac{\partial}{\partial B^z} \right \rangle
\end{equation*}
(the minus signs make it a four-vector, as was the case with $\partialup$---otherwise we'd have an object whose components transform \emph{inverse}-Lorentzianly, rather than Lorentzianly). With $L$ as the Lagrangian, the four-vector version of the Euler--Lagrange equation in our unorthodox notation is:
\begin{equation*}
\dfrac{1}{c} \, \dfrac{\dd}{\dd t_0} \left( \partialup^\ddagger L \right) = \partialup L .
\end{equation*}

To cut to the chase, the Lagrangian we want for a particle in an electromagnetic field should have two terms constructed from four-vectors and invariants: one for the particle as if it were inertial (absent the field and any other forces) and one for its coupling with the field. Now, we haven't yet dealt with the principle of stationary action directly, but here and there we've had occasion to discuss the elapsed proper time along a particle's trajectory, and we even mentioned in passing that the \emph{inertial} journey between a pair of timelike-separated events is the one that maximizes the proper time elapsed en route. Because of this principle of maximal aging, the Lagrangian term for the inertial particle ($L_{\textrm{inertial}}$) should be a \emph{constant}, so that its action integral $\int \mkern-4mu L_{\textrm{inertial}} \, c \, dt_0$ for the path taken is proportional to the particle's elapsed proper time.\footnote{If we were actually using the calculus of variations to \emph{find} the path of stationary action, we'd have to integrate over some other variable whose values at the integration limits are \emph{fixed}, regardless of the path taken between them (i.e., not the path-dependent proper time). After finding the path of stationary action, it's fine to equate the integration variable with the particle's proper time \emph{for that path}, and that's all we've done.} Something like $L_{\textrm{inertial}} = E_0 (\vv B \cdot \vv B)$ should do the trick (we don't want to reduce $\vv B \cdot \vv B$ to $B^2 = 1$ at this stage---if we do, we won't get a four-acceleration when we differentiate, and then how will we recover the Lorentz four-force?). For the coupling term, perhaps the simplest sensible invariant we can make from the field and the particle is $q (\vv B \cdot \vv A)$. Let's try it. Our candidate Lagrangian is then:
\begin{equation*}
L = \dfrac{1}{2} \, E_0 (\vv B \cdot \vv B) + q (\vv B \cdot \vv A)
\end{equation*}
(we've tossed in the $1/2$ to cancel the $2$ that will inevitably arise when we differentiate). Here, $\vv B = \vv B (ct_0)$ is the particle's four-velocity, $E_0$ its rest energy, $q$ its charge, and $\vv A = \vv A (\vv R (ct_0))$ the four-potential at the particle's four-position $\vv R = \vv R (ct_0)$. If we've chosen well, then plugging our Lagrangian into the four-vector version of the Euler--Lagrange equation and carrying out the derivatives should get us the Lorentz four-force.

First we'll handle the ``canonical four-momentum" $\partialup^\ddagger L$:
\begin{equation*}
\partialup^\ddagger L =  \dfrac{1}{2} \, E_0 \, \partialup^\ddagger (\vv B \cdot \vv B) + q \, \partialup^\ddagger (\vv B \cdot \vv A) .
\end{equation*}
We can express $\partialup^\ddagger (\vv B \cdot \vv B)$ and $\partialup^\ddagger (\vv B \cdot \vv A)$ in component form to make sense of them:
\begin{equation*}
\begin{split}
& \partialup^\ddagger(\vv B \cdot \vv B) \\[2pt]
&= \left \langle \dfrac{\partial}{\partial B^{ct}} , \,  - \dfrac{\partial}{\partial B^x} , \, - \dfrac{\partial}{\partial B^y} , \, - \dfrac{\partial}{\partial B^z} \right \rangle \left( B^{ct \, 2} - B^{x \, 2} - B^{y \, 2} - B^{z \, 2} \right) \\[3pt]
&= \left \langle \dfrac{\partial}{\partial B^{ct}} \left( B^{ct \, 2} \right) , \, - \dfrac{\partial}{\partial B^x} \left( - B^{x \, 2} \right) , \, - \dfrac{\partial}{\partial B^y} \left( - B^{y \, 2} \right)  , \, - \dfrac{\partial}{\partial B^z} \left( - B^{z \, 2} \right)  \right \rangle \\[2pt]
&= 2 \vv B
\end{split}
\end{equation*}
(there's that $2$); likewise:
\begin{equation*}
\begin{split}
&\partialup^\ddagger (\vv B \cdot \vv A) \\[2pt]
&= \left \langle \dfrac{\partial}{\partial B^{ct}} , \,  - \dfrac{\partial}{\partial B^x} , \, - \dfrac{\partial}{\partial B^y} , \, - \dfrac{\partial}{\partial B^z} \right \rangle \left( B^{ct} A^{ct} - B^x A^x - B^y A^y - B^z A^z \right) \\[3pt]
&= \left \langle A^{ct} \, \dfrac{\partial B^{ct}}{\partial B^{ct}} , \,  (- A^x) \left( - \dfrac{\partial B^x}{\partial B^x} \right) , \, (-A^y) \left( - \dfrac{\partial B^y}{\partial B^y} \right) , \, (-A^z) \left( - \dfrac{\partial B^z}{\partial B^z} \right) \right \rangle \\[2pt]
&= \vv A 
\end{split}
\end{equation*}
($\vv A = \vv A(\vv R(ct_0))$ has no $\vv B$-dependence). So canonical four-momentum is:
\begin{equation*}
\partialup^\ddagger L =  E_0 \vv B + q \vv A
\end{equation*}
(the sum of $q \vv A$ and the ``normal" four-momentum $\vv P = E_0 \vv B$), and the left side of our Euler--Lagrange equation is its $ct_0$-derivative:
\begin{equation*}
\dfrac{1}{c} \, \dfrac{\dd}{\dd t_0} \left( \partialup^\ddagger L \right) =  E_0 \mathring{\vv B} + q \mathring{\vv A} .
\end{equation*}
The right side of the Euler--Lagrange equation is easy:
\begin{equation*}
\begin{split}
\partialup L &= \dfrac{1}{2} \, E_0 \, \partialup (\vv B \cdot \vv B) + q \, \partialup (\vv B \cdot \vv A) \\[2pt]
&= q \, \partialup_{\vv A} (\vv B \cdot \vv A)
\end{split}
\end{equation*}
(subscript optional, since $\vv B = \vv B(ct_0)$ doesn't depend on $\vv R$). Putting it all together:
\begin{equation*}
\begin{split}
\dfrac{1}{c} \, \dfrac{\dd}{\dd t_0} \left( \partialup^\ddagger L \right) &= \partialup L \\[2pt]
E_0 \mathring{\vv B} + q \mathring{\vv A} &= q \, \partialup (\vv B \cdot \vv A) \\[3pt]
\vv F &= q \, \partialup (\vv B \cdot \vv A) - q \mathring{\vv A} ,
\end{split}
\end{equation*}
where we've used $\vv F = E_0 \mathring{\vv B}$.

One more step. Consider the following operator, which appears in the Lorentz four-force (Equation \ref{eq:lff}):
\begin{equation*}
\begin{split}
\vv B \cdot \partialup &= \left \langle c \mathring{t}, \mathring{\vv r} \right \rangle \cdot \left \langle \partial^{ct}, - \del \right \rangle \\[4pt]
&= \dfrac{\dd (ct)}{\dd (ct_0)} \, \dfrac{\partial}{\partial (ct)} + \dfrac{\dd x}{\dd (ct_0)} \, \dfrac{\partial}{\partial x} +  \dfrac{\dd y}{\dd (ct_0)} \, \dfrac{\partial}{\partial y} + \dfrac{\dd z}{\dd (ct_0)} \, \dfrac{\partial}{\partial z} .
\end{split}
\end{equation*}
By the multivariable chain rule, this operator is equivalently $\dd / \dd (ct_0)$ (the total $ct_0$-derivative) if the function being differentiated depends only on ${\vv R (ct_0) = \langle ct (ct_0), \, x (ct_0), \, y (ct_0), \, z (ct_0) \rangle}$. Our four-potential fits the bill, so $\mathring{\vv A} = (\vv B \cdot \partialup) \vv A $, and:
\begin{equation*}
\begin{split}
\vv F &= q \, \bigl[ \partialup (\vv B \cdot \vv A) - (\vv B \cdot \partialup) \vv A \bigr] = q \, \bigl[ \vv B \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A) \bigr] ,
\end{split}
\end{equation*}
which is indeed the Lorentz four-force (Equations \ref{eq:ff} and \ref{eq:lff}).

\subsection{The Unfinished Business: Energy and Light}

``All" of classical electromagnetism is contained in Equations \ref{eq:fp} and \ref{eq:ff}. Now let's apply it. Remember, our goal is to obtain Equation \ref{eq:8} without using the Planck--Einstein relation. To do this, we'll first coax some general information about electromagnetic energy (and momentum) from the Lorentz four-force. Then we'll briefly discuss light---i.e., solutions to Equation \ref{eq:fp} when $\vv J = \textrm{\mbox{\boldmath $\emptyset$}}$---and try to figure out how its energy transforms under a Lorentz boost.

\subsubsection{The Lorentz Four-Force Density; Poynting's Theorem}\label{sssec:lffd}

If we differentiate the Lorentz four-force (Equation \ref{eq:ff}) with respect to the proper volume ${V_0 = \gamma V}$ of a small piece of a continuous charge distribution, we get a four-vector called the \textbf{Lorentz four-force density}:
\begin{equation*}
\dfrac{\dd \vv F_{\mathrm{L}}}{\dd V_0} = \dfrac{\dd}{\dd V_0} \left( q \, \bigl[ \vv B \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A) \bigr]  \right) .
\end{equation*}
Only $q$ has volume-dependence here, so that's:
\begin{equation*}
\dfrac{\dd \vv F_{\mathrm{L}}}{\dd V_0} = \rho_0 \bigl[ \vv B \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A) \bigr] .
\end{equation*}
Now apply Equations \ref{eq:fc} and \ref{eq:fp} ($\rho_0 \vv B = \vv J = \Box \vv A$ in the Lorenz gauge):
\begin{equation}\label{eq:lffd}
\boxed{ \dfrac{\dd \vv F_{\mathrm{L}}}{\dd V_0} = \Box \vv A \mkern1mu `` \mkern-4mu \times (\partialup \times \mkern-4mu " \mkern1mu \vv A) } \, ,
\end{equation}
or
\begin{equation*}
\dfrac{\dd \vv F_{\mathrm{L}}}{\dd V_0} = \partialup_{\vv A} (\Box \vv A \cdot \vv A) - (\Box \vv A \cdot \partialup) \vv A ,
\end{equation*}
where $\partialup_{\vv A}$ operates only on the ``boxless" $\vv A$.

We can associate a Lorentz four-force density with every event in spacetime, so it's really a vector \emph{field}. Its value represents the (proper) per-unit-volume rate at which the $\vv A$ field transfers four-momentum to an infinitesimal charge distribution. Equation \ref{eq:lffd} beautifully expresses this quantity in terms of the Lorenz-gauge $\vv A$ field alone! In free space (where $\vv J = \Box \vv A = \textrm{\mbox{\boldmath $\emptyset$}}$), the Lorentz four-force density vanishes, as it must if four-momentum is to be conserved. That doesn't mean there's no four-momentum in the $\vv A$ field in free space---just that whatever four-momentum is there doesn't magically disappear.

Recall that the temporal and spatial components of the Lorentz four-force are $\gamma \mathcal{P}_{\mathrm{L}}$ and $\gamma \vv f_{\textrm{L}}$, where $\gamma$ is a test particle's Lorentz factor, and $\mathcal{P}_{\mathrm{L}}$ and $\vv f_{\textrm{L}}$ are the power and three-force exerted on the particle as a result of its interaction with the $\vv A$ field (see Equations  \ref{eq:ltf} and \ref{eq:lp}). The component form of the Lorentz four-force \emph{density} is then:
\begin{equation*}
\dfrac{\dd \vv F_{\mathrm{L}}}{\dd V_0} = \left \langle \gamma \, \dfrac{\dd \mathcal{P}_{\textrm{L}} }{\dd V_0}, \, \gamma \, \dfrac{\dd \vv f_{\textrm{L}}}{\dd V_0} \right \rangle,
\end{equation*}
where $\gamma$ is the Lorentz factor of the infinitesimal \emph{distribution} (and has no volume-dependence). By ``volume contraction" ($\dd V_0 = \gamma \mkern1mu \dd V$), we've got:
\begin{equation}\label{eq:lffdc}
\dfrac{\dd \vv F_{\mathrm{L}}}{\dd V_0} = \left \langle \dfrac{\dd \mathcal{P}_{\mathrm{L}}}{\dd V}, \, \dfrac{\dd \vv f_{\mathrm{L}}}{\dd V} \right \rangle .
\end{equation}
So in a given frame, the temporal component is the per-unit-$V$ $ct$-rate of \emph{energy} transfer from $\vv A$ to the distribution, and the spatial component is the per-unit-$V$ $ct$-rate of \emph{three-momentum} transfer.

With this understanding, let's now write the component form of the Lorentz four-force density in terms of the $\vv e$ and $\vv b$ fields. First, from Equation \ref{eq:lfc} (and the relations $\rho_0 \gamma \vvbeta = \vv j$ and $\rho_0 \gamma = \rho$ for a continuous charge distribution):
\begin{equation*}
\dfrac{\dd \vv F_{\mathrm{L}}}{\dd V_0} = \left \langle \, \vv j \cdot \vv e, \, \rho \mkern1mu \vv e + \vv j \times \vv b \right \rangle ,
\end{equation*}
and with substitutions from Equations \ref{eq:ace} and \ref{eq:abe}:
\begin{equation}\label{eq:lffdc2}
\dfrac{\dd \vv F_{\mathrm{L}}}{\dd V_0} = \Big \langle \left( \del \times \vv b - \partial^{ct} \vv e \right) \cdot \vv e , \, \left( \del \cdot \vv e \right) \vv e + \left( \del \times \vv b - \partial^{ct} \vv e \right) \times \vv b \Big \rangle .
\end{equation}
Doesn't look promising! Yet, these components can be simplified in a way that yields succinct statements of the (local) conservation of energy and three-momentum that explicitly account for the energy and momentum stored in the $\vv e$ and $\vv b$ fields. Let's go ahead and spruce up that temporal component. Start with the identity $\del \cdot (\vv e \times \vv b) = \vv b \cdot (\del \times \vv e) - \vv e \cdot (\del \times \vv b)$:
\begin{equation*}
\vv e \cdot \left( \del \times \vv b - \partial^{ct} \vv e \right) = \vv b \cdot \left( \del \times \vv e \right) - \del \cdot \left( \vv e \times \vv b \right) - \vv e \cdot \partial^{ct} \vv e .
\end{equation*}
Then use $\del \times \vv e = - \partial^{ct} \vv b$ (Equations \ref{eq:me}), and bring in Equation \ref{eq:lffdc}:
\begin{equation*}
\begin{split}
\dfrac{\dd \mathcal{P}_{\textrm{L}}}{\dd V} &= - \del \cdot \left( \vv e \times \vv b \right) - \vv e \cdot \partial^{ct} \vv e - \vv b \cdot \partial^{ct} \vv b  \\
&= - \del \cdot \left( \vv e \times \vv b \right) - \frac{1}{2} \, \partial^{ct} \left( \vv e \cdot \vv e + \vv b \cdot \vv b \right) \\[2pt]
&= - \del \cdot \vv s - \partial^{ct} u ,
\end{split}
\end{equation*}
where $\vv s \equiv \vv e \times \vv b$ and $u \equiv (e^2 + b^2)/2$. Reordering the terms, we have:
\begin{equation}\label{eq:py}
\boxed{ - \partial^{ct} u =  \del \cdot \vv s + \dfrac{\dd \mathcal{P}_{\textrm{L}}}{\dd V} } \, .
\end{equation}

Equation \ref{eq:py} is \textbf{Poynting's theorem}. That it's the energy-conservation statement we were looking for is easier to grok if we integrate it over a finite volume $V$:
\begin{equation*}
- \int_V \partial^{ct} u \, \dd V = \int_V \left(  \del \cdot \vv s + \dfrac{\dd \mathcal{P}_{\textrm{L}}}{\dd V} \right) \dd V ,
\end{equation*}
and use the divergence theorem:
\begin{equation*}
- \dfrac{1}{c} \, \dfrac{\dd}{\dd t} \int_V u \, \dd V = \int_S \left( \vv s \cdot \vv{\hat{n}} \right) \dd S + \int_V \dd \mathcal{P}_{\textrm{L}} 
\end{equation*}
($S$ is the bounding surface of $V$, and $\vv{\hat{n}}$ is an outward-pointing unit vector normal to the surface element $\dd S$). Let's also sub in $\dot{E}_{\mkern1mu \textrm{particles}}$ for $\mathcal{P}_{\textrm{L}}$ (it's the charged particles in the volume that the Lorentz four-force acts on):
\begin{equation*}
- \dfrac{1}{c} \, \dfrac{\dd}{\dd t} \int_V u \, \dd V = \int_S \left( \vv s \cdot \vv{\hat{n}} \right) \dd S + \dfrac{1}{c} \, \dfrac{\dd}{\dd t} \int_V \dd E_{\mkern1mu \textrm{particles}}.
\end{equation*}
Evidently, $u$ is the volume-density of something, and the surface integral is the flux of that something (that is, the rate at which that something flows out of the volume's bounds). The last term clues us in as to what that something is: the energy stored in the fields. Poynting's theorem says that the rate at which electromagnetic energy decreases in a volume equals the rate at which electromagnetic energy is flowing out of the region through its surface plus the rate at which the fields are doing work on charged particles within the region (via the Lorentz four-force). Naturally, we call $u = (e^2 + b^2)/2$ the \textbf{electromagnetic energy density}. The \textbf{Poynting vector} $\vv s = \vv e \times \vv b$ gives the flow of electromagnetic energy (its per-unit-area rate of transport).

We have what we need from the Lorentz four-force density to take care of our unfinished business. Later we'll return to Equation \ref{eq:lffdc2} and simplify its \emph{spatial} component into a succinct statement of three-momentum conservation.


\subsubsection{Light}\label{sssec:li}

Light is an electromagnetic wave---a disturbance in the (Lorenz-gauge!) four-potential field $\vv A = \vv A (\vv R)$ that propagates through free space (where the four-current density field $\vv J = \vv J (\vv R)$ is zero).\footnote{Our ``potential-first" approach in this section is \emph{only} valid in the Lorenz gauge. The results it generates for the $\vv e$ and $\vv b$ fields, however, are gauge-invariant.} To discuss it mathematically, then, means working with solutions to the homogeneous wave equation
\begin{equation*}
\Box \vv A =  \left( \dfrac{1}{c^2} \, \dfrac{\partial^2}{\partial t^2} - \del^2 \right) \vv A = \textrm{\mbox{\boldmath $\emptyset$}}
\end{equation*}
(Equation \ref{eq:fp} with $\vv J = \textrm{\mbox{\boldmath$\emptyset$}}$). We won't go over \emph{how to solve} this second-order linear partial differential equation. Rather, we'll take for granted the result from Fourier analysis that any of its solutions can be expressed as a superposition of monochromatic plane waves, and that its general monochromatic solution is:
\begin{equation}\label{eq:pw}
\vv A (\vv R) = \Re \left \lbrace \bar{\vv A} \mathrm{e}^{\mathrm{i} \mkern.5mu \vv K \cdot \vv R} \right \rbrace ,
\end{equation}
where the magnitude of the constant four-vector $\bar{\vv A} = \langle \bar{A}^{ct}, \, \bar{\vv a} \rangle$ is the wave's peak amplitude, and the likewise constant \textbf{four-wavevector} ${\vv K = \langle K^{ct}, \, \vv k \rangle}$ points in the spacetime-direction of the wave's propagation ($\vv k$ points in the \emph{spatial} direction of the wave's propagation).\footnote{The usual notation for the amplitude vector is $\vv A_0$, but we've been using the naught subscript for proper quantities, so we'll use bar notation instead.} If you were expecting a sinusoidal function of something like $(\vv k \cdot \vv r - 2 \pi \nu t )$, note that by Euler's formula ${\Re \lbrace \mathrm{e}^{\mathrm{i} \mkern.5mu \theta} \rbrace = \Re \lbrace \cos \theta + \mathrm{i} \sin \theta \rbrace = \cos \theta}$, and also that $\vv K \cdot \vv R = K^{ct}ct - \vv k \cdot \vv r$, with $K^{ct}$ related to the frequency $\nu$ by $K^{ct} = 2 \pi \nu / c$ (cosine is an even function, so it doesn't matter which term gets subtracted from the other). The complex exponential function is convenient for linear operations; only when we perform non-linear operations (we won't) or need the field itself do we take the real part.

We'll assume that the amplitude vector $\bar{\vv A}$ is real, but it's worth mentioning that in general it may be complex. In the simplest case of complex $\bar{\vv A}$, we'd have $\bar{\vv A} = \mathcal{A} \mathrm{e}^{\mathrm{i} \mkern.5mu \delta}$ for some real four-vector $\mathcal{A}$, and $\vv A$ would then be the real part of ${\mathcal{A} \mathrm{e}^{\mathrm{i} \mkern.5mu \delta} \mathrm{e}^{\mathrm{i} \mkern.5mu \vv K \cdot \vv R} = \mathcal{A} \mathrm{e}^{\mathrm{i} \mkern.5mu ( \vv K \cdot \vv R \, + \, \delta )}}$, which differs from Equation \ref{eq:pw} for real $\bar{\vv A}$ only by the inclusion of a phase constant in the exponent. A more complicated scenario arises when complex $\bar{\vv A}$ \emph{cannot} be expressed as $\mathcal{A} \mathrm{e}^{\mathrm{i} \mkern.5mu \delta}$ for real $\mathcal{A}$, but no matter---the resulting wave is always equivalent to a superposition of solutions that are free of this complication.\footnote{Such a wave is elliptically or circularly polarized, as opposed to linearly polarized.} In the end, keeping $\bar{\vv A}$ real won't affect our main results, though strictly speaking we should add a phase constant to the exponential and sinusoidal arguments we encounter. The four-wavevector $\vv K$ is necessarily real (for our free-space waves).

Now, surely $\vv K$ is lightlike (null), yes? As a sanity check, let's set the d'Alembertian of Equation \ref{eq:pw} to zero and see what happens (we need not take the real part, since differentiation is a linear operation). Introducing $f(\vv R) \equiv \vv K \cdot \vv R$ to avoid clutter, and using a product rule:\footnote{This product rule is $\partialup \cdot (g \vv Q) = g (\partialup \cdot \vv Q) + (\partialup g) \cdot \vv Q$ for a four-vector $\vv Q$ and a spacetime scalar function $g$, easily verified by writing it out in component form. In our case, $\vv Q = \partialup h$ (four-gradient of a scalar $h$), and so $\partialup \cdot (g \, \partialup h) = g \Box h + \partialup g \cdot \partialup h$.}
\begin{equation*}
\begin{aligned}
\textrm{\mbox{\boldmath $\emptyset$}} &= \Box \left( \bar{\vv A} \mathrm{e}^{\mathrm{i} f} \right) = \bar{\vv A} \, \Box \mkern1mu \mathrm{e}^{\mathrm{i} f} \\
0 &= \partialup \cdot \partialup \mathrm{e}^{\mathrm{i} f} = \partialup \cdot \left( \mathrm{i} \mathrm{e}^{\mathrm{i} f} \, \partialup f \right) \\
0 &= \mathrm{e}^{\mathrm{i} f} \, \Box f + \partialup \mathrm{e}^{\mathrm{i} f} \cdot \partialup f  \\
0 &=  \Box f + \mathrm{i} \, \partialup f \cdot \partialup f ,
\end{aligned}
\end{equation*}
which requires that $\Box f = 0$ and $\partialup f \cdot \partialup f = 0$. But $\partialup f = \partialup (\vv K \cdot \vv R) = \vv K$:
\begin{equation*}
\begin{aligned}
\partialup (\vv K \cdot \vv R) &= \left \langle \dfrac{1}{c} \, \dfrac{\partial}{\partial t}, \, - \dfrac{\partial}{\partial x}, \, - \dfrac{\partial}{\partial y}, \, - \dfrac{\partial}{\partial z} \right \rangle \left( K^{ct} ct - K^x x - K^y y - K^z z \right) \\[3pt]
&= \left \langle K^{ct} \dfrac{\partial (ct)}{\partial (ct)}, \, K^x \dfrac{\partial x}{\partial x}, \, K^y \dfrac{\partial y}{\partial y} , \, K^z \dfrac{\partial z}{\partial z} \right \rangle = \vv K ,
\end{aligned}
\end{equation*}
and our constraints become $\partialup \cdot \vv K = 0$ (trivial---$\vv K$ is constant) and $\vv K \cdot \vv K = 0$. So $\vv K$ \emph{is} lightlike, and let's further insist that it's future-pointing (otherwise we allow for light waves traveling backward in time!). The Lorenz condition ($\partialup \cdot \vv A = 0$) imposes an additional constraint on $\vv K$:
\begin{equation*}
\begin{aligned}
0 &= \partialup \cdot \left( \bar{\vv A} \mathrm{e}^{\mathrm{i} f} \right) \\
&= \bar{\vv A} \cdot \partialup \mathrm{e}^{\mathrm{i} f} \\
&= \mathrm{i} \mathrm{e}^{\mathrm{i} f} \bar{\vv A} \cdot \partialup f ,
\end{aligned}
\end{equation*}
giving $\bar{\vv A} \cdot \vv K = 0$. Combined with the fact that $\vv K$ is future-pointing lightlike ($K^{ct} = k$), this orthogonality means that $\bar{A}^{ct} K^{ct} = \bar{A}^{ct} k = \bar{\vv a} \cdot \vv k$.

With these constraints and Equations \ref{eq:eb}, we can differentiate Equation \ref{eq:pw} to obtain the electric and magnetic fields of the monochromatic plane wave. We'll keep using $f(\vv R) = \vv K \cdot \vv R$ for short, bearing in mind that $\partialup f = \vv K$, whose components are $\partial^{ct} f = K^{ct}$ and $- \del f = \vv k$. First the magnetic field:\footnote{The omitted $\Re \lbrace \rbrace$ notation is implied. We'll take the real part after differentiating.}
\begin{equation*}
\begin{aligned}
\del \times \vv a &= \del \times \left( \bar{\vv a} \mkern1mu \mathrm{e}^{\mathrm{i} f} \right) \\
&= \del \mathrm{e}^{\mathrm{i} f} \times \bar{\vv a} \\
&= \mathrm{i} \mathrm{e}^{\mathrm{i} f} \del f \times \bar{\vv a} \\
&= - \mathrm{i} \mathrm{e}^{\mathrm{i} f} \, \vv k \times \bar{\vv a},
\end{aligned}
\end{equation*}
where we've used the product rule for the curl of a scalar-times-a-vector. Defining $\bar{\vv b} \equiv \vv k \times \bar{\vv a}$ and noting $\Re \lbrace - \mathrm{i} \mathrm{e}^{\mathrm{i} f} \rbrace = \Re \lbrace -\mathrm{i} ( \cos f + \mathrm{i} \sin f ) \rbrace = \sin f$, the real part is:\footnote{A few paragraphs up, we mentioned that it doesn't matter whether we use positive or negative $f = \vv K \cdot \vv R$ because cosine is an even function. Well, here we have a sine function, which is \emph{odd}. Does it matter now? No: if we'd started with $\mathrm{e}^{-\mathrm{i} f}$, we'd have ended up with $- \sin (-f) = \sin f$.}
\begin{equation}\label{eq:bpw}
\vv b (\vv R) = \bar{\vv b} \, \sin \left( \vv K \cdot \vv R \right) .
\end{equation}
Now the electric field, using $K^{ct} = k = (\vv{\hat{k}} \cdot \vv k)$ and $\bar{A}^{ct} = \bar{\vv a} \cdot \vv k / k = (\vv{\hat{k}} \cdot \bar{\vv a} )$:
\begin{equation*}
\begin{aligned}
- \del A^{ct} - \partial^{ct} \vv a &= - \del \left( \bar{A}^{ct} \mathrm{e}^{\mathrm{i} f} \right) - \partial^{ct} \left( \bar{\vv a} \mkern1mu \mathrm{e}^{\mathrm{i} f} \right) \\
&= - \mathrm{i} \mathrm{e}^{\mathrm{i} f} \left( \bar{A}^{ct} \del f + \bar{\vv a} \, \partial^{ct} f \right) \\
&= - \mathrm{i} \mathrm{e}^{\mathrm{i} f} \left( K^{ct} \bar{\vv a} - \bar{A}^{ct} \vv k \right) \\[1pt]
&= - \mathrm{i} \mathrm{e}^{\mathrm{i} f} \left( \bar{\vv a} (\vv{\hat{k}} \cdot \vv k )  - \vv k (\vv{\hat{k}} \cdot \bar{\vv a} ) \right) \\[1pt]
&= - \mathrm{i} \mathrm{e}^{\mathrm{i} f} \, \vv{\hat{k}} \times (\bar{\vv a} \times \vv k) = - \mathrm{i} \mathrm{e}^{\mathrm{i} f} \, \bar{\vv b} \times \vv{\hat{k}}
\end{aligned}
\end{equation*}
(``$bac - cab$" rule). Defining $\bar{\vv e} \equiv \bar{\vv b} \times \vv{\hat{k}}$, the real part is:
\begin{equation}\label{eq:epw}
\vv e (\vv R) = \bar{\vv e} \, \sin \left( \vv K \cdot \vv R \right) .
\end{equation}
The takeaway here is that for a light wave,\footnote{We should say, ``for a \emph{progressive} light wave," since a superposition of plane waves may yield a \emph{standing} wave (as in a microwave oven), where the situation is a bit different. But the superposed plane waves are individually progressive.} $\vv e$, $\vv b$, and $\vv k$ (in cyclic permutations of that order) constitute a right-handed set of mutually perpendicular three-vectors, with $\vv e$ and $\vv b$ having equal magnitudes (recall our discovery in Section \ref{sssec:lff} that $\vv e \perp \vv b$ and $e = b$ are Lorentz-invariant conditions). It follows that the electromagnetic energy density $u = (e^2 + b^2) / 2$ at any point reduces to $u = e^2 = b^2$, and that the Poynting vector $\vv s = \vv e \times \vv b$ reduces to $\vv s = u \vv{\hat{k}} = e^2 \vv{\hat{k}} = b^2 \vv{\hat{k}}$.\footnote{\label{fn:epc}In Section \ref{sssec:emm}, we'll learn that the Poynting vector is the electromagnetic momentum density (\emph{in addition} to being the electromagnetic energy flux). This means that the relation $s = u$ gives $E = pc$ for light (we'll have derived Equation \ref{eq:3}).} By Equations \ref{eq:bpw} and \ref{eq:epw}, the wave's \emph{average} energy density is $\langle u \rangle = \bar{e}^2 / 2 = \bar{b}^2 / 2$ (the average of a squared sinusoidal function over the course of a cycle is always $1/2$), and the wave's \emph{average} Poynting vector is $\langle \vv s \rangle = \langle u \rangle \vv{\hat{k}} = \vv{\hat{k}} \mkern1mu \bar{e}^2 / 2 = \vv{\hat{k}} \mkern1mu \bar{b}^2 / 2$.


\subsubsection{Transforming Light's Energy}

Having established that a light wave's average energy density is $\langle u \rangle = \bar{e}^2 / 2$, we can learn how light's energy transforms under a Lorentz boost by transforming $\langle u \rangle V = \bar{e}^2 V / 2$, where $V$ is the volume containing the electromagnetic energy in the first frame. Of course, dividing by two has no effect on a transformation rule, and $\bar{\vv e}$ is just the maximum value of $\vv e$ during a cycle. So $\langle u \rangle$ and $u$ transform the same way, and our mission stated more simply is to transform $uV = e^2 V$.

For $u = e^2 = \vv e \cdot \vv e$, let's use Equations \ref{eq:ebr}:
\begin{equation*}
\begin{aligned}
u^\prime &= \left( \cosh{\phi} \; \vv e + \sinh{\phi} \; (\hatphi \times \vv b) - 2 \sinh^2 \frac{\phi}{2} \; (\hatphi \cdot \vv e) \hatphi \right)^2 \\
&= e^2 \cosh^2{\phi} + (\hatphi \times \vv b)^2 \sinh^2{\phi} + 4(\hatphi \cdot \vv e)^2 \sinh^4{\frac{\phi}{2}} \\
& \qquad + 2 \vv e \cdot (\hatphi \times \vv b) \cosh{\phi} \, \sinh{\phi} - 4 (\hatphi \cdot \vv e)^2 \cosh{\phi} \,\sinh^2{\frac{\phi}{2}}
\end{aligned}
\end{equation*}
($\hatphi \cdot (\hatphi \times \vv b) = 0$). What a mess! But our hyperbolic identities save the day. First consider just the two terms with $\phi / 2$ arguments, temporarily suppressing their common $(\hatphi \cdot \vv e)^2$ factor. Using $2\sinh^2(\phi/2) = \cosh \phi - 1$ and $\cosh^2 \phi - \sinh^2 \phi = 1$:
\begin{equation*}
\begin{aligned}
& 4 \sinh^4{\frac{\phi}{2}} - 4 \cosh{\phi} \,\sinh^2{\frac{\phi}{2}} \\[2pt]
& \qquad  = 4 \left[ \left( \dfrac{\cosh{\phi} - 1}{2} \right)^2 - \cosh{\phi} \left( \dfrac{\cosh{\phi} - 1}{2} \right) \right] \\[4pt]
& \qquad = \left( \cosh^2{\phi} - 2 \cosh{\phi} + 1 \right) - 2 \left( \cosh^2{\phi} - \cosh{\phi} \right) \\[3pt]
& \qquad = - \sinh^2{\phi} .
\end{aligned}
\end{equation*}
Now we can group $- (\hatphi \cdot \vv e)^2 \sinh^2{\phi}$ with the remaining $\sinh^2{\phi}$ term:
\begin{equation*}
u^\prime = e^2 \cosh^2{\phi} + \left[ (\hatphi \times \vv b)^2 - (\hatphi \cdot \vv e)^2 \right] \sinh^2{\phi} - 2 e^2 ( \hatphi \cdot \vv{\hat s}) \cosh{\phi} \, \sinh{\phi} ,
\end{equation*}
where we've also used $\vv e \cdot (\hatphi \times \vv b) = \hatphi \cdot (\vv b \times \vv e) = - e^2 (\hatphi \cdot \vv{\hat s})$ (Poynting vector is $\vv s = \vv e \times \vv b$, and $e = b$ for light). Next, work on what's in the square brackets, invoking the identity $(\vv q \times \vv w)^2 = q^2 w^2 - (\vv q \cdot \vv w)^2$:
\begin{equation*}
\begin{aligned}
(\hatphi \times \vv b)^2 - (\hatphi \cdot \vv e)^2 &= b^2 - (\hatphi \cdot \vv b)^2 - (\hatphi \cdot \vv e)^2 \\
&= e^2 \left( 1 - (\hatphi \cdot \vv{\hat b})^2 - (\hatphi \cdot \vv{\hat e})^2 \right) \\
&= e^2 ( \hatphi \cdot \vv{\hat s} )^2
\end{aligned}
\end{equation*}
(because $\vv e$, $\vv b$, and $\vv s$ are mutually perpendicular for light).\footnote{This is merely the Euclidean distance formula in disguise: $(\hatphi \cdot \vv{\hat e})$, $(\hatphi \cdot \vv{\hat b})$, and $( \hatphi \cdot \vv{\hat s} )$ are the direction cosines for $\hatphi$ with respect to the ``Cartesian axes" $\vv e$, $\vv b$, and $\vv s$, so their squares must sum to $1$.} Suddenly our transformation for $u$ looks quite nice:
\begin{equation*}
u^\prime = e^2 \left( \cosh^2{\phi} - 2 ( \hatphi \cdot \vv{\hat s}) \cosh{\phi} \, \sinh{\phi} + ( \hatphi \cdot \vv{\hat s} )^2 \sinh^2{\phi} \right) ,
\end{equation*}
or subbing in $u$ for $e^2$ and factoring:
\begin{equation}\label{eq:edt}
u^\prime = u \left( \cosh{\phi} - \sinh{\phi} \, \cos{\theta} \right)^2 ,
\end{equation}
$\theta$ being the angle between the boost direction $\hatphi$ and the light wave's direction of propagation $\vv{\hat s}$.

Comparing Equation \ref{eq:edt} to our ``target" energy transformation (Equation \ref{eq:8}), and knowing that we'll have to multiply $u^\prime$ by the transformed volume $V^\prime$ to get the transformed energy, we see that the volume transformation \emph{must} have the form $V^\prime = V / ( \cosh{\phi} - \sinh{\phi} \, \cos{\theta} )$. But what volume are we talking about here? The energy in question doesn't ``belong" to anything that has a rest frame; rather, it's being transported at the speed of light for all inertial observers ($\vv s = u \vv{\hat{k}}$). There's no proper volume in this context, and our ``volume contraction" formula will do us no good.\footnote{This is similar to how length contraction played no role in our derivation of the relativistic Doppler effect for wavelength back in Section \ref{sssec:rdf} (Equation \ref{eq:rdw}). In fact, it will turn out that wavelength and the volume we're seeking transform in the same way.}

Here's one way to think about the volume we're interested in (more or less how Einstein did it). An omnipotent inertial observer ``pauses" time and places a make-believe sphere around a patch of space through which a monochromatic plane wave is propagating (or was, anyway, before time was paused). The sphere must be unphysical because when the observer ``unpauses" time, it travels \emph{with} the wave at the speed of light, always enclosing the same ``part" of the wave and thus the same amount of electromagnetic energy---equal to the wave's (average) energy density times the sphere's volume. What we want to do is write an equation for the sphere's surface in terms of position and time in the omnipotent observer's ``unprimed" frame, use the Lorentz transformation to sub in the ``primed" coordinates (without expecting the primed observer to regard the shape as a sphere), and calculate the shape's unprimed and primed volumes accordingly.

In Cartesian coordinates, the equation for the surface of a sphere is:
\begin{equation*}
(x - x_0)^2 + (y - y_0)^2 + (z - z_0)^2 = R^2,
\end{equation*}
where $R$ is the sphere's radius and the point $(x_0, y_0, z_0)$ is the sphere's center. The volume is $V = 4 \pi R^3 / 3$. Let's say that our omnipotent inertial observer ``pauses" time at ${ct = 0}$ and centers the sphere around the unprimed spatial origin. The initial unprimed equation for the surface is then:
\begin{equation*}
x^2 + y^2 + z^2 = R^2.
\end{equation*}
When time is ``unpaused," the sphere and its center move at the speed of light in the direction of the light's three-wavevector $\vv k$. At some later time $ct = ct$, then, the $x$-coordinate of the sphere's center is its initial $x$-coordinate ($0$) plus the $x$-component of the light's ``displacement" since $ct = 0$, which is the $x$-component of the light's (normalized) velocity vector $\vv{\hat{k}}$ times $ct$.\footnote{The three-wavevector $\vv k$ isn't dimensionless, but the unit vector $\vv{\hat{k}} = \vv k / k$ is, so $\vv{\hat{k}}$ is indeed the light's normalized velocity. Same goes for $\vv{\hat s} = \vv s / s$.} If $\theta$ is the angle between the positive $x$-direction and $\vv{\hat{k}}$, then the $x$-component of $\vv{\hat{k}}$ is $\Vert \vv{\hat{k}} \Vert \cos{\theta} = \cos{\theta}$. The $x$-coordinate of the sphere's center is therefore $x_0 = \cos{\theta} \, ct$. Likewise, $y_0 = \cos{\alpha} \, \mkern.5mu ct$ and $z_0 = \cos{\kappa} \, \mkern.5mu ct$ for the corresponding angles $\alpha$ and $\kappa$, and the sphere's equation is:
\begin{equation*}
(x - \cos{\theta} \, ct)^2 + (y - \cos{\alpha} \, \mkern.5mu ct)^2 + (z - \cos{\kappa} \, \mkern.5mu ct)^2 = R^2 .
\end{equation*}

Under a standard-configuration boost, the primed frame moves in the positive $x$-direction, and $\theta$ is consequently the angle between the \emph{boost direction} and $\vv{\hat{k}}$. By Equations \ref{eq:lt}, $R^2$ in terms of the primed coordinates is:
\begin{equation*}
\begin{aligned}
&R^2 = \left[ \gamma (x^\prime + \beta ct^\prime )  - \cos{\theta} \, \gamma (ct^\prime + \beta x^\prime) \right]^2 \\[2pt]
& \qquad + \left[ y^\prime - \cos{\alpha} \, \mkern.5mu \gamma (ct^\prime + \beta x^\prime) \right]^2 + \left[ z^\prime - \cos{\kappa} \, \mkern.5mu \gamma (ct^\prime + \beta x^\prime) \right]^2.
\end{aligned}
\end{equation*}
The volume remains constant over time, so let's choose $ct^\prime = 0$:
\begin{equation}\label{eq:ell}
\left[ \gamma ( 1 - \beta \cos{\theta} ) \right]^2 x^{\prime \, 2} + \left( y^\prime -  \gamma \beta x^\prime \cos{\alpha} \right)^2 + \left( z^\prime - \gamma \beta x^\prime \cos{\kappa} \right)^2 = R^2 .
\end{equation}
As we anticipated, this primed equation isn't a sphere; it's an \emph{ellipsoid}. We need the ellipsoid's volume, which is proportional to the product of its three principal semi-axes. In his original paper on special relativity, Einstein shrugs this off as a ``simple calculation." It's not so simple! The problem is those $x^\prime$ terms in $(y^\prime - \gamma \beta x^\prime \cos \alpha)^2$ and $(z^\prime - \gamma \beta x^\prime \cos \kappa)^2$.\footnote{\label{fn:sph}These subtracted expressions are \emph{not} constants. At least one (highly recommended!) text makes this error, getting the right answer by happenstance while mischaracterizing the ellipsoid as a spheroid (an ellipsoid with at least two of its three principal semi-axes sharing a length): Ta-Pei Cheng, \emph{Einstein's Physics: Atoms, Quanta, and Relativity Derived, Explained, and Appraised} (Oxford: Oxford, 2013), 164.} If they weren't there, we could divide both sides by $R^2$ to put the ellipsoid in standard form, and the principal semi-axes would reveal themselves. But they're there, and we have to deal with them.

There's a hard way and a shortcut, and they both involve linear algebra. Let's take the shortcut first.\footnote{Hat tip: \url{https://www.mathpages.com/home/kmath354/kmath354.htm}.} The trick is that for the purpose of finding the ellipsoid's volume, \emph{we can pretend the troublesome terms aren't there at all}! That is, the volume of our ellipsoid is equal to the volume of \emph{another} ellipsoid that looks just like Equation \ref{eq:ell} without those terms. The reason is that a transformation of the form $X^\prime = x^\prime$, $Y^\prime = y^\prime + A x^\prime$, $Z^\prime = z^\prime + B x^\prime$ for constants $A$ and $B$ is a three-dimensional \emph{shearing}, and three-dimensional shearings \emph{preserve volume}. As a quick example of how this works, consider a cube in three-dimensional Euclidean space spanned by the basis vectors $\vv{\hat{x}} = \langle 1, 0, 0 \rangle$, $\vv{\hat{y}} = \langle 0, 1, 0 \rangle$, and $\vv{\hat{z}} = \langle 0, 0, 1 \rangle$. The volume of the cube (or indeed of any parallelepiped spanned by three vectors) is the absolute value of the scalar triple product: $| \vv{\hat{x}} \cdot (\vv{\hat{y}} \times \vv{\hat{z}}) | = | \vv{\hat{x}} \cdot \vv{\hat{x}} | = 1$. Now, the transformation matrix for the shearing we've described is:\footnote{Readers well-versed in linear algebra will know that the determinant of this matrix is the transformation's volume-scaling factor. It's $1$, so volume is preserved.}
\begin{equation*}
\mathcal{S} =
\begin{bmatrix}
1 & 0 & 0 \\
A & 1 & 0 \\
B & 0 & 1
\end{bmatrix}.
\end{equation*}
Applying the transformation to our vectors gives $\mathcal{S} [\vv{\hat{y}}] = [\vv{\hat{y}}]$ and $\mathcal{S} [\vv{\hat{z}}] = [\vv{\hat{z}}]$ but $\mathcal{S} [\vv{\hat{x}}] = [ \vv q ]$ where $\vv q = \langle 1, A, B \rangle$. Then the absolute value of the scalar triple product of our \emph{new} vectors is ${| \vv q \cdot (\vv{\hat{y}} \times \vv{\hat{z}}) | = | \langle 1, A, B \rangle \cdot \langle 1, 0, 0 \rangle | = 1}$, and the shearing has preserved the volume of the spanned solid. It's not difficult to obtain this result for a more complicated trio of original vectors (they needn't be mutually orthogonal or have the same length)---we'll always find that the magnitude of the scalar triple product (and hence the volume of the spanned parallelepiped) is the same before and after the transformation. We can appeal to integral calculus to extend the argument to our ellipsoid: if we approximate its structure by assembling many tiny parallelepipeds (cubes, say), then their total volume (preserved under a shearing) equals the volume of the ellipsoid in the limit that their number approaches infinity. Shearing the ellipsoid preserves its volume.

All that is to say, our ellipsoid has the same volume as this friendlier one:
\begin{equation*}
\left[ \gamma ( 1 - \beta \cos{\theta} ) \right]^2 x^{\prime \, 2} + y^{\prime \, 2} + z^{\prime \, 2} = R^2 .
\end{equation*}
The volume is $V^\prime = 4 \pi abc / 3$, where $a$, $b$, and $c$ are the lengths of the principal semi-axes, satisfying:
\begin{equation*}
\dfrac{x^{\prime \, 2}}{a^2} + \dfrac{y^{\prime \, 2}}{b^2} + \dfrac{z^{\prime \, 2}}{c^2} = 1 .
\end{equation*}
We have $b = c = R$ and $a = R / [\gamma ( 1 - \beta \cos{\theta} )]$, and so:
\begin{equation}\label{eq:vol}
\begin{split}
V^\prime &= \dfrac{4 \pi}{3} \, abc = \dfrac{4 \pi }{3} \, R^3 \, \dfrac{1}{\gamma \left( 1 - \beta \cos{\theta} \right)} \\
V^\prime &= \dfrac{V}{\gamma \left( 1 - \beta \cos{\theta} \right)} ,
\end{split}
\end{equation}
which is precisely the transformation we were expecting. Perhaps unsurprisingly, this volume transforms the same way as wavelength (Equation \ref{eq:rdw}). Combining Equations \ref{eq:edt} and \ref{eq:vol}, we see that the energy $E$ of a light wave, equal to its (average) energy density $u$ times a ``containing" volume $V$, transforms under a Lorentz boost like this:
\begin{equation}\label{eq:len}
\begin{split}
E^\prime &= u^\prime V^\prime \\
&= \left[ u \gamma^2 ( 1 - \beta \cos{\theta} )^2  \right] \left[ \dfrac{V}{\gamma ( 1 - \beta \cos{\theta} )} \right] \\[4pt]
&= uV \, \dfrac{\gamma^2 ( 1 - \beta \cos{\theta} )^2}{\gamma ( 1 - \beta \cos{\theta} )} \\[5pt]
E^\prime &= \gamma E \left( 1 - \beta \cos{\theta} \right) .
\end{split}
\end{equation}
Equations \ref{eq:len} and \ref{eq:8} are identical, and light's energy transforms the same way as its frequency (Equation \ref{eq:rdf}). We've made good on our promise to show this without resorting to the Planck--Einstein relation.

For the sake of completeness, here is the hard way to get the volume of the ellipsoid in Equation \ref{eq:ell} (consult a linear algebra text if you have trouble following along). Start by expanding it into quadratic form:
\begin{equation*}
Ax^{\prime \, 2} + y^{\prime \, 2} + z^{\prime \, 2} + D x^\prime y^\prime + E x^\prime z^\prime = R^2,
\end{equation*}
where $A = \gamma^2 (1 - \beta \cos \theta)^2 + (\gamma \beta)^2 ( \cos^2 \alpha + \cos^2 \kappa )$, $D = -2 \gamma \beta \cos \alpha$, and $E = -2 \gamma \beta \cos \kappa$. Note that $A = \gamma^2 (1 - \beta \cos \theta)^2 + (D^2 + E^2)/4$. In matrix form:
\begin{equation}\label{eq:ellm}
\begin{bmatrix}
x^\prime & y^\prime & z^\prime
\end{bmatrix}
\begin{bmatrix}
A & D/2 & E/2 \\
D/2 & 1 & 0 \\
E/2 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
x^\prime \\
y^\prime \\
z^\prime
\end{bmatrix}
= R^2 .
\end{equation}
To reduce this ellipsoid to standard form, we'll have to make the appropriate \emph{orthogonal} coordinate transformation (one that maps orthonormal bases to orthonormal bases). In the new coordinates, then, we'll have:
\begin{equation*}
\begin{bmatrix}
X^\prime & Y^\prime & Z^\prime
\end{bmatrix}
\begin{bmatrix}
\lambda_1 & 0 & 0 \\
0 & \lambda_2 & 0 \\
0 & 0 & \lambda_3
\end{bmatrix}
\begin{bmatrix}
X^\prime \\
Y^\prime \\
Z^\prime
\end{bmatrix}
= R^2 ,
\end{equation*}
where $\lambda_1$, $\lambda_2$, and $\lambda_3$ are the eigenvalues of the 3-by-3 matrix in Equation \ref{eq:ellm} (i.e., we diagonalize the original 3-by-3 to avoid cross-terms in the new coordinates). It follows that the lengths of the ellipsoid's principal semi-axes are $R / \sqrt{\lambda_1}$, $R / \sqrt{\lambda_2}$, and $R / \sqrt{\lambda_3}$, and we need only find the eigenvalues to determine the volume. We find them by solving for $\lambda$ here:
\begin{equation*}
\det
\left(
\begin{bmatrix}
A - \lambda & D/2 & E/2 \\
D/2 & 1 - \lambda & 0 \\
E/2 & 0 & 1 - \lambda
\end{bmatrix}
\right)
= 0 .
\end{equation*}
By Laplace expansion and some algebra, that's:
\begin{equation*}
\begin{aligned}
0 &= \left( A - \lambda \right) \left( 1 - \lambda \right)^2 - \dfrac{D}{2} \left( \dfrac{D}{2} \left(1 - \lambda \right) \right) + \dfrac{E}{2} \left( - \left(1 - \lambda \right) \dfrac{E}{2} \right) \\[5pt]
&= \left(1 - \lambda \right) \left( \lambda^2 - \left( 1 + A \right)\lambda + A - \dfrac{D^2 + E^2}{4} \right).
\end{aligned}
\end{equation*}
One eigenvalue is $1$. Getting the others isn't as bad as it looks. Sub in our $A$, $D$, and $E$ values, remembering that $A = \gamma^2 (1 - \beta \cos \theta)^2 + (D^2 + E^2)/4$:
\begin{equation*}
\lambda^2 - \left[ 1 + \gamma^2 (1 - \beta \cos \theta)^2 + (\gamma \beta)^2 ( \cos^2 \alpha + \cos^2 \kappa ) \right] \lambda + \gamma^2 (1 - \beta \cos \theta)^2 = 0.
\end{equation*}
Use $\cos^2 \alpha + \cos^2 \kappa = 1 - \cos^2 \theta$ (squared direction cosines sum to $1$), and simplify what's in the square brackets (we'll also need $\gamma^2 \beta^2 + 1 = \gamma^2$):
\begin{equation*}
\lambda^2 - 2 \gamma^2 ( 1 - \beta \cos \theta ) \lambda + \gamma^2 (1 - \beta \cos \theta)^2 = 0.
\end{equation*}
It's hard to spot, but because $(1 - \beta)(1 + \beta) = \gamma^{-2}$, that factors to:
\begin{equation*}
\left[ \lambda - \gamma^2 (1 - \beta \cos \theta) (1 + \beta) \right] \left[ \lambda - \gamma^2 (1 - \beta \cos \theta) (1 - \beta) \right] = 0,
\end{equation*}
and our other two eigenvalues are $\lambda = \gamma^2 (1 - \beta \cos \theta)(1 \pm \beta)$. The three principal semi-axes of the ellipsoid then have lengths $R$, $R /  \sqrt{\gamma^2 (1 - \beta \cos \theta)(1 + \beta)}$, and $R / \sqrt{\gamma^2 (1 - \beta \cos \theta)(1 - \beta)}$. Their product is $R^3 / [\gamma(1 - \beta \cos \theta)]$, and we can indeed reproduce Equation \ref{eq:vol}. Note that no two principal semi-axes of our ellipsoid have the same length. It is \emph{not} a spheroid (see Footnote \ref{fn:sph}).


\subsection[Supplement: "Electrodyadics" (Tensors Lite)]{Supplement: ``Electrodyadics" (Tensors Lite)}\label{ssec:dy}

\subsubsection{Three-Momentum and Euclidean Dyadics}\label{sssec:emm}

In Section \ref{sssec:lffd}, we simplified the temporal component of the Lorentz four-force density into a succinct statement of energy conservation. Now that we've taken care of our unfinished business concerning light and energy, we return to the Lorentz four-force density and simplify its three-vector \emph{spatial} component into a succinct statement of \emph{three-momentum} conservation. As we might expect from what we saw with Poynting's theorem (Equation \ref{eq:py}), this statement will equate the rate of decrease of electromagnetic three-momentum from a region to the outward ``flux" of electromagnetic three-momentum plus the rate at which the fields transfer three-momentum to particles within the region. Why the scare quotes around the word \emph{flux}? Because flux is the rate of flow of a \emph{scalar} quantity through a surface, and three-momentum is a \emph{vector} quantity. In particular, flux (of a scalar quantity) is the volume-integral of the divergence of a vector field, whereas the corresponding integrand for the ``flux" of a \emph{vector} quantity would have to be the ``divergence" of a \emph{rank-2 tensor} field (see Footnote \ref{fn:gd}). As we haven't dealt explicitly with tensor notation or the ``divergence" of anything but a vector field, this presents a challenge!\footnote{That said, we deal implicitly with rank-2 tensors every time we take the Laplacian or d'Alembertian of a vector field (the gradient of a vector is a rank-2 tensor).}

We take this opportunity to dip our toes into rank-2 tensors, without getting into their full machinery, and without getting bogged down in too much new notation. Actually, we've already done so: at the end of Section \ref{sec:in}, we represented one three-vector as a column matrix and another as a row matrix, we multiplied the matrices (in that order---column times row), and we mentioned in passing that the resulting 3-by-3 was the matrix representation of the \emph{dyadic product} of the two vectors (we used the symbol $\otimes$ for this operation). What we didn't mention is that the dyadic product is a special case of the \textbf{tensor product}, and that the \textbf{dyad} it outputs is indeed a kind of rank-2 tensor. We'll limit our discussion of rank-2 tensors to \textbf{dyadics}, which are dyads and their sums.

It's important to understand that tensors, like vectors (but unlike matrices), are \emph{geometric} objects, characterized by components that transform a certain way under a coordinate transformation. In fact, vectors \emph{are} tensors---of rank-1. And scalars are tensors of rank-0. A tensor's rank tells you (among other things) how many indices you need to uniquely specify one of its components. Not all three-dimensional Euclidean dyadics are dyads (that is, the dyadic product of two vectors), but those that aren't are \emph{sums} of dyads, and all Euclidean dyadics have nine ``doubled up" Cartesian components ($xx$, $xy$, $xz$, $yx$, $yy$, $yz$, $zx$, $zy$, $zz$), perfect for a square-matrix representation.\footnote{To keep things simple we'll continue to use Cartesian coordinates only.} (A rank-3 tensor can be represented by a cubic array, and so on.)

The geometric nature of tensors is why we're careful to use language like ``the matrix representation of a dyadic," and why we'll continue to use different notation for matrix representations of geometric objects than we do for the geometric objects themselves. For example, we'll use $[\vv e]$ for the column-matrix representation of the electric field, and we'll use $[\vv e]^{\textrm{T}}$ for the transpose of that matrix (i.e., for the row-matrix representation of $\vv e$). Likewise, $[\vv c][\vv d]^{\textrm{T}}$ is the matrix representation of the dyad $\vv c \otimes \vv d$, and ${[\vv d][\vv c]^{\textrm{T}} = ([\vv c][\vv d]^{\textrm{T}})^{\textrm{T}}}$ is the matrix representation of $\vv d \otimes \vv c$ (the dyadic product is not commutative). As for the divergence $\del \cdot \vv c$ (of a vector), we have the matrix representation of the dot product $[\del]^{\textrm{T}}[\vv c]$.

And what about the ``divergence" of a Euclidean dyadic? Or more generally, what about the Euclidean ``dot product" of a vector and a dyadic? The operation should produce a \emph{vector}. Let's let our matrix notation lead the way. If we have a vector $\vv m$ that we'd like to ``dot" with the dyad $\vv c \otimes \vv d$, there are actually \emph{two different} matrix products involving $[\vv c][\vv d]^{\textrm{T}}$ and a matrix representation of $\vv m$ that generate a vector. The first is $[\vv m]^{\mathrm{T}}[\vv c][\vv d]^{\textrm{T}}$, which gives the row-matrix representation of a vector, and the second is $[\vv c][\vv d]^{\textrm{T}}[\vv m]$, which gives the column-matrix representation of a \emph{different} vector. So let's define \emph{two} ``dot products" between a vector and a dyad: we define $\vv m \cdot (\vv c \otimes \vv d)$ as the vector $(\vv m \cdot \vv c) \vv d$ whose row-matrix representation is $[\vv m]^{\mathrm{T}}[\vv c][\vv d]^{\textrm{T}}$, and we define $(\vv c \otimes \vv d) \cdot \vv m$ as the vector $(\vv d \cdot \vv m) \vv c$ whose column-matrix representation is $[\vv c][\vv d]^{\textrm{T}}[\vv m]$. Since the matrix product is distributive over addition, and since a dyadic that isn't a dyad can be expressed as the sum of dyads, we can extend these dot-product definitions to dyadics generally (just replace $[\vv c][\vv d]^{\textrm{T}}$ with the square-matrix representation of the dyadic).\footnote{This will be a theme: define for dyads, and invoke linearity to extend to dyadics.} For the divergence, we can apply our first dot-product definition to $\del$, so that $\del \cdot (\vv c \otimes \vv d)$ is the vector whose row-matrix representation is $[\del]^{\textrm{T}}([\vv c][\vv d]^{\textrm{T}})$, where we've included the parentheses to make clear that $\del$ operates not just on $\vv c$ but on the dyadic $\vv c \otimes \vv d$. We'll make use of a product rule that goes $[\del]^{\textrm{T}}([\vv c][\vv d]^{\textrm{T}}) = [\del_{\vv c}]^{\textrm{T}}[\vv c][\vv d]^{\textrm{T}} + [\vv c]^{\textrm{T}}[\del_{\vv d}][\vv d]^{\textrm{T}}$ (if you need convincing, carry out the matrix multiplication). In our preferred vector notation, that's $\del \cdot (\vv c \otimes \vv d) = (\del \cdot \vv c) \vv d + (\vv c \cdot \del) \vv d$, so any time we spot the $(\del \cdot \vv c) \vv d + (\vv c \cdot \del) \vv d$ pattern in the wild we may express it as the divergence of $\vv c \otimes \vv d$.

One more preliminary. The Euclidean dyadic whose matrix representation (in Cartesian coordinates) is the identity matrix $I_3$ is the \textbf{unit dyadic}, which we'll notate $\inlinedy{\upiota}$ (that's a lowercase roman iota topped with a double-sided arrow). Its divergence $\del \cdot \inlinedy{\upiota}$ vanishes, since $[\del]^{\textrm{T}}I_3 = [\vv 0]^{\textrm{T}}$. Note that for a scalar function $f$, the divergence $\del \cdot (f \mkern-2mu \inlinedy{\upiota})$ is just $\del f$ (gradient of $f$) because $[\del]^{\textrm{T}}(f I_3) = [\del f]^{\textrm{T}} I_3 + f [\del]^{\textrm{T}} I_3 = [\del f]^{\textrm{T}}$.\footnote{The unit dyadic is related to the more broadly applicable \textbf{Kronecker delta}.}

With all that out of the way, let's proceed. From Equations \ref{eq:lffdc} and \ref{eq:lffdc2}:
\begin{equation*}
\dfrac{\dd \vv f_{\mathrm{L}}}{\dd V} = \left( \del \cdot \vv e \right) \vv e - \vv b \times \left( \del \times \vv b \right) + \vv b \times \partial^{ct} \vv e .
\end{equation*}
The first term on the right we recognize as one of the two ingredients we'd need to make $\del \cdot (\vv e \otimes \vv e)$. The ingredient we don't have is $(\vv e \cdot \del) \vv e$, which we could get as the ``$cab$" part of a ``$bac - cab$" expansion of $\vv e \times (\del \times \vv e)$ if only we had an $\vv e \times (\del \times \vv e)$. On the flip side, we do have a $\vv b \times (\del \times \vv b)$, which we could ``$bac - cab$" to get the $(\vv b \cdot \del) \vv b$ ingredient of $\del \cdot (\vv b \otimes \vv b)$, but we're missing $(\del \cdot \vv b) \vv b$. Well, actually we \emph{do} have $(\del \cdot \vv b) \vv b$, because by Maxwell's equations that's just $\vv 0$ (Equations \ref{eq:me}). As for $\vv e \times (\del \times \vv e)$, consider the last term in the equation. By the product rule for the cross product (mind the signs!), $\vv b \times \partial^{ct} \vv e$ is equivalently $\vv e \times \partial^{ct} \vv b - \partial^{ct} \left( \vv e \times \vv b \right)$, which we can shorten to $\vv e \times \partial^{ct} \vv b - \partial^{ct} \vv s$ (Poynting vector). But since Maxwell's equations also give us $\partial^{ct} \vv b = - \del \times \vv e$, we have $\vv b \times \partial^{ct} \vv e = - \vv e \times (\del \times \vv e) - \partial^{ct} \vv s$, and so:
\begin{equation*}
\dfrac{\dd \vv f_{\mathrm{L}}}{\dd V} = \left( \del \cdot \vv e \right) \vv e - \vv e \times (\del \times \vv e) + (\del \cdot \vv b) \vv b - \vv b \times \left( \del \times \vv b \right) - \partial^{ct} \vv s .
\end{equation*}
Now use the ``$bac - cab$" rule on the differential vector triple products, noting that the ``$bac$" parts are $\frac{1}{2} \del (\vv e \cdot \vv e)$ and $\frac{1}{2} \del (\vv b \cdot \vv b)$, which sum to $\del u$ (gradient of the electromagnetic energy density):
\begin{equation*}
\begin{aligned}
\dfrac{\dd \vv f_{\mathrm{L}}}{\dd V} &= \left( \del \cdot \vv e \right) \vv e + \left( \vv e \cdot \del \right) \vv e + (\del \cdot \vv b) \vv b + \left( \vv b \cdot \del \right) \vv b - \del u - \partial^{ct} \vv s \\
&= \del \cdot (\vv e \otimes \vv e) + \del \cdot (\vv b \otimes \vv b) - \del \cdot (u \mkern-4.5mu \inlinedy{\upiota}) - \partial^{ct} \vv s \\[2pt]
&= \del \cdot \inlinedy{\upsigma} - \, \partial^{ct} \vv s ,
\end{aligned}
\end{equation*}
where the dyadic $\inlinedy{\upsigma} \equiv \vv e \otimes \vv e + \vv b \otimes \vv b - u \mkern-4.5mu \inlinedy{\upiota}$ is the \textbf{Maxwell stress tensor}. Rearranging, our statement of momentum conservation is:
\begin{equation}\label{eq:tmc}
\boxed{ - \partial^{ct} \vv s = - \del \cdot \inlinedy{\upsigma} \, + \, \dfrac{\dd \vv f_{\mathrm{L}}}{\dd V} } \, .
\end{equation}
A comparison with Poynting's theorem (Equation \ref{eq:py}) shows that $\vv s$ and $\inlinedy{\upsigma}$ are to electromagnetic three-momentum as $u$ and $\vv s$ are to electromagnetic energy. The Poynting vector does ``double duty": from Poynting's theorem we learned that it represents the flow of electromagnetic energy, but with Equation \ref{eq:tmc} we see that it's also the \textbf{electromagnetic momentum density}.\footnote{In light of what we learned in Section \ref{sssec:li} (see Footnote \ref{fn:epc}), we've now derived $E = pc$ for electromagnetic waves (full version of Equation \ref{eq:3}).} And the Maxwell stress tensor represents the flow of electromagnetic three-momentum. Don't worry about the sign difference in the divergence terms---the volume-integral of $- \del \cdot \inlinedy{\upsigma}$ is indeed the \emph{outward} flux of electromagnetic three-momentum from a region.


\subsubsection{Minkowski Dyadics; Metric and Field Tensors}\label{sssec:md}

Let's take a closer look at the matrix representation of the Euclidean dyadic product $\vv c \otimes \vv d$:
\begin{equation*}
\begin{bmatrix}
c_x \\
c_y \\
c_z
\end{bmatrix}
\begin{bmatrix}
d_x & d_y & d_z
\end{bmatrix}
=
\begin{bmatrix}
c_x d_x & c_x d_y & c_x d_z \\
c_y d_x & c_y d_y & c_y d_z \\
c_z d_x & c_z d_z & c_z d_z 
\end{bmatrix} .
\end{equation*}
Under a counterclockwise rotation of the $x$- and $y$-axes through the angle $\theta$, the components of $\vv c$ transform like
\begin{equation*}
\begin{bmatrix}
c_x^\prime \\[1pt]
c_y^\prime \\[1pt]
c_z^\prime
\end{bmatrix}
=
\begin{bmatrix}
\cos \theta & \sin \theta & 0 \\
- \sin \theta & \cos \theta & 0 \\
0 & 0 & 1 
\end{bmatrix}
\begin{bmatrix}
c_x \\
c_y \\
c_z
\end{bmatrix} ,
\end{equation*}
and likewise for $\vv d$. If we label that 3-by-3 rotation matrix $R$, it follows that the nine ``doubled up" Cartesian components of the dyad $\vv c \otimes \vv d$ transform by two ``applications" of the rotation matrix like this:
\begin{equation*}
[\vv c^\prime \mkern1mu ] [\vv d^\prime \mkern1mu ]^{\mathrm{T}} = ( R [\vv c] ) ( R [\vv d] )^\mathrm{T} = R \, [\vv c] [\vv d]^{\mathrm{T}} R^{\mathrm{T}} ,
\end{equation*}
where $R^{\mathrm{T}}$ is the matrix transpose of $R$ (swap the signs of the sines). Indeed, the Cartesian components of \emph{every} Euclidean dyadic (dyad or sum of dyads) transform this way under the specified rotation, with the matrix representation of the dyadic flanked on the left by $R$ and on the right by $R^{\mathrm{T}}$. The Maxwell stress tensor from the previous section, for example:
\begin{equation*}
[ \mkern2mu {\inlinedy{\upsigma}} ^{\mkern1mu \prime} \mkern1mu ] = R \mkern1mu [\mkern1.5mu \inlinedy{\upsigma} \mkern1.5mu] R^{\mathrm{T}}
\end{equation*}
(we've extended our bracket notation to signify a \emph{dyadic's} square-matrix representation). Euclidean dyadics can be \emph{defined} as geometric objects whose Cartesian components transform like this under a rotation of axes.

In a similar way, a dyad in Minkowski spacetime (a \textbf{Minkowski dyad} or a \textbf{four-dyad}) is the dyadic product of \emph{four}-vectors---e.g., $\vv Q \otimes \vv W$---and its \emph{sixteen} ``doubled up" ($ct$ + Cartesian) components consequently transform like so under a \emph{Lorentz} transformation:
\begin{equation*}
[\vv Q^\prime \mkern1mu ] [\vv W^\prime \mkern1mu ]^{\mathrm{T}} = ( \Lambda [\vv Q] ) ( \Lambda [\vv W] )^\mathrm{T} = \Lambda [\vv Q] [\vv W]^{\mathrm{T}} \Lambda^{\mathrm{T}} ,
\end{equation*}
where $\Lambda$ is the Lorentz-transformation matrix and $\Lambda^{\mathrm{T}}$ is its transpose (they're both Equation \ref{eq:bm} if the transformation is a standard-configuration boost along the $x$-axis). \emph{Dyadics} in Minkowski spacetime (\textbf{Minkowski dyadics} or \textbf{four-dyadics}) are four-dyads and their sums. They can be \emph{defined} as geometric objects whose sixteen ``doubled up" ($ct$ + Cartesian) components transform like those of four-dyads under a Lorentz transformation:
\begin{equation}\label{eq:ltd}
\boxed{ [ \mkern1mu {\capdy{\mathrm{D}} {\vphantom{D}}^\prime} \mkern1mu ] = \Lambda [ \capdy{\mathrm{D}} ] \Lambda^{\mathrm{T}}}
\end{equation}
(we're keeping our double-sided arrow notation, though instead of the lowercase roman type that we use for Euclidean three-dyadics we'll prefer \emph{uppercase} roman for four-dyadics, mirroring our use of lowercase for three-vectors and uppercase for four-vectors).

In the context of electromagnetism, a particularly useful four-dyadic (field) is $\partialup \otimes \vv A - \vv A \otimes \partialup_{\vv A}$.\footnote{The quantity $\partialup \otimes \vv A$ is the \emph{gradient} of the four-potential.} In matrix notation, that's $[\partialup][\vv A]^{\mathrm{T}} - [\vv A][\partialup_{\vv A}]^{\mathrm{T}}$, or:
\begin{equation*}
\begin{bmatrix}
\partial^{ct} \\
\partial^x \\
\partial^y \\
\partial^z
\end{bmatrix}
\begin{bmatrix}
A^{ct} & A^x & A^y & A^z
\end{bmatrix}
-
\begin{bmatrix}
A^{ct} \\
A^x \\
A^y \\
A^z
\end{bmatrix}
\begin{bmatrix}
\partial^{ct} & \partial^x & \partial^y & \partial^z
\end{bmatrix}_{\vv A},
\end{equation*}
which comes to
\begin{equation*}
\begin{bmatrix}
0 & ( \partial^{ct} A^x - \partial^x A^{ct} ) & ( \partial^{ct} A^y - \partial^y A^{ct} ) & ( \partial^{ct} A^z - \partial^z A^{ct} ) \\[1.5ex]
( \partial^x A^{ct} - \partial^{ct} A^x ) & 0 & ( \partial^x A^y - \partial^y A^x ) & ( \partial^x A^z - \partial^z A^x ) \\[1.5ex]
( \partial^y A^{ct} - \partial^{ct} A^y ) & ( \partial^y A^x - \partial^x A^y ) & 0 & ( \partial^y A^z - \partial^z A^y ) \\[1.5ex]
( \partial^z A^{ct} - \partial^{ct} A^z ) & ( \partial^z A^x - \partial^x A^z ) & ( \partial^z A^y - \partial^y A^z ) & 0
\end{bmatrix} .
\end{equation*}
We call this dyadic the \textbf{Faraday tensor}, or the \textbf{electromagnetic field tensor}.\footnote{\label{fn:dy}Specifically, dyadics are \emph{contravariant} rank-2 tensors, a detail that matters in more advanced treatments of Minkowski spacetime (doesn't matter for Euclidean space---in practice, anyway).} Labeling it $\inlinedy{\mathrm{F}} = \inlinedy{\mathrm{F}} (\vv R)$, and invoking Equations \ref{eq:ebc}, we have:
\begin{equation}\label{eq:emt}
[\capdy{\mathrm{F}}]
=
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\[.5ex]
e_x & 0 & -b_z & b_y \\[.5ex]
e_y & b_z & 0 & -b_x \\[.5ex]
e_z & -b_y & b_x & 0
\end{bmatrix} .
\end{equation}
The matrix is antisymmetric, and we call the dyadic itself antisymmetric. (Likewise, a dyadic whose matrix representation is \emph{symmetric} is said to be symmetric.) It's also convenient to call a dyadic's \emph{transpose} the dyadic whose matrix representation is the transpose of the first's. Then we can say ${\inlinedy{\mathrm{F}} = \partialup \otimes \vv A - \vv A \otimes \partialup_{\vv A} = \partialup \otimes \vv A - (\partialup \otimes \vv A)^\mathrm{T}}$, dropping the subscript notation. Finally, there's a shorthand notation for this procedure of producing an antisymmetric dyadic from a pair of vectors: $\vv Q \wedge \vv W \equiv \vv Q \otimes \vv W - \vv W \otimes \vv Q$. So:
\begin{equation}\label{eq:emta}
\boxed{ \vphantom{A_{A_A}} \capdy{\mathrm{F}} \equiv \partialup \wedge \vv A } \, . 
\end{equation}

To see why the Faraday tensor is useful, we'll define the Minkowski dot product(s) between a four-dyadic $\inlinedy{\mathrm{D}}$ and a four-vector $\vv Q$. We can let the matrix notation guide us again, but this time there are negative signs to account for---i.e., the matrix representation of $\vv Q \cdot \vv W$ (dot product of two four-vectors) isn't $[\vv Q]^{\textrm{T}} [\vv W]$ but is rather $[\vv Q]^{\textrm{T}} \eta [\vv W]$, with
\begin{equation}\label{eq:mmt}
\eta
=
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & -1 & 0 \\
0 & 0 & 0 & -1
\end{bmatrix} .
\end{equation}
Using the matrix $\eta$, we define $\vv S \cdot (\vv Q \otimes \vv W)$ as the four-vector $(\vv S \cdot \vv Q) \vv W$ whose row-matrix representation is $[\vv S]^\mathrm{T} \eta [\vv Q] [\vv W]^\mathrm{T}$, and we define $(\vv Q \otimes \vv W) \cdot \vv S$ as the four-vector $(\vv W \cdot \vv S) \vv Q$ whose column-matrix representation is $[\vv Q] [\vv W]^\mathrm{T} \eta [\vv S]$. By the same logic we used with Euclidean dyadics (the matrix product is distributive over addition, and a dyadic that isn't a dyad can be expressed as the sum of dyads), we extend these operations for four-dyads to four-\emph{dyadics} by replacing $[\vv Q][\vv W]^{\textrm{T}}$ with the square-matrix representation of the dyadic in question. Then $\vv Q \cdot \inlinedy{\mathrm{D}}$ is the four-vector whose row-matrix representation is $[\vv Q]^{\mathrm{T}} \eta [\inlinedy{\mathrm{D}}]$, and the ``reverse" operation $\inlinedy{\mathrm{D}} \cdot \, \vv Q$ gives the four-vector whose column-matrix representation is $[\inlinedy{\mathrm{D}}] \eta [\vv Q]$. The four-\emph{divergence} of the dyadic $\inlinedy{\mathrm{D}}$ we notate $\partialup \cdot \inlinedy{\mathrm{D}}$, and so we define it as the four-vector whose row-matrix representation is $[\partialup]^{\mathrm{T}} \eta [\inlinedy{\mathrm{D}}]$. When we have the combination $[\partialup]^\mathrm{T} \eta$ in our matrix equations, it's understood that $\partialup$ operates on what's to the \emph{right} of $\eta$, after we matrix-multiply $\eta$ with one of its neighbors to give everything the right sign. As a quick example:\footnote{Notice that the product $[\partialup]^\mathrm{T} \eta$ is a row matrix whose entries are our \emph{positive} partial derivatives. Back in Section \ref{sssec:fd}, we found that these four operators together transform \emph{inverse}-Lorentzianly, and we mentioned in Footnote \ref{fn:cv} that an object whose components transform this way is a four-covector (or covariant four-vector). We won't make direct use of the covector concept, but we remark in passing that for \emph{any} four-vector $\vv Q$, the products $[\vv Q]^\mathrm{T} \eta$ and $\eta [\vv Q]$ give matrix representations of the covector whose components are $\langle Q_{ct}, \, Q_x, \, Q_y, \, Q_z \rangle = \langle Q^{ct}, \, -Q^x, \, -Q^y, \, -Q^z \rangle$.}
\begin{equation*}
\begin{aligned}
\mkern0mu [\partialup]^{\mathrm{T}} \eta [\capdy{\mathrm{D}}]
&=
\begin{bmatrix}
\partial^{ct} & \partial^x & \partial^y & \partial^z
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & -1 & 0 \\
0 & 0 & 0 & -1
\end{bmatrix}
\begin{bmatrix}
D^{ct \mkern1.5mu ct} & D^{ct \mkern1.5mu x} & D^{ct \mkern1.5mu y} & D^{ct \mkern1.5mu z} \\
D^{x \mkern1.5mu ct} & D^{x x} & D^{x y} & D^{x z}  \\
D^{y \mkern1.5mu ct} & D^{y x} & D^{y y} & D^{y z}  \\
D^{z \mkern1.5mu ct} & D^{z x} & D^{z y} & D^{z z} 
\end{bmatrix}
\\[3pt]
&=
\begin{bmatrix}
\partial^{ct} & - \partial^x & - \partial^y & -\partial^z
\end{bmatrix}
\begin{bmatrix}
D^{ct \mkern1.5mu ct} & D^{ct \mkern1.5mu x} & D^{ct \mkern1.5mu y} & D^{ct \mkern1.5mu z} \\
D^{x \mkern1.5mu ct} & D^{x x} & D^{x y} & D^{x z}  \\
D^{y \mkern1.5mu ct} & D^{y x} & D^{y y} & D^{y z}  \\
D^{z \mkern1.5mu ct} & D^{z x} & D^{z y} & D^{z z} 
\end{bmatrix}
\\[3pt]
&=
\begin{bmatrix}
\partial^{ct} & \partial^x & \partial^y & \partial^z
\end{bmatrix}
\begin{bmatrix}
D^{ct \mkern1.5mu ct} & D^{ct \mkern1.5mu x} & D^{ct \mkern1.5mu y} & D^{ct \mkern1.5mu z} \\
- D^{x \mkern1.5mu ct} & - D^{x x} & - D^{x y} & - D^{x z}  \\
- D^{y \mkern1.5mu ct} & - D^{y x} & - D^{y y} & - D^{y z}  \\
- D^{z \mkern1.5mu ct} & - D^{z x} & - D^{z y} & - D^{z z} 
\end{bmatrix}.
\end{aligned}
\end{equation*}

Really, Equation \ref{eq:mmt} is the matrix representation of a four-dyadic, since it satisfies the four-dyadic transformation rule Equation \ref{eq:ltd} (try it!---under a Lorentz boost, the $1$ transforms to $1$, the $-1$'s transform to $-1$'s, and the $0$'s transform to $0$'s). This dyadic is called the \textbf{Minkowski metric tensor}, and we use it implicitly every time we take a Minkowski dot product.\footnote{To be precise, what we use implicitly when we take Minkowski dot products is the \emph{covariant} Minkowski metric tensor, whereas the dyadic we're introducing is the \emph{contravariant} Minkowski metric tensor (see Footnote \ref{fn:dy}).} It's essentially the spacetime equivalent of the Euclidean unit dyadic we encountered in Section \ref{sssec:emm}. When we want to notate it explicitly \emph{as} a four-dyadic, we'll write $\inlinedy{\upeta}$ (a conventional exception to our preference for uppercase). Mostly, though, we'll keep using the familiar dot notation and throw $\eta = [ \mkern1mu \inlinedy{\upeta} \mkern.5mu ]$ into matrix equations as needed.

Anyway, now we can write the Lorentz four-force (Equation \ref{eq:ff}) more compactly in terms of the Faraday tensor:
\begin{equation}\label{eq:lfffd}
\boxed{ \vphantom{A_{A_A}} \vv F_{\mathrm{L}} = q \capdy{\mathrm{F}} \cdot \vv B } \, ,
\end{equation}
as we can verify with Equations \ref{eq:emt}, \ref{eq:mmt}, and \ref{eq:31}:
\begin{equation*}
q [\capdy{\mathrm{F}}] \eta [\vv B]
=
q
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\
e_x & 0 & -b_z & b_y \\
e_y & b_z & 0 & -b_x \\
e_z & -b_y & b_x & 0
\end{bmatrix}
\begin{bmatrix}
\gamma \\
- \gamma \beta_x \\
- \gamma \beta_y \\
- \gamma \beta_z
\end{bmatrix}
=
q
\begin{bmatrix}
\gamma \vvbeta \cdot \vv e \\
[ \gamma \vv e + \gamma \vvbeta \times \vv b ]
\end{bmatrix}
\end{equation*}
(cf. Equation \ref{eq:lfc}). And remember when we noted that Maxwell's equations (Equations \ref{eq:me}) actually constitute two four-vectors, and that their forms suggest expressions involving $\partialup$? Well, now we have the first of them (for $\del \cdot \vv e = \rho$ and $ \del \times \vv b - \partial^{ct} \vv e = \vv j$):
\begin{equation}\label{eq:ime}
\boxed{ \vphantom{A_{A_A}} \partialup \cdot \capdy{\mathrm{F}} = \vv J } \, ,
\end{equation}
where the left side in matrix notation is:
\begin{equation*}
[\partialup]^{\mathrm{T}} \eta [\capdy{\mathrm{F}}]
=
\begin{bmatrix}
\partial^{ct} & - \partial^x & - \partial^y & - \partial^z
\end{bmatrix}
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\
e_x & 0 & -b_z & b_y \\
e_y & b_z & 0 & -b_x \\
e_z & -b_y & b_x & 0
\end{bmatrix} ,
\end{equation*}
giving $\partialup \cdot \inlinedy{\mathrm{F}} = \langle \del \cdot \vv e , \, - \partial^{ct} \vv e + \del \times \vv b \rangle$ (mind that $\del = \langle - \partial^x , - \partial^y , - \partial^z \rangle$). As for the other one (for $\del \cdot \vv b = 0$ and $\del \times \vv e + \partial^{ct} \vv b = \vv 0$), we note that the left sides of the remaining Maxwell equations are just like the left sides of the first two, but with the roles of $\vv e$ and $\vv b$ reversed and a sign changed. It should therefore be possible to form a new Minkowski dyadic $\inlinedy{\mathrm{G}}$ from the Faraday tensor (Equation \ref{eq:emt}) by ``swapping" the $\vv e$ and $\vv b$ components and flipping the appropriate signs; the four-divergence of this new dyadic should equal the zero four-vector $\textrm{\mbox{\boldmath $\emptyset$}}$. A bit of thought (or trial and error) shows that the way to do this is to replace $\vv e$ with $\vv b$ and $\vv b$ with $- \vv e$:
\begin{equation}\label{eq:mt}
[ \capdy{\mathrm{G}} ]
=
\begin{bmatrix}
0 & -b_x & -b_y & -b_z \\[.5ex]
b_x & 0 & e_z & -e_y \\[.5ex]
b_y & -e_z & 0 & e_x \\[.5ex]
b_z & e_y & -e_x & 0
\end{bmatrix}
\end{equation}
(to verify that $\inlinedy{\mathrm{G}}$ is indeed a four-dyadic, check that its components transform according to Equation \ref{eq:ltd} under a Lorentz boost; for standard configuration, use Equations \ref{eq:bm} and \ref{eq:ebt}). Then:
\begin{equation}\label{eq:hme}
\boxed{ \vphantom{A_{A_A}} \partialup \cdot \capdy{\mathrm{G}} = \textrm{\mbox{\boldmath $\emptyset$}} } \, ,
\end{equation}
where the left side in matrix notation is:
\begin{equation*}
[\partialup]^{\mathrm{T}} \eta [\capdy{\mathrm{G}}]
=
\begin{bmatrix}
\partial^{ct} & - \partial^x & - \partial^y & - \partial^z
\end{bmatrix}
\begin{bmatrix}
0 & -b_x & -b_y & -b_z \\
b_x & 0 & e_z & -e_y \\
b_y & -e_z & 0 & e_x \\
b_z & e_y & -e_x & 0
\end{bmatrix} ,
\end{equation*}
giving $\partialup \cdot \inlinedy{\mathrm{G}} = \langle \del \cdot \vv b , \, - \partial^{ct} \vv b - \del \times \vv e \rangle$. By Equations \ref{eq:ebc}, $[\inlinedy{\mathrm{G}}]$ is also:
\begin{equation*}
\begin{bmatrix}
0 & ( \partial^y A^z - \partial^z A^y ) & ( \partial^z A^x - \partial^x A^z ) & ( \partial^x A^y - \partial^y A^x ) \\[1.5ex]
( \partial^z A^y - \partial^y A^z ) & 0 & ( \partial^z A^{ct} - \partial^{ct} A^z ) & ( \partial^{ct} A^y - \partial^y A^{ct} ) \\[1.5ex]
( \partial^x A^z - \partial^z A^x ) & ( \partial^{ct} A^z - \partial^z A^{ct} ) & 0 & ( \partial^x A^{ct} - \partial^{ct} A^x ) \\[1.5ex]
( \partial^y A^x - \partial^x A^y ) & ( \partial^y A^{ct} - \partial^{ct} A^y ) & ( \partial^{ct} A^x - \partial^x A^{ct} ) & 0
\end{bmatrix} .
\end{equation*}
Using \emph{that} matrix to carry out $[\partialup]^{\mathrm{T}} \eta [\inlinedy{\mathrm{G}}]$, we see clearly why each component of $\partialup \cdot \inlinedy{\mathrm{G}}$ vanishes. For example, the time component is:
\begin{equation*}
(\partialup \cdot \capdy{\mathrm{G}})^{ct} = - \partial^x ( \partial^z A^y - \partial^y A^z ) - \partial^y ( \partial^x A^z - \partial^z A^x ) - \partial^z ( \partial^y A^x - \partial^x A^y ) ,
\end{equation*}
with each term canceling its additive inverse. So we get Equation \ref{eq:hme} ``for free" by virtue of the Faraday tensor's having the form $\partialup \wedge \vv A$.

Equations \ref{eq:ime} and \ref{eq:hme} are Maxwell's equations in an explicitly relativistic and \emph{gauge-invariant} form. Equation \ref{eq:fp} contains the same information but is specific to the Lorenz gauge. By the way, our new four-dyadic $\inlinedy{\mathrm{G}}$ is called the \textbf{Maxwell tensor} (not to be confused with the Euclidean Maxwell \emph{stress} tensor $\inlinedy{\upsigma}$ that we met in Section \ref{sssec:emm}).


\subsubsection{Hodge Duals and Quasi-Four-Vectors}

The Maxwell tensor $\inlinedy{\mathrm{G}}$ is the \textbf{Hodge dual} of the Faraday tensor $\inlinedy{\mathrm{F}}$. To explain what this means ``geometrically," we first introduce a limited sense in which three-vectors may be treated as ``quasi"-four-vectors: \emph{at a single moment for a given inertial frame}, we can ``append" a zero temporal component to a three-vector (that isn't already the spatial component of a true four-vector) and manipulate the four ``components" as if they constituted a spacelike four-vector orthogonal to the frame's (normalized) four-velocity $\vv B = \langle 1, \vv 0 \rangle$. We stress that such quasi-four-vectors aren't \emph{actually} four-vectors---their ``components" don't transform Lorentzianly between frames! But sometimes intra-frame equations involving them \emph{have the same form} in all inertial frames.

We'll now demonstrate that for a given inertial frame, we can form an antisymmetric Minkowski dyadic $\inlinedy{\mathrm{A}}$ in terms of two of these quasi-four-vectors $\tilde{\vv q} = \langle 0, \vv q \rangle$ and $\tilde{\vv w} = \langle 0, \vv w \rangle$ like this:
\begin{equation*}
\capdy{\mathrm{A}} = \tilde{\vv q} \wedge \vv B - \{ \tilde{\vv w} \times \},
\end{equation*}
where $\vv B$ is the frame's four-velocity $\langle 1, \vv 0 \rangle$, and where $\{ \tilde{\vv w} \times \}$ denotes the quasi-four-dyadic that satisfies $\{ \tilde{\vv w} \times \} \cdot \vv H = \langle 0, \vv w \times \vv h \rangle$ for \emph{any} four-vector ${\vv H = \langle H^{ct}, \vv h \rangle}$ (again, we're working with one inertial frame at a single moment). Explicitly, the matrix representation of $\{ \tilde{\vv w} \times \}$ must be:
\begin{equation}\label{eq:wt}
[\{ \tilde{\vv w} \times \}] =
\begin{bmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & w_z & -w_y \\
0 & -w_z & 0 & w_x \\
0 & w_y & -w_x & 0
\end{bmatrix},
\end{equation}
so that $[\{ \tilde{\vv w} \times \}] \eta [\vv H]$ (the matrix representation of $\{ \tilde{\vv w} \times \} \cdot \vv H$) is:
\begin{equation*}
\begin{bmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & w_z & -w_y \\
0 & -w_z & 0 & w_x \\
0 & w_y & -w_x & 0
\end{bmatrix}
\begin{bmatrix}
H^{ct} \\
- h_x \\
- h_y \\
- h_z
\end{bmatrix}
=
\begin{bmatrix}
0 \\
w_y h_z - w_z h_y \\
w_z h_x - w_x h_z \\
w_x h_y - w_y h_x
\end{bmatrix},
\end{equation*}
which is indeed the column-matrix representation of $\langle 0, \vv w \times \vv h \rangle$. Then for the antisymmetric quasi-four-dyadic $\tilde{\vv q} \wedge \vv B = \tilde{\vv q} \otimes \vv B - \vv B \otimes \tilde{\vv q}$, we have the matrix representation:
\begin{equation}\label{eq:qb}
\begin{bmatrix}
0 \\
q_x \\
q_y \\
q_z
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 0 & 0
\end{bmatrix}
-
\begin{bmatrix}
1 \\
0 \\
0 \\
0
\end{bmatrix}
\begin{bmatrix}
0 & q_x & q_y & q_z
\end{bmatrix}
=
\begin{bmatrix}
0 & -q_x & -q_y & -q_z \\
q_x & 0 & 0 & 0 \\
q_y & 0 & 0 & 0 \\
q_z & 0 & 0 & 0
\end{bmatrix}.
\end{equation}
Subtracting Equation \ref{eq:wt} from Equation \ref{eq:qb}, we get:
\begin{equation*}
[\capdy{\mathrm{A}}] = [\tilde{\vv q} \wedge \vv B] - [\{ \tilde{\vv w} \times \}]
=
\begin{bmatrix}
0 & -q_x & -q_y & -q_z \\[.5ex]
q_x & 0 & -w_z & w_y \\[.5ex]
q_y & w_z & 0 & -w_x \\[.5ex]
q_z & -w_y & w_x & 0
\end{bmatrix},
\end{equation*}
and we see that the matrix representation of $\inlinedy{\mathrm{A}}$ has exactly the same form as that of the Faraday tensor $\inlinedy{\mathrm{F}}$ (Equation \ref{eq:emt}). It follows that if we transform $\vv q$ and $\vv w$ under a Lorentz boost in the same way that we transform the electric and magnetic fields (Equations \ref{eq:ebt} or \ref{eq:ebr}), we'll find that the form $\inlinedy{\mathrm{A}} = \tilde{\vv q} \wedge \vv B - \{ \tilde{\vv w} \times \}$ holds for \emph{all} inertial frames (where for each frame $\vv B$ is \emph{that frame's} four-velocity $\langle 1, \vv 0 \rangle$), and $\inlinedy{\mathrm{A}}$ must be a true antisymmetric Minkowski dyadic like $\inlinedy{\mathrm{F}}$ is. With the right choice of $\tilde{\vv q}$ and $\tilde{\vv w}$, any antisymmetric four-dyadic can be expressed in this manner.
%\footnote{For a deeper dive that proves it, see this unpublished paper from 2018 by Jonas Larsson and Karl Larsson: \url{https://urn.kb.se/resolve?urn=urn:nbn:se:umu:diva-152747}.}

So the combination of quasi-four-dyadics $\tilde{\vv q} \wedge \vv B - \{ \tilde{\vv w} \times \}$ makes a \emph{true} antisymmetric dyadic. For the Faraday tensor, it's $\inlinedy{\mathrm{F}} = \tilde{\vv e} \wedge \vv B - \{ \tilde{\vv b} \times \}$. And if we study the matrix representation of the Maxwell tensor (Equation \ref{eq:mt}), we find that $\inlinedy{\mathrm{G}} = \tilde{\vv b} \wedge \vv B + \{ \tilde{\vv e} \times \} = \tilde{\vv b} \wedge \vv B - \{ - \tilde{\vv e} \times \}$. That's how $\inlinedy{\mathrm{F}}$ and $\inlinedy{\mathrm{G}}$ are related: the \emph{Hodge dual} of an antisymmetric Minkowski dyadic $\inlinedy{\mathrm{A}} = \tilde{\vv q} \wedge \vv B - \{ \tilde{\vv w} \times \}$ is given by $\star \inlinedy{\mathrm{A}} = \tilde{\vv w} \wedge \vv B - \{ - \tilde{\vv q} \times \}$ (for obvious reasons, we call $\star$ the \textbf{star operator}).\footnote{Some other types of geometric objects have Hodge duals, too, but we're only concerning ourselves with the Hodge duals of antisymmetric Minkowski dyadics.} The Hodge dual of the Hodge dual is then $\star \star \inlinedy{\mathrm{A}} = - \tilde{\vv q} \wedge \vv B - \{ - \tilde{\vv w} \times \} = - \inlinedy{\mathrm{A}}$. Thus:
\begin{equation}\label{eq:mthd}
\boxed{ \vphantom{A_{A_A}} \capdy{\mathrm{G}} \equiv \star \capdy{\mathrm{F}} } \, ,
\end{equation}
and $\star \inlinedy{\mathrm{G}} = \star \star \inlinedy{\mathrm{F}} = - \inlinedy{\mathrm{F}}$.

It \emph{is} possible to express $\inlinedy{\mathrm{G}}$ as a sum of Minkowski dyads, as we've done with $\inlinedy{\mathrm{F}}$ (Equation \ref{eq:emta}), and as is true of all non-dyad four-dyadics. But we won't bother; for our purposes, Equation \ref{eq:mthd} will suffice as a geometric definition of the Maxwell tensor. We should note, though, that because $\vv b$ is a pseudovector that changes sign under certain types of coordinate transformations (see Footnote \ref{fn:ps}), the act of ``swapping" $\tilde{\vv e}$ and $\tilde{\vv b}$ to get the Hodge dual of $\inlinedy{\mathrm{F}}$ makes the resulting Maxwell tensor $\inlinedy{\mathrm{G}}$ itself a \emph{pseudodyadic} (or \emph{pseudotensor}) that does the same. The Hodge dual of an ordinary antisymmetric four-dyadic is \emph{always} a Minkowski pseudodyadic, and vice versa.

Last, we now understand that getting Equation \ref{eq:hme} ``for free" by virtue of the Faraday tensor's having the form $\partialup \wedge \vv A$ is an instance of a mathematical identity: for any four-vector (field) $\vv Q$, the Hodge dual of $\partialup \wedge \vv Q$ is divergenceless (i.e., $\partialup \cdot \, \star ( \partialup \wedge \vv Q ) = \textrm{\mbox{\boldmath $\emptyset$}}$). This rule is directly analogous to the divergencelessness of a curl in three-dimensional Euclidean space. It's also a manifestation of a broader principle called the \textbf{Bianchi identity}---the details involve differential forms and are well beyond our scope, but the basic geometric idea is that ``the boundary of a boundary is zero."\footnote{See Chapter 15 of \emph{Gravitation} by Charles W. Misner, Kip S. Thorne, and John Archibald Wheeler (``MTW"), reprinted in 2017 by Princeton University Press (originally published in 1973 by W. H. Freeman in San Francisco).}


\subsubsection{Light with Four-Dyadics}

In Section \ref{sssec:li}, we discussed electromagnetic plane waves in an explicitly relativistic fashion by starting with Equation \ref{eq:fp} ($\vv J = \Box \vv A$). But that equation is valid only in the Lorenz gauge, and combining Equations \ref{eq:ime} and \ref{eq:emta} we see that in general $\vv J = \partialup \cdot ( \partialup \otimes \vv A - \vv A \otimes \partialup_{\vv A} )$, or:
\begin{equation*}
\vv J = \Box \vv A - \partialup (\partialup \cdot \vv A)
\end{equation*}
(Equation \ref{eq:gfp}, which we originally gave without motivation). We might wonder, then, whether it's possible to describe light in a way that's both explicitly relativistic \emph{and gauge-invariant}. It is! First impose the free-space condition ($\vv J = \textrm{\mbox{\boldmath $\emptyset$}}$) on Equation \ref{eq:gfp}:
\begin{equation*}
\Box \vv A = \partialup(\partialup \cdot \vv A).
\end{equation*}
Next, take the d'Alembertian of the Faraday tensor (Equation \ref{eq:emta}) and sub in $\partialup(\partialup \cdot \vv A)$ for $\Box \vv A$, letting $\chi = \partialup \cdot \vv A$ for short:\footnote{We're verging on rank-3 territory now, since the d'Alembertian is the divergence of the gradient. This implied gradient of a dyadic is as close as we'll come to tensors beyond the second rank; any closer and we'd really need to introduce more flexible notation.}
\begin{equation}\label{eq:fthwe}
\begin{split}
\Box \capdy{\mathrm{F}} &= \Box ( \partialup \wedge \vv A ) \\
&= \partialup \wedge ( \Box \vv A) \\
&= \partialup \wedge (\partialup \chi) \\
&= ( \partialup \wedge \partialup ) \chi \\
\Box \capdy{\mathrm{F}} &= \capdy{0} .
\end{split}
\end{equation}
Here $\inlinedy{0}$ is the \textbf{zero four-dyadic} whose components are all zero. The key step $\partialup \wedge (\partialup \chi) = (\partialup \wedge \partialup) \chi$ follows from the fact that the order of mixed partials is reversible; verify with matrix notation that $\partialup \otimes ( \partialup \chi ) - ( \partialup \chi ) \otimes \partialup_{\chi} = \inlinedy{0}$. Equation \ref{eq:fthwe} is a homogeneous wave equation for the Faraday tensor in free space. Its general monochromatic plane-wave solution is (the real part of):
\begin{equation}\label{eq:ftwe}
\capdy{\mathrm{F}} (\vv R) = \bar{\capdy{\mathrm{F}}} \mathrm{e}^{\mathrm{i} \mkern.5mu \vv K \cdot \vv R} ,
\end{equation}
where the ``barred" amplitude four-dyadic is constant and antisymmetric, and where $\vv K = \langle K^{ct}, \vv k \rangle$ is the constant four-wavevector as before.

The solution is constrained by $\partialup \cdot \inlinedy{\mathrm{F}} = \textrm{\mbox{\boldmath $\emptyset$}}$ (Equation \ref{eq:ime} with $\vv J = \textrm{\mbox{\boldmath $\emptyset$}}$):
\begin{equation*}
\begin{split}
\textrm{\mbox{\boldmath $\emptyset$}} &= \partialup \cdot \bigl( \bar{\capdy{\mathrm{F}}} \mathrm{e}^{\mathrm{i} \mkern.5mu \vv K \cdot \vv R} \bigr) \\[2pt]
&= \left( \partialup \mathrm{e}^{\mathrm{i} \mkern.5mu \vv K \cdot \vv R} \right) \cdot \bar{\capdy{\mathrm{F}}} \\[2pt]
\textrm{\mbox{\boldmath $\emptyset$}} &= \vv K \cdot \bar{\capdy{\mathrm{F}}} .
\end{split}
\end{equation*}
If we label the ``barred" four-dyadic's components $\bar{e}_x$, $\bar{b}_x$, etc. (corresponding to their ``unbarred" counterparts in the Faraday tensor), and if we remember that $K^{ct} = k$ (because the four-wavevector is future-pointing lightlike), then carrying out the associated matrix multiplication gives us the conditions $\vv k \cdot \bar{\vv e} = 0$ and ${\bar{\vv e} = \bar{\vv b} \times \vv{\hat k}}$, matching part of what we found in Section \ref{sssec:li}. To show the rest ($\bar{e} = \bar{b}$ and $\bar{\vv b} \perp \vv k$), take the Hodge dual of Equation \ref{eq:ftwe} and apply $\partialup \cdot \inlinedy{\mathrm{G}} = \textrm{\mbox{\boldmath $\emptyset$}}$ (Equation \ref{eq:hme}), yielding:
\begin{equation*}
\textrm{\mbox{\boldmath $\emptyset$}} = \vv K \cdot \bar{\capdy{\mathrm{G}}} ,
\end{equation*}
wherefrom $\vv k \cdot \bar{\vv b} = 0$ and ${\bar{\vv b} = \vv{\hat k}} \times \bar{\vv e}$. So everything checks out, and Equation \ref{eq:ftwe} is an explicitly relativistic \emph{gauge-invariant} description of monochromatic electromagnetic plane waves.\footnote{You might note that the real part of Equation \ref{eq:ftwe} gives a \emph{cosine} function for $\vv e$ and $\vv b$, whereas Equations \ref{eq:bpw} and \ref{eq:epw} gave \emph{sine} functions. Strictly speaking, we should allow for a phase constant in the exponential argument.}

For the Lorenz gauge specifically, we can just combine Equations \ref{eq:pw} and \ref{eq:emta} (real part implied):
\begin{equation*}
\begin{split}
\capdy{\mathrm{F}} (\vv R) &= \partialup \wedge (  \bar{\vv A} \mathrm{e}^{\mathrm{i} \mkern.5mu \vv K \cdot \vv R} ) \\[2pt]
&= ( \partialup \mathrm{e}^{\mathrm{i} \mkern.5mu \vv K \cdot \vv R} ) \wedge \bar{\vv A}
\end{split}
\end{equation*}
(by a product rule that some matrix-ing can verify; $\bar{\vv A} = \langle \bar{A}^{ct}, \bar{\vv a} \rangle$ is constant, recall). Differentiating:
\begin{equation}\label{eq:emtl}
\capdy{\mathrm{F}} (\vv R) = \mathrm{i} \mathrm{e}^{\mathrm{i} \mkern.5mu \vv K \cdot \vv R} \, \vv K \wedge \bar{\vv A} .
\end{equation}
Let's make sure that the plane-wave solutions for $\vv e$ and $\vv b$ that we found in Section \ref{sssec:li} are embedded here. We'll reintroduce our shorthand $f = \vv K \cdot \vv R$. In matrix notation (using Equation \ref{eq:emt}), our Lorenz-gauge Equation \ref{eq:emtl} is (the real part of):
\begin{equation*}
[ \capdy{\mathrm{F}} ]
=
\begin{bmatrix}
0 & [ - \vv e]^\mathrm{T} \\[1ex]
[\vv e] & [ \{ \vv b \times \} ]
\end{bmatrix}
=
\mathrm{i} \mathrm{e}^{\mathrm{i} f}
\begin{bmatrix}
0 & [K^{ct} \bar{\vv a} - \bar{A}^{ct} \vv k ]^\mathrm{T} \mkern1mu \\[1ex]
[\bar{A}^{ct} \vv k - K^{ct} \bar{\vv a}] & [ \vv k \wedge \bar{\vv a} ] 
\end{bmatrix} ,
\end{equation*}
where $\{ \vv b \times \}$ is the three-dyadic that satisfies $\{ \vv b \times \} \cdot \vv q = \vv b \times \vv q$ for any three-vector $\vv q$, and where the three-dyadic $\vv k \wedge \bar{\vv a} = {\vv k \otimes \bar{\vv a} - \bar{\vv a} \otimes \vv k}$. This gives $\vv b = \mathrm{i} \mathrm{e}^{\mathrm{i} f} \, \bar{\vv a} \times \vv k$ (check that!) and $\vv e = \mathrm{i} \mathrm{e}^{\mathrm{i} f} ( \bar{A}^{ct} \vv k - K^{ct} \bar{\vv a} )$, which were indeed steps on the way to Equations \ref{eq:bpw} and \ref{eq:epw}. Everything checks out again (and by now we've noticed a relationship between cross products and antisymmetric dyadics, yes?).


\subsubsection{Double-Dotting Four-Dyadics; Trace; Field Invariants}

If we have a pair of four-dyads $\vv Q \otimes \vv W$ and $\vv S \otimes \vv U$, we can make a Lorentz-invariant scalar from them like this:
\begin{equation*}
(\vv Q \otimes \vv W) : (\vv S \otimes \vv U) \equiv (\vv Q \cdot \vv S) (\vv W \cdot \vv U).
\end{equation*}
This commutative operation is called the \textbf{double dot product}. To extend the idea to non-dyad dyadics, we put them into sum-of-dyads form and invoke linearity:
\begin{equation*}
\begin{aligned}
&(\vv Q \otimes \vv W + \vv H \otimes \vv E + \dots) : (\vv S \otimes \vv U + \vv M \otimes \vv N + \dots) \\[3pt]
&\qquad = (\vv Q \otimes \vv W) : (\vv S \otimes \vv U) + (\vv Q \otimes \vv W) : (\vv M \otimes \vv N) + (\vv Q \otimes \vv W) : \, \dots \\
& \qquad \quad + (\vv H \otimes \vv E) : (\vv S \otimes \vv U) + (\vv H \otimes \vv E) : (\vv M \otimes \vv N) + (\vv H \otimes \vv E) : \, \dots \\
& \qquad \quad + \dots
\end{aligned}
\end{equation*}
Of course, if we already have the sixteen ``doubled up" ($ct$ + Cartesian) components of each input-dyadic, it may be easier to calculate with matrices directly than to put the dyadics into sum-of-dyads form and fuss with all that distribution. But what \emph{is} the matrix representation of the double dot product? For dyads it's clearly $[ (\vv Q \otimes \vv W) : (\vv S \otimes \vv U) ] = [\vv Q]^{\mathrm{T}} \eta [\vv S] [\vv W]^{\mathrm{T}} \eta [\vv U]$, a 1-by-1 matrix, but this won't help us for non-dyad dyadics (where would we plug them in?). It would be handy to have an alternative matrix representation of the double dot product for dyads, one that doesn't ``separate" an input-dyad's constituent vectors from each other.

The key is to bring in the matrix \emph{trace}, which is the sum of the main-diagonal entries of a square matrix (a 1-by-1 qualifies!). We'll take advantage of the fact that the trace of a matrix product is invariant under cyclic permutations of the multiplied matrices:
\begin{equation*}
\begin{aligned}
(\vv Q \otimes \vv W) : (\vv S \otimes \vv U) &= (\vv Q \cdot \vv S) (\vv W \cdot \vv U) \\
&= (\vv S \cdot \vv Q) (\vv W \cdot \vv U) \\
&= \mathrm{Tr} \left( [\vv S]^{\mathrm{T}} \eta [\vv Q] [\vv W]^{\mathrm{T}} \eta [\vv U] \right) \\[2pt]
&= \mathrm{Tr} \left( \eta [\vv Q] [\vv W]^{\mathrm{T}} \eta [\vv U] [\vv S]^{\mathrm{T}} \right) \\[2pt]
&= \mathrm{Tr} \left( \eta [\vv Q \otimes \vv W] \eta [\vv S \otimes \vv U]^\mathrm{T} \right).
\end{aligned}
\end{equation*}
\emph{Now} we have a matrix representation of the double dot product (several, actually) that we can use for arbitrary four-dyadics $\inlinedy{\mathrm{D}}$ and $\inlinedy{\mathrm{K}}$:
\begin{equation*}
\capdy{\mathrm{D}} : \capdy{\mathrm{K}} = \mathrm{Tr} \left( \eta [\capdy{\mathrm{D}}] \eta [ \capdy{\mathrm{K}} ]^{\mathrm{T}} \right) = \mathrm{Tr} \left( [\capdy{\mathrm{K}}] \eta [ \capdy{\mathrm{D}} ]^{\mathrm{T}} \eta \right) = \mathrm{Tr} \left( \eta [\capdy{\mathrm{K}}] \eta [ \capdy{\mathrm{D}} ]^{\mathrm{T}} \right) = \capdy{\mathrm{K}} : \capdy{\mathrm{D}} ,
\end{equation*}
where we've transposed the matrix product to show that commutativity still holds (the transpose of a matrix product is the product of the transposes in reverse order, and the symmetric $\eta$ is its own transpose). This works because a matrix and its transpose share a trace. Trivially from the above, then:
\begin{equation*}
\capdy{\mathrm{D}} \mkern0mu ^\mathrm{T} : \capdy{\mathrm{K}} \mkern0mu ^\mathrm{T} = \mathrm{Tr} \left( \eta [\capdy{\mathrm{D}}]^{\mathrm{T}} \eta [ \capdy{\mathrm{K}} ] \right) = \capdy{\mathrm{D}} : \capdy{\mathrm{K}} .
\end{equation*}
A special case is when a dyadic is double-dotted with $\inlinedy{\upeta}$, the Minkowski metric tensor (whose matrix representation in every frame is $\eta$):
\begin{equation*}
\begin{split}
\capdy{\mathrm{D}} : \inlinedy{\upeta} &= \mathrm{Tr} \left( \eta [\capdy{\mathrm{D}}] \eta [ \mkern1mu \inlinedy{\upeta} \mkern.5mu ]^{\mathrm{T}} \right) \\
&= \mathrm{Tr} \left( \eta [\capdy{\mathrm{D}}] \right) = D^{ct \mkern1.5mu ct} - D^{xx} - D^{yy} - D^{zz}
\end{split}
\end{equation*}
(verify that $\eta \eta = I_4$, the 4-by-4 identity matrix). This invariant is called the trace \emph{of the dyadic} $\inlinedy{\mathrm{D}}$, though its matrix representation is $\mathrm{Tr} (\eta [ \inlinedy{\mathrm{D}} ])$, \emph{not} $\mathrm{Tr} ( [ \inlinedy{\mathrm{D}} ] )$. For example, the trace of $\inlinedy{\upeta}$ is $\mathrm{Tr} (I_4) = 4$. Obviously all antisymmetric dyadics are traceless (their matrix representations have only zeros on the main diagonal). More generally, the double dot product of an antisymmetric dyadic $\inlinedy{\mathrm{A}}$ with \emph{any symmetric} dyadic $\inlinedy{\mathrm{S}}$ is zero, since $\inlinedy{\mathrm{A}} = - \inlinedy{\mathrm{A}} \mkern-2mu ^\mathrm{T}$ and $\inlinedy{\mathrm{S}} = \inlinedy{\mathrm{S}} \mkern-2mu^\mathrm{T}$:
\begin{equation*}
\capdy{\mathrm{A}} : \capdy{\mathrm{S}} = - \capdy{\mathrm{A}} \mkern0mu ^\mathrm{T} : \capdy{\mathrm{S}} \mkern0mu ^\mathrm{T} = - \capdy{\mathrm{A}} : \capdy{\mathrm{S}}
\end{equation*}
(if it's equal to its additive inverse then it must be zero).

An immediate application of the double dot product is to discover new(?) Lorentz invariants by double-dotting the Faraday and Maxwell tensors (Equations \ref{eq:emt} and \ref{eq:mt}) with each other and with themselves. First:
\begin{equation}\label{eq:fddg}
\begin{split}
\capdy{\mathrm{F}} : \capdy{\mathrm{G}} &= \mathrm{Tr} \left( \eta [ \capdy{\mathrm{F}} ] \eta [ \capdy{\mathrm{G}} ]^{\mathrm{T}} \right) \\[5pt]
&=
\mathrm{Tr}
\left(
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\
-e_x & 0 & b_z & -b_y \\
-e_y & -b_z & 0 & b_x \\
-e_z & b_y & -b_x & 0
\end{bmatrix}
\begin{bmatrix}
0 & b_x & b_y & b_z \\
b_x & 0 & e_z & -e_y \\
b_y & -e_z & 0 & e_x \\
b_z & e_y & -e_x & 0
\end{bmatrix}
\right) \\[5pt]
\capdy{\mathrm{F}} : \capdy{\mathrm{G}} &= -4 (\vv e \cdot \vv b).
\end{split}
\end{equation}
That's \emph{not} new---except for the factor of $-4$, it's one of the two invariants we made from $\vv e$ and $\vv b$ back in Section \ref{sssec:lff}, but now we see where it comes from geometrically (and now we see that the double dot product of an ordinary dyadic and a pseudodyadic is a pseudoscalar). The other invariant was $e^2 - b^2$, and sure enough it's what we get when we double-dot $\inlinedy{\mathrm{F}}$ or $\inlinedy{\mathrm{G}}$ with \emph{itself}:
\begin{equation}\label{eq:fddf}
\begin{split}
\capdy{\mathrm{F}} : \capdy{\mathrm{F}} &= \mathrm{Tr} \left( \eta [ \capdy{\mathrm{F}} ] \eta [ \capdy{\mathrm{F}} ]^{\mathrm{T}} \right) \\[5pt]
&=
\mathrm{Tr}
\left(
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\
-e_x & 0 & b_z & -b_y \\
-e_y & -b_z & 0 & b_x \\
-e_z & b_y & -b_x & 0
\end{bmatrix}
\begin{bmatrix}
0 & e_x & e_y & e_z \\
e_x & 0 & -b_z & b_y \\
e_y & b_z & 0 & -b_x \\
e_z & -b_y & b_x & 0
\end{bmatrix}
\right) \\[5pt]
\capdy{\mathrm{F}} : \capdy{\mathrm{F}} &= 2(b^2 - e^2),
\end{split}
\end{equation}
and:
\begin{equation}\label{eq:gddg}
\begin{split}
\capdy{\mathrm{G}} : \capdy{\mathrm{G}} &= \mathrm{Tr} \left( \eta [ \capdy{\mathrm{G}} ] \eta [ \capdy{\mathrm{G}} ]^{\mathrm{T}} \right) \\[5pt]
&=
\mathrm{Tr}
\left(
\begin{bmatrix}
0 & -b_x & -b_y & -b_z \\
-b_x & 0 & -e_z & e_y \\
-b_y & e_z & 0 & -e_x \\
-b_z & -e_y & e_x & 0
\end{bmatrix}
\begin{bmatrix}
0 & b_x & b_y & b_z \\
b_x & 0 & e_z & -e_y \\
b_y & -e_z & 0 & e_x \\
b_z & e_y & -e_x & 0
\end{bmatrix}
\right) \\[5pt]
\capdy{\mathrm{G}} : \capdy{\mathrm{G}} &= 2 (e^2 - b^2)
\end{split}
\end{equation}
(double-dotting a pseudodyadic with a pseudodyadic gives an ordinary scalar).

There's another Lorentz invariant we can make from a pair of four-dyads:
\begin{equation*}
(\vv Q \otimes \vv W) \cdot \cdot \, \mkern1mu (\vv S \otimes \vv U) \equiv (\vv Q \cdot \vv U) (\vv W \cdot \vv S) ,
\end{equation*}
where again the matrix trace is convenient for extending the operation to arbitrary four-dyadics $\inlinedy{\mathrm{D}}$ and $\inlinedy{\mathrm{K}}$ (same line of reasoning as before):
\begin{equation*}
\capdy{\mathrm{D}} \cdot \cdot \, \mkern1mu \capdy{\mathrm{K}} = \mathrm{Tr} \left( \eta [\capdy{\mathrm{D}}] \eta [ \capdy{\mathrm{K}} ] \right) = \mathrm{Tr} \left( [ \capdy{\mathrm{K}} ]^\mathrm{T} \eta [\capdy{\mathrm{D}}]^\mathrm{T} \eta \right) = \capdy{\mathrm{D}} \mkern0mu ^\mathrm{T} \mkern-2mu \cdot \cdot \, \mkern1mu \capdy{\mathrm{K}} \mkern0mu ^\mathrm{T} .
\end{equation*}
This second ``kind" of double dot product is likewise commutative, and in general it produces an altogether different scalar than the first kind does. But if at least one of the input-dyadics is symmetric then $\inlinedy{\mathrm{D}}  \cdot \cdot \inlinedy{\mathrm{K}} = \inlinedy{\mathrm{D}} : \inlinedy{\mathrm{K}}$ (because we can replace the symmetric dyadic with its transpose), and if at least one is antisymmetric then $\inlinedy{\mathrm{D}}  \cdot \cdot \inlinedy{\mathrm{K}} = - \inlinedy{\mathrm{D}} : \inlinedy{\mathrm{K}}$ (because we can replace the antisymmetric dyadic with its \emph{negative} transpose). Both the Faraday and Maxwell tensors are antisymmetric, so we don't get new electromagnetic invariants with the second kind of double dot product; rather, ${\inlinedy{\mathrm{F}}  \cdot \cdot \inlinedy{\mathrm{G}} = - \inlinedy{\mathrm{F}} : \inlinedy{\mathrm{G}}}$, and so on. 


\subsubsection{Single-Dotting Four-Dyadics; EM Stress--Energy Tensor}

We have a dot product between four-vectors, dot products between four-dyadics and four-vectors, and \emph{double} dot products between four-dyadics, but what about just a ``single" dot product between four-dyadics? If we have a dyad $\vv Q \otimes \vv W$ that we'd like to ``dot" with another dyad $\vv S \otimes \vv U$, our matrix notation (with $\eta$ for the dot!) naturally suggests:
\begin{equation*}
\begin{aligned}
\mkern0mu [ \vv Q \otimes \vv W ] \eta [ \vv S \otimes \vv U ] &= [\vv Q][\vv W]^{\mathrm{T}} \eta [\vv S][\vv U]^{\mathrm{T}} \\
&= (\vv W \cdot \vv S) \, [\vv Q] [\vv U]^{\mathrm{T}} \\[3pt]
(\vv Q \otimes \vv W) \cdot (\vv S \otimes \vv U) & \equiv (\vv W \cdot \vv S) (\vv Q \otimes \vv U) .
\end{aligned}
\end{equation*}
So the dot product between two four-dyads generates a new dyad, and the operation isn't commutative. If we want to dot dyadics that aren't simple dyads, we may put them in sum-of-dyads form and distribute:
\begin{equation*}
\begin{aligned}
&(\vv Q \otimes \vv W + \vv H \otimes \vv E + \dots ) \cdot (\vv S \otimes \vv U + \vv M \otimes \vv N + \dots ) \\[3pt]
&\qquad = (\vv Q \otimes \vv W) \cdot (\vv S \otimes \vv U) + (\vv Q \otimes \vv W) \cdot (\vv M \otimes \vv N) + (\vv Q \otimes \vv W) \cdot \, \dots \\
& \qquad \quad + (\vv H \otimes \vv E) \cdot (\vv S \otimes \vv U) + (\vv H \otimes \vv E) \cdot (\vv M \otimes \vv N) + (\vv H \otimes \vv E) \cdot \, \dots \\
& \qquad \quad + \dots
\end{aligned}
\end{equation*}
(a new dyadic). If we already know the components of the input-dyadics $\inlinedy{\mathrm{D}}$ and $\inlinedy{\mathrm{K}}$, it may be easier to calculate with matrices directly:
\begin{equation*}
[\capdy{\mathrm{D}} \cdot \capdy{\mathrm{K}}] = [\capdy{\mathrm{D}}] \eta [\capdy{\mathrm{K}}] .
\end{equation*}
We see that $\inlinedy{\mathrm{D}} \cdot \inlinedy{\upeta} = \inlinedy{\upeta} \cdot \inlinedy{\mathrm{D}} =  \inlinedy{\mathrm{D}}$, further ``justifying" the notion that the Minkowski metric tensor is like the unit dyadic of spacetime. We also have $(\inlinedy{\mathrm{D}} \cdot \inlinedy{\mathrm{K}}) : \inlinedy{\upeta} = \inlinedy{\mathrm{D}} \mkern1mu \cdot \cdot \inlinedy{\mathrm{K}}$, which equals $- \inlinedy{\mathrm{D}} : \inlinedy{\mathrm{K}}$ in the case that $\inlinedy{\mathrm{D}}$ or $\inlinedy{\mathrm{K}}$ is antisymmetric (relevant for the Faraday and Maxwell tensors). Along the same lines, $ \inlinedy{\mathrm{A}} \mkern-2mu ^\mathrm{T} \cdot \inlinedy{\mathrm{D}} = - \inlinedy{\mathrm{A}} \cdot \inlinedy{\mathrm{D}}$ and $\inlinedy{\mathrm{D}} \cdot \inlinedy{\mathrm{A}} \mkern-2mu ^\mathrm{T} = - \inlinedy{\mathrm{D}} \cdot \inlinedy{\mathrm{A}}$ for antisymmetric $\inlinedy{\mathrm{A}}$.

Let's test out the operation on the Faraday and Maxwell tensors (Equations \ref{eq:emt} and \ref{eq:mt}). First $\inlinedy{\mathrm{F}} \cdot \inlinedy{\mathrm{G}}$:
\begin{equation*}
\begin{aligned}
\mkern0mu [\capdy{\mathrm{F}}] \eta [\capdy{\mathrm{G}}]
&=
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\
e_x & 0 & -b_z & b_y \\
e_y & b_z & 0 & -b_x \\
e_z & -b_y & b_x & 0
\end{bmatrix}
\begin{bmatrix}
0 & -b_x & -b_y & -b_z \\
-b_x & 0 & -e_z & e_y \\
-b_y & e_z & 0 & -e_x \\
-b_z & -e_y & e_x & 0
\end{bmatrix} \\[5pt]
&=
\begin{bmatrix}
\vv e \cdot \vv b & 0 & 0 & 0 \\
0 & - \vv e \cdot \vv b & 0 & 0 \\
0 & 0 & - \vv e \cdot \vv b & 0 \\
0 & 0 & 0 & - \vv e \cdot \vv b
\end{bmatrix}
=
- \dfrac{1}{4} \, ( \capdy{\mathrm{F}} : \capdy{\mathrm{G}} ) \eta
\end{aligned}
\end{equation*}
(we've used Equation \ref{eq:fddg}), and $\inlinedy{\mathrm{F}} \cdot \inlinedy{\mathrm{G}} = - ( \inlinedy{\mathrm{F}} : \inlinedy{\mathrm{G}} ) \inlinedy{\upeta} / 4$, a symmetric pseudodyadic. The ``inverse" product $\inlinedy{\mathrm{G}} \cdot \inlinedy{\mathrm{F}}$ actually gives the same pseudodyadic, even though the dot product between dyadics isn't generally commutative.

Now $\inlinedy{\mathrm{F}} \cdot \inlinedy{\mathrm{F}}$:
\begin{equation*}
\begin{aligned}
\mkern0mu [\capdy{\mathrm{F}}] \eta [\capdy{\mathrm{F}}]
&=
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\
e_x & 0 & -b_z & b_y \\
e_y & b_z & 0 & -b_x \\
e_z & -b_y & b_x & 0
\end{bmatrix}
\begin{bmatrix}
0 & -e_x & -e_y & -e_z \\
-e_x & 0 & b_z & -b_y \\
-e_y & -b_z & 0 & b_x \\
-e_z & b_y & -b_x & 0
\end{bmatrix} \\[5pt]
&=
\begin{bmatrix}
e^2 & (e_y b_z - e_z b_y) & (e_z b_x - e_x b_z) & (e_x b_y - e_y b_x) \\[1ex]
(e_y b_z - e_z b_y) & (b_y^2 + b_z^2 - e_x^2) & (-e_x e_y - b_x b_y) & (- e_x e_z - b_x b_z) \\[1ex]
(e_z b_x - e_x b_z) & (-e_x e_y - b_x b_y) & (b_z^2 + b_x^2 - e_y^2)  & (- e_y e_z - b_y b_z) \\[1ex]
(e_x b_y - e_y b_x) & (- e_x e_z - b_x b_z) & (- e_y e_z - b_y b_z) & (b_x^2 + b_y^2 - e_z^2)
\end{bmatrix} ,
\end{aligned}
\end{equation*}
or:
\begin{equation*}
\mkern0mu [\capdy{\mathrm{F}}] \eta [\capdy{\mathrm{F}}]
=
\begin{bmatrix}
e^2 & s_x & s_y & s_z \\[1ex]
s_x & (b^2 - b_x^2 - e_x^2) & - \sigma_{x y} & - \sigma_{x z} \\[1ex]
s_y & - \sigma_{y x} & (b^2 - b_y^2 - e_y^2)  & - \sigma_{y z} \\[1ex]
s_z & - \sigma_{z x} & - \sigma_{z y} & (b^2 - b_z^2 - e_z^2) 
\end{bmatrix} ,
\end{equation*}
where $\vv s = \vv e \times \vv b$ (Poynting vector), $\inlinedy{\upsigma} = \vv e \otimes \vv e + \vv b \otimes \vv b - u \mkern-5mu \inlinedy{\upiota}$ (Maxwell stress tensor), $u = (e^2 + b^2) / 2$ (electromagnetic energy density), and $\inlinedy{\upiota}$ is the Euclidean unit dyadic. Similarly, $\inlinedy{\mathrm{G}} \cdot \inlinedy{\mathrm{G}}$:
\begin{equation*}
\begin{aligned}
\mkern0mu [\capdy{\mathrm{G}}] \eta [\capdy{\mathrm{G}}]
&=
\begin{bmatrix}
0 & -b_x & -b_y & -b_z \\
b_x & 0 & e_z & -e_y \\
b_y & -e_z & 0 & e_x \\
b_z & e_y & -e_x & 0
\end{bmatrix}
\begin{bmatrix}
0 & -b_x & -b_y & -b_z \\
-b_x & 0 & -e_z & e_y \\
-b_y & e_z & 0 & -e_x \\
-b_z & -e_y & e_x & 0
\end{bmatrix} \\[5pt]
&=
\begin{bmatrix}
b^2 & s_x & s_y & s_z \\[1ex]
s_x & (e^2 - e_x^2 - b_x^2) & - \sigma_{x y} & - \sigma_{x z} \\[1ex]
s_y & - \sigma_{y x} & (e^2 - e_y^2 - b_y^2)  & - \sigma_{y z} \\[1ex]
s_z & - \sigma_{z x} & - \sigma_{z y} & (e^2 - e_z^2 - b_z^2) 
\end{bmatrix}
\end{aligned}
\end{equation*}
(not a pseudodyadic). The symmetric dyadics $\inlinedy{\mathrm{F}} \cdot \inlinedy{\mathrm{F}}$ and $\inlinedy{\mathrm{G}} \cdot \inlinedy{\mathrm{G}}$ are begging to be summed!
\begin{equation*}
[\capdy{\mathrm{F}}] \eta [\capdy{\mathrm{F}}] + [\capdy{\mathrm{G}}] \eta [\capdy{\mathrm{G}}]
=
\begin{bmatrix}
2 u & 2 s_x & 2 s_y & 2 s_z \\[1ex]
2 s_x & - 2 \sigma_{x x} & - 2 \sigma_{x y} & - 2 \sigma_{x z} \\[1ex]
2 s_y & - 2 \sigma_{y x} & - 2 \sigma_{yy}  & - 2 \sigma_{y z} \\[1ex]
2 s_z & - 2 \sigma_{z x} & - 2 \sigma_{z y} & - 2 \sigma_{zz}
\end{bmatrix}.
\end{equation*}
Define:
\begin{equation}\label{eq:eset}
\boxed { \capdy{\mathrm{T}} \equiv \dfrac{1}{2} \, ( \capdy{\mathrm{F}} \cdot \capdy{\mathrm{F}} \, + \, \capdy{\mathrm{G}} \cdot \capdy{\mathrm{G}} ) } \, ,
\end{equation}
and we have a four-dyadic that combines $u$, $\vv s$, and $\inlinedy{\upsigma}$:
\begin{equation*}
[\capdy{\mathrm{T}}]
=
\begin{bmatrix}
u & [\vv s]^{\mathrm{T}} \\[1ex]
[\vv s] & - [ \mkern1mu \inlinedy{\upsigma} \mkern1mu ]
\end{bmatrix} .
\end{equation*}
This dyadic is symmetric and also traceless ($\inlinedy{\mathrm{T}} : \inlinedy{\upeta} = 0$):
\begin{equation*}
\begin{split}
2 \mkern1mu \capdy{\mathrm{T}} : \inlinedy{\upeta} &= ( \capdy{\mathrm{F}} \cdot \capdy{\mathrm{F}}  ) : \inlinedy{\upeta} \, + \; ( \capdy{\mathrm{G}} \cdot \capdy{\mathrm{G}}  ) : \inlinedy{\upeta} \\
&= \capdy{\mathrm{F}} \cdot \cdot \, \mkern1mu \capdy{\mathrm{F}} \, + \, \capdy{\mathrm{G}} \cdot \cdot \, \mkern1mu \capdy{\mathrm{G}} \\
&=  - \capdy{\mathrm{F}} : \capdy{\mathrm{F}} - \capdy{\mathrm{G}} : \capdy{\mathrm{G}} ,
\end{split}
\end{equation*}
which vanishes by Equations \ref{eq:fddf} and \ref{eq:gddg}. Note that to get from $[\inlinedy{\mathrm{F}} \cdot \inlinedy{\mathrm{F}}]$ to $[\inlinedy{\mathrm{T}}]$ requires only the addition of the invariant $(b^2 - e^2) / 2 = ( \inlinedy{\mathrm{F}} : \inlinedy{\mathrm{F}} ) / 4$ to the top-left entry and $( \inlinedy{\mathrm{G}} : \inlinedy{\mathrm{G}} ) / 4 = - ( \inlinedy{\mathrm{F}} : \inlinedy{\mathrm{F}} ) / 4$ to the other three entries on the main diagonal. Likewise for getting from $[ \inlinedy{\mathrm{G}} \cdot \inlinedy{\mathrm{G}} ]$ to $[ \inlinedy{\mathrm{T}} ]$, but with the signs of the added invariants reversed. So there are several alternative ways to express $\inlinedy{\mathrm{T}}$ in terms of the Faraday and Maxwell tensors, all equivalent to Equation \ref{eq:eset}. Here are a few of them:
\begin{equation*}
\begin{aligned}
\capdy{\mathrm{T}} &= \capdy{\mathrm{F}} \cdot \capdy{\mathrm{F}} +  \dfrac{1}{4} \, ( \capdy{\mathrm{F}} : \capdy{\mathrm{F}} ) \inlinedy{\upeta} \, = \, \capdy{\mathrm{F}} \cdot \capdy{\mathrm{F}} -  \dfrac{1}{4} \, ( \capdy{\mathrm{F}} \cdot \cdot \, \mkern1mu \capdy{\mathrm{F}} ) \inlinedy{\upeta} \\[5pt]
&= \capdy{\mathrm{G}} \cdot \capdy{\mathrm{G}} +  \dfrac{1}{4} \, ( \capdy{\mathrm{G}} : \capdy{\mathrm{G}} ) \inlinedy{\upeta}  \, = \,  \capdy{\mathrm{G}} \cdot \capdy{\mathrm{G}} -  \dfrac{1}{4} \, ( \capdy{\mathrm{G}} \cdot \cdot \, \capdy{\mathrm{G}} ) \inlinedy{\upeta}  \\[5pt]
&= \capdy{\mathrm{F}} \cdot \capdy{\mathrm{F}} -  \dfrac{1}{4} \, ( \capdy{\mathrm{G}} : \capdy{\mathrm{G}} ) \inlinedy{\upeta} \, = \, \capdy{\mathrm{G}} \cdot \capdy{\mathrm{G}} +  \dfrac{1}{4} \, ( \capdy{\mathrm{F}} \cdot \cdot \, \mkern1mu \capdy{\mathrm{F}} ) \inlinedy{\upeta}
\end{aligned}
\end{equation*}
(expressions with just $\inlinedy{\mathrm{F}}$ are most common in the literature).\footnote{Because the Faraday and Maxwell tensors are antisymmetric, we have relationships like $\footnotedy{\mathrm{F}} \cdot \footnotedy{\mathrm{F}} \mkern-2mu^\mathrm{T} = \footnotedy{\mathrm{F}} \mkern-2mu^\mathrm{T} \cdot \footnotedy{\mathrm{F}} = - \footnotedy{\mathrm{F}} \cdot \footnotedy{\mathrm{F}}$, and we get even more expressions for $\footnotedy{\mathrm{T}}$ if we throw those into the mix. The signs of terms would also be affected if we'd chosen the ($-$$+$$+$$+$) sign convention instead of ($+$$-$$-$$-$). All this is to say, don't be alarmed to find different ``variations" on how $\footnotedy{\mathrm{T}}$ is written in other sources; they're reconcilable. Where index notation is used, check the \emph{order} and \emph{placement} of the indices.}

Our new dyadic $\inlinedy{\mathrm{T}}$ is the \textbf{electromagnetic stress--energy tensor}. Its significance is revealed when we closely examine Equation \ref{eq:py} (Poynting's theorem, a statement of energy conservation) and Equation \ref{eq:tmc} (the corresponding statement of three-momentum conservation). Together, these expressions convey exactly the same information as:
\begin{equation}\label{eq:fmc}
\boxed{ \partialup \cdot \capdy{\mathrm{T}} = - \dfrac{\dd \vv F_{\mathrm{L}}}{\dd V_0} } \, ,
\end{equation}
because in matrix notation that's:
\begin{equation*}
\begin{bmatrix}
\partial^{ct} & [ \del ]^{\mathrm{T}} \mkern1mu
\end{bmatrix}
\begin{bmatrix}
u & [\vv s]^{\mathrm{T}} \\[1ex]
[\vv s] & - [ \mkern1mu \inlinedy{\upsigma} \mkern1mu ]
\end{bmatrix}
=
\begin{bmatrix}
- \dfrac{\dd \mathcal{P}_{\textrm{L}}}{\dd V} & - \dfrac{\dd \vv f_{\mathrm{L}}}{\dd V}
\end{bmatrix}
\end{equation*}
(no naught subscripts on the right---see Equation \ref{eq:lffdc}). Equation \ref{eq:fmc} is a statement of \emph{four-momentum} conservation that explicitly accounts for the electromagnetic field. The four-divergence of $\inlinedy{\mathrm{T}}$ vanishes when the fields are transferring no four-momentum to charged particles. We can express the right side of Equation \ref{eq:fmc} in terms of the fields and charges (differentiate Equation \ref{eq:lfffd} with respect to proper volume, and sub in $\vv J$ for $\rho_0 \vv B$):
\begin{equation*}
\partialup \cdot \capdy{\mathrm{T}} = - \capdy{\mathrm{F}} \cdot \, \vv J .
\end{equation*}


\end{document}